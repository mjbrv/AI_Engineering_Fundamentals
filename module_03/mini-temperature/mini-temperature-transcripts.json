{
  "notebook": "mini-temperature.ipynb",
  "module": "Module 2: LLM Core Concepts",
  "total_duration": "~15 min",
  "transcripts": [
    {
      "cell_index": 0,
      "cell_type": "markdown",
      "section": "Introduction",
      "transcript": "Hey everyone! Welcome to our Temperature Effects mini-lab. So temperature is one of those parameters that everyone's heard of, but I find a lot of people don't really understand what it's actually doing under the hood. Today we're gonna demystify that. We'll look at the actual math, see how it affects the probability distribution, and most importantly, develop intuition for when to use low versus high temperature. This is like... fifteen minutes that'll really level up your prompt engineering. Let's go!"
    },
    {
      "cell_index": 1,
      "cell_type": "markdown",
      "section": "1. Setup",
      "transcript": "Quick setup first - just loading our standard libraries."
    },
    {
      "cell_index": 2,
      "cell_type": "code",
      "section": "1. Setup",
      "concepts": ["OpenAI client", "environment setup", "markdown display"],
      "transcript": "Just our standard imports here - OpenAI client, dotenv for the API keys, and a little markdown helper for pretty output. Setup complete, we're ready to roll!"
    },
    {
      "cell_index": 3,
      "cell_type": "markdown",
      "section": "2. How Temperature Works (The Math)",
      "transcript": "Okay, let's talk about what temperature actually does mathematically. It's applied as a scaling factor to the logits - those raw scores - before the softmax function. When temperature approaches zero, the distribution becomes super peaked, basically deterministic. At temperature equals one, you get the original distribution. And above one, it flattens out, making everything more random."
    },
    {
      "cell_index": 4,
      "cell_type": "code",
      "section": "2. How Temperature Works (The Math)",
      "concepts": ["temperature scaling", "softmax", "probability distribution", "entropy"],
      "transcript": "Let me show you this visually. I've got simulated logits for 5 tokens - 'best', 'good', 'fine', 'okay', 'bad'. At temperature 0.1, look at that - 'best' has 99.3% probability! It's almost guaranteed to pick that one. The entropy is super low, 0.041, meaning very little randomness. Now as we increase temperature... at 0.5, 'best' drops to 63.6% and other options start getting a chance. At temperature 1.0 - the default - it's 42.9% for 'best'. And at temperature 2.0, we're down to 31% for 'best', with 'bad' now having an 11.4% chance of being picked! See how the entropy keeps increasing? Higher entropy means more randomness, more surprising outputs. This is the fundamental tradeoff."
    },
    {
      "cell_index": 5,
      "cell_type": "markdown",
      "section": "3. Temperature in Practice",
      "transcript": "Alright, that's the theory. Let's see how this actually plays out with real model outputs."
    },
    {
      "cell_index": 6,
      "cell_type": "code",
      "section": "3. Temperature in Practice",
      "concepts": ["temperature comparison", "output variability", "factual questions"],
      "transcript": "So here's the experiment - we ask the same question at different temperatures and run it 3 times each. 'What is an apple? Answer in one short sentence.' At temperature 0, all three runs give the exact same answer. At 0.5, you might see slight variations in wording. At 1.0, more variety. And at 1.5 and 2.0, the outputs can get pretty different from each other. For factual questions like this, you generally want low temperature because there's really one correct answer."
    },
    {
      "cell_index": 7,
      "cell_type": "code",
      "section": "3. Temperature in Practice",
      "concepts": ["creative tasks", "temperature for creativity"],
      "transcript": "Now let's try something creative - 'Write a creative one-sentence story about a time-traveling cat.' This is where higher temperature shines! At temperature 0, you'll get the same story every time, which defeats the purpose of creativity. But at 1.0 and 1.5, each run gives you genuinely different stories. Different plots, different styles. This is what you want for brainstorming, creative writing, anything where variety is valuable."
    },
    {
      "cell_index": 8,
      "cell_type": "code",
      "section": "3. Temperature in Practice",
      "concepts": ["list generation", "diversity in outputs"],
      "transcript": "Same thing with list generation - 'Name 3 unusual hobbies.' At temperature 0, you'll get the same hobbies every time. At higher temperatures, you'll see different suggestions each run. Really useful when you're trying to generate diverse options."
    },
    {
      "cell_index": 9,
      "cell_type": "markdown",
      "section": "4. Temperature Guidelines by Use Case",
      "transcript": "Let me give you a quick reference for temperature settings by use case. This table breaks it down nicely."
    },
    {
      "cell_index": 10,
      "cell_type": "code",
      "section": "4. Temperature Guidelines by Use Case",
      "concepts": ["use case guidelines", "code generation", "summarization", "creative writing", "brainstorming"],
      "transcript": "Let's see this in action with actual tasks. Code generation - temperature 0. We want deterministic, correct code. No creativity needed here, just accuracy. Email summary at temperature 0.3 - mostly factual, slight variation allowed for natural language. And creative marketing at temperature 0.9 - we want catchy, memorable, different options. See how the outputs match the use case? The code is clean and consistent, the summary is accurate, and the tagline has some flair to it."
    },
    {
      "cell_index": 11,
      "cell_type": "markdown",
      "section": "5. Temperature and Reproducibility",
      "transcript": "One more thing that's super important for production - reproducibility. Sometimes you need the same output every time, but you also want some variety."
    },
    {
      "cell_index": 12,
      "cell_type": "code",
      "section": "5. Temperature and Reproducibility",
      "concepts": ["reproducibility", "seed parameter", "deterministic outputs"],
      "transcript": "Check this out - with temperature 0, no seed needed, all three runs give 'Elysia Thornshadow'. Perfectly deterministic. Now here's the cool part - temperature 0.7 WITH seed=42, we get 'Elysia Thornwhisper' three times. Same every time because of the seed! But temperature 0.7 WITHOUT a seed? 'Elowen Thistledown', 'Thalindra Moonshadow', 'Elysia Thornweaver' - all different! So the seed parameter lets you have controlled randomness. Super useful for testing and debugging when you need reproducibility but still want some temperature variation."
    },
    {
      "cell_index": 13,
      "cell_type": "markdown",
      "section": "Summary",
      "transcript": "Let's recap! Temperature scales logits before softmax. T approaching 0 means deterministic, always picking the highest probability. T above 1 means more random, flatter distribution. For practical use: temperature 0 for factual, code, classification. 0.3 to 0.5 for summarization and Q&A. 0.7 for conversation. And 1.0 and above for creative tasks. Use temperature 0 or the seed parameter when you need reproducibility. Next up, check out mini-sampling to learn about Top-K and Top-P - they work together with temperature for even more control!"
    }
  ]
}
