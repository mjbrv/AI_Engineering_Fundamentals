{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Mini-Lab: Streaming Responses\n",
    "\n",
    "**Module 2: LLM Core Concepts** | **Duration: ~20 min** | **Type: Mini-Lab**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this mini-lab, you will be able to:\n",
    "\n",
    "1. **Understand** how autoregressive generation works (token-by-token)\n",
    "2. **Understand** how streaming differs from regular API responses\n",
    "3. **Implement** streaming with OpenAI's API\n",
    "4. **Process** tokens as they arrive in real-time\n",
    "5. **Build** interactive user experiences with streaming\n",
    "\n",
    "## Target Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| Autoregressive Generation | LLMs generate text one token at a time, each depending on previous tokens |\n",
    "| Streaming | Receiving tokens incrementally as they're generated |\n",
    "\n",
    "## üß† How LLMs Generate Text: Autoregressive Generation\n",
    "\n",
    "**LLMs generate text one token at a time.** This is called **autoregressive generation**:\n",
    "\n",
    "```\n",
    "Prompt: \"The capital of France is\"\n",
    "\n",
    "Step 1: Model predicts ‚Üí \"Paris\" (based on prompt)\n",
    "Step 2: Model predicts ‚Üí \".\" (based on prompt + \"Paris\")\n",
    "Step 3: Model predicts ‚Üí \"\\n\" (based on prompt + \"Paris\" + \".\")\n",
    "...and so on until a stop condition is met\n",
    "```\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "1. **Each token depends on ALL previous tokens** - the model \"reads\" everything before predicting the next word\n",
    "2. **Generation is sequential** - you can't generate token 5 before tokens 1-4\n",
    "3. **This is why streaming is possible** - we can send each token as it's generated\n",
    "4. **This is why context matters** - earlier tokens influence all later predictions\n",
    "\n",
    "### Visualizing the Process\n",
    "\n",
    "```\n",
    "Time ‚Üí\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ The ‚îÇ cap  ‚îÇ ital  ‚îÇ of   ‚îÇ Fr  ‚îÇance ‚îÇ  ‚Üê Input (prompt)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                        ‚Üì\n",
    "                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                    ‚îÇ is    ‚îÇ  Token 1 (generated)\n",
    "                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                            ‚Üì\n",
    "                                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                        ‚îÇ Paris ‚îÇ  Token 2 (generated)\n",
    "                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                                ‚Üì\n",
    "                                            ‚îå‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                            ‚îÇ . ‚îÇ  Token 3 (generated)\n",
    "                                            ‚îî‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Streaming exposes this token-by-token generation to your application!**\n",
    "\n",
    "## Why Streaming Matters\n",
    "\n",
    "- **Reduced perceived latency**: Users see output immediately\n",
    "- **Better UX**: Feels more interactive and responsive\n",
    "- **Early termination**: Can stop generation if output is wrong\n",
    "- **Real-time processing**: Process tokens as they arrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "def md(text):\n",
    "    display(Markdown(text))\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Non-Streaming vs Streaming Comparison\n",
    "\n",
    "First, let's compare the two approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Latency Comparison\n",
      "==================================================\n",
      "Prompt: Explain the concept of machine learning in 3 sentences.\n",
      "\n",
      "1Ô∏è‚É£ NON-STREAMING:\n",
      "   Time to first character: 1.71s\n",
      "   Total time: 1.71s\n",
      "   Response length: 464 chars\n",
      "\n",
      "2Ô∏è‚É£ STREAMING:\n",
      "   Time to first character: 0.31s\n",
      "   Total time: 1.51s\n",
      "   Response length: 473 chars\n",
      "\n",
      "‚ú® Streaming is 82% faster for first character!\n"
     ]
    }
   ],
   "source": [
    "def compare_latency(prompt):\n",
    "    \"\"\"Compare perceived latency between streaming and non-streaming.\"\"\"\n",
    "    \n",
    "    print(\"\\nüìä Latency Comparison\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    \n",
    "    # Non-streaming: measure time to first character\n",
    "    print(\"1Ô∏è‚É£ NON-STREAMING:\")\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=200,\n",
    "        stream=False\n",
    "    )\n",
    "    first_char_time = time.time() - start\n",
    "    total_time = first_char_time  # Same as total for non-streaming\n",
    "    content = response.choices[0].message.content\n",
    "    print(f\"   Time to first character: {first_char_time:.2f}s\")\n",
    "    print(f\"   Total time: {total_time:.2f}s\")\n",
    "    print(f\"   Response length: {len(content)} chars\")\n",
    "    \n",
    "    # Streaming: measure time to first token\n",
    "    print(\"\\n2Ô∏è‚É£ STREAMING:\")\n",
    "    start = time.time()\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=200,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    first_token_time = None\n",
    "    streamed_content = \"\"\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            if first_token_time is None:\n",
    "                first_token_time = time.time() - start\n",
    "            streamed_content += chunk.choices[0].delta.content\n",
    "    \n",
    "    total_time_stream = time.time() - start\n",
    "    \n",
    "    print(f\"   Time to first character: {first_token_time:.2f}s\")\n",
    "    print(f\"   Total time: {total_time_stream:.2f}s\")\n",
    "    print(f\"   Response length: {len(streamed_content)} chars\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    improvement = ((first_char_time - first_token_time) / first_char_time) * 100\n",
    "    print(f\"\\n‚ú® Streaming is {improvement:.0f}% faster for first character!\")\n",
    "\n",
    "compare_latency(\"Explain the concept of machine learning in 3 sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Streaming Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are five benefits of learning programming:\n",
       "\n",
       "1. **Problem-Solving Skills**: Programming enhances logical thinking and problem-solving abilities, enabling individuals to break down complex issues into manageable parts and find effective solutions.\n",
       "\n",
       "2. **Career Opportunities**: Proficiency in programming opens doors to numerous career paths in technology, software development, data analysis, and more, often accompanied by high demand and competitive salaries.\n",
       "\n",
       "3. **Creativity and Innovation**: Learning to code allows individuals to bring their ideas to life, fostering creativity and the ability to create unique applications, websites, and software solutions.\n",
       "\n",
       "4. **Understanding Technology**: Knowledge of programming helps individuals understand how technology works, making them more adept at utilizing and adapting to new technologies in an increasingly digital world.\n",
       "\n",
       "5. **Collaboration and Teamwork**: Programming often involves working in teams, leading to improved collaboration skills as individuals learn to communicate ideas and work together efficiently on projects."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def basic_streaming(prompt):\n",
    "    \"\"\"Basic streaming demonstration.\"\"\"\n",
    "    \n",
    "    print(f\"üìù Prompt: {prompt}\\n\")\n",
    "    print(\"üì§ Streaming response:\\n\")\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=200,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    for chunk in stream:\n",
    "        # Each chunk contains a delta (partial content)\n",
    "        if chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            # streming with print command\n",
    "            # print(content, end=\"\", flush=True)  # Print without newline\n",
    "            full_response += content\n",
    "\n",
    "            # streming with markdown command\n",
    "            # Clear and re-render for live update effect\n",
    "            clear_output(wait=True)\n",
    "            md(full_response)\n",
    "\n",
    "    print(\"\\n\")  # Final newline\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "basic_streaming(\"List 5 benefits of learning programming, briefly.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Streaming with Live Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt:** *What are the key principles of good software design?*\n",
       "\n",
       "---\n",
       "\n",
       "# Key Principles of Good Software Design\n",
       "\n",
       "Good software design is crucial for creating maintainable, scalable, and robust software applications. Here are some key principles to consider:\n",
       "\n",
       "## 1. Separation of Concerns\n",
       "- **Definition**: Divide a program into distinct sections, each addressing a separate concern or functionality.\n",
       "- **Benefit**: Enhances maintainability and allows for easier debugging and testing.\n",
       "\n",
       "## 2. DRY (Don't Repeat Yourself)\n",
       "- **Definition**: Avoid duplication of code and logic.\n",
       "- **Benefit**: Reduces redundancy, minimizes errors, and makes changes easier to implement across the codebase.\n",
       "\n",
       "## 3. KISS (Keep It Simple, Stupid)\n",
       "- **Definition**: Design systems in the simplest way possible; avoid over-complication.\n",
       "- **Benefit**: Simplifies understanding and reduces the chance of introducing bugs.\n",
       "\n",
       "## 4. YAGNI (You Aren't Gonna Need It)\n",
       "- **Definition**: Do not add functionality until it is necessary.\n",
       "- **Benefit**: Prevents bloat and ensures that the software remains focused on the actual requirements.\n",
       "\n",
       "## 5. Single Responsibility Principle (SRP)\n",
       "- **Definition**: A module or class should have only one reason to change, meaning it should only have one responsibility.\n",
       "- **Benefit**: Enhances code clarity and minimizes the impact of changes.\n",
       "\n",
       "## 6. Open/Closed Principle\n",
       "- **Definition**: Software entities (classes, modules, functions, etc"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def interactive_streaming(prompt):\n",
    "    \"\"\"Stream with live markdown rendering in Jupyter.\"\"\"\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt + \"\\n\\nFormat your response in markdown.\"}\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            full_response += content\n",
    "            \n",
    "            # Clear and re-render for live update effect\n",
    "            clear_output(wait=True)\n",
    "            md(f\"**Prompt:** *{prompt}*\\n\\n---\\n\\n{full_response}‚ñå\")\n",
    "    \n",
    "    # Final render without cursor\n",
    "    clear_output(wait=True)\n",
    "    md(f\"**Prompt:** *{prompt}*\\n\\n---\\n\\n{full_response}\")\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "interactive_streaming(\"What are the key principles of good software design?\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing Streamed Content\n",
    "\n",
    "Process tokens as they arrive for various use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Prompt: Explain the benefits of neural networks and deep learning for AI applications.\n",
      "üëÄ Watching for: ['neural', 'deep', 'learning', 'training', 'model']\n",
      "\n",
      "Ne\n",
      "üéØ Found 'neural'!\n",
      "ural networks and\n",
      "üéØ Found 'deep'!\n",
      " deep\n",
      "üéØ Found 'learning'!\n",
      " learning have revolutionized the field of artificial intelligence (AI) by providing powerful tools for tackling complex problems. Here are some of the key benefits of these technologies in AI applications:\n",
      "\n",
      "1. **High Performance on Complex Tasks**:\n",
      "   - Neural networks, especially deep learning\n",
      "üéØ Found 'model'!\n",
      " models, excel at handling complex tasks such as image recognition, natural language processing, and game playing. Their ability to model intricate patterns and relationships in data allows them to achieve state-of-the-art performance in various domains.\n",
      "\n",
      "2. **Automated Feature Extraction**:\n",
      "   - Traditional machine learning techniques often require manual feature engineering, which can be time-consuming and requires domain expertise. Deep learning models automatically learn to extract relevant features from raw data, reducing the need for manual intervention and allowing for more efficient model development.\n",
      "\n",
      "3. **Scalability**:\n",
      "   - Neural networks can scale effectively with the availability of data. As the amount of\n",
      "üéØ Found 'training'!\n",
      " training data increases, deep learning models can continue to improve their performance, making them\n",
      "\n",
      "üìä Summary: Found 5/5 watch words\n",
      "\n",
      "üìã Rendered Output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Neural networks and deep learning have revolutionized the field of artificial intelligence (AI) by providing powerful tools for tackling complex problems. Here are some of the key benefits of these technologies in AI applications:\n",
       "\n",
       "1. **High Performance on Complex Tasks**:\n",
       "   - Neural networks, especially deep learning models, excel at handling complex tasks such as image recognition, natural language processing, and game playing. Their ability to model intricate patterns and relationships in data allows them to achieve state-of-the-art performance in various domains.\n",
       "\n",
       "2. **Automated Feature Extraction**:\n",
       "   - Traditional machine learning techniques often require manual feature engineering, which can be time-consuming and requires domain expertise. Deep learning models automatically learn to extract relevant features from raw data, reducing the need for manual intervention and allowing for more efficient model development.\n",
       "\n",
       "3. **Scalability**:\n",
       "   - Neural networks can scale effectively with the availability of data. As the amount of training data increases, deep learning models can continue to improve their performance, making them"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('Neural networks and deep learning have revolutionized the field of artificial intelligence (AI) by providing powerful tools for tackling complex problems. Here are some of the key benefits of these technologies in AI applications:\\n\\n1. **High Performance on Complex Tasks**:\\n   - Neural networks, especially deep learning models, excel at handling complex tasks such as image recognition, natural language processing, and game playing. Their ability to model intricate patterns and relationships in data allows them to achieve state-of-the-art performance in various domains.\\n\\n2. **Automated Feature Extraction**:\\n   - Traditional machine learning techniques often require manual feature engineering, which can be time-consuming and requires domain expertise. Deep learning models automatically learn to extract relevant features from raw data, reducing the need for manual intervention and allowing for more efficient model development.\\n\\n3. **Scalability**:\\n   - Neural networks can scale effectively with the availability of data. As the amount of training data increases, deep learning models can continue to improve their performance, making them',\n",
       " ['neural', 'deep', 'learning', 'model', 'training'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stream_with_word_detection(prompt, watch_words):\n",
    "    \"\"\"Stream and detect specific words in real-time.\"\"\"\n",
    "    \n",
    "    print(f\"üìù Prompt: {prompt}\")\n",
    "    print(f\"üëÄ Watching for: {watch_words}\\n\")\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=200,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    found_words = []\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            full_response += content\n",
    "            \n",
    "            # Check for watch words\n",
    "            for word in watch_words:\n",
    "                if word.lower() in full_response.lower() and word not in found_words:\n",
    "                    found_words.append(word)\n",
    "                    print(f\"\\nüéØ Found '{word}'!\")\n",
    "            \n",
    "            print(content, end=\"\", flush=True)\n",
    "    \n",
    "    print(f\"\\n\\nüìä Summary: Found {len(found_words)}/{len(watch_words)} watch words\")\n",
    "    \n",
    "    # Display as rendered markdown for better readability\n",
    "    print(\"\\nüìã Rendered Output:\\n\")\n",
    "    md(full_response)\n",
    "    \n",
    "    return full_response, found_words\n",
    "\n",
    "stream_with_word_detection(\n",
    "    \"Explain the benefits of neural networks and deep learning for AI applications.\",\n",
    "    [\"neural\", \"deep\", \"learning\", \"training\", \"model\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Prompt: Count from 1 to 20, writing each number on a new line.\n",
      "üõë Will stop at: '10'\n",
      "\n",
      "Sure! Here you go:\n",
      "\n",
      "1  \n",
      "2  \n",
      "3  \n",
      "4  \n",
      "5  \n",
      "6  \n",
      "7  \n",
      "8  \n",
      "9  \n",
      "10\n",
      "\n",
      "‚èπÔ∏è Stopped: Found '10'\n"
     ]
    }
   ],
   "source": [
    "def stream_with_early_stop(prompt, stop_phrase):\n",
    "    \"\"\"Stream and stop when a specific phrase is detected.\"\"\"\n",
    "    \n",
    "    print(f\"üìù Prompt: {prompt}\")\n",
    "    print(f\"üõë Will stop at: '{stop_phrase}'\\n\")\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=500,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    stopped_early = False\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            full_response += content\n",
    "            print(content, end=\"\", flush=True)\n",
    "            \n",
    "            # Check for stop phrase\n",
    "            if stop_phrase.lower() in full_response.lower():\n",
    "                print(f\"\\n\\n‚èπÔ∏è Stopped: Found '{stop_phrase}'\")\n",
    "                stopped_early = True\n",
    "                break\n",
    "    \n",
    "    if not stopped_early:\n",
    "        print(\"\\n\\n‚úÖ Completed without finding stop phrase\")\n",
    "    \n",
    "    return full_response, stopped_early\n",
    "\n",
    "stream_with_early_stop(\n",
    "    \"Count from 1 to 20, writing each number on a new line.\",\n",
    "    stop_phrase=\"10\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Token Counting During Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In lines of logic, dreams take flight,  \n",
      "With keystrokes dancing, day and night.  \n",
      "A whisper of syntax, a spark of thought,  \n",
      "Building worlds with the codes that we‚Äôve wrought.  \n",
      "\n",
      "Loops and functions, a rhythm so clear,  \n",
      "Debugging the chaos, conquering fear.  \n",
      "Each semicolon like a heartbeat's pause,  \n",
      "In the realm of pixels, we‚Äôre the hidden cause.  \n",
      "\n",
      "From zero to one, creation's embrace,  \n",
      "In virtual landscapes, we find our place.  \n",
      "So here's to the coders, both near and far,  \n",
      "Crafting the future, one line‚Äîa star.\n",
      "\n",
      "==================================================\n",
      "üìä Streaming Statistics:\n",
      "   Chunks received: 132\n",
      "   Time to first token: 0.490s\n",
      "   Total time: 2.488s\n",
      "   Characters: 523\n",
      "   Prompt tokens: 14\n",
      "   Completion tokens: 129\n",
      "   Speed: 51.8 tokens/second\n",
      "\n",
      "==================================================\n",
      "üìã Rendered Output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In lines of logic, dreams take flight,  \n",
       "With keystrokes dancing, day and night.  \n",
       "A whisper of syntax, a spark of thought,  \n",
       "Building worlds with the codes that we‚Äôve wrought.  \n",
       "\n",
       "Loops and functions, a rhythm so clear,  \n",
       "Debugging the chaos, conquering fear.  \n",
       "Each semicolon like a heartbeat's pause,  \n",
       "In the realm of pixels, we‚Äôre the hidden cause.  \n",
       "\n",
       "From zero to one, creation's embrace,  \n",
       "In virtual landscapes, we find our place.  \n",
       "So here's to the coders, both near and far,  \n",
       "Crafting the future, one line‚Äîa star."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def stream_with_stats(prompt, max_tokens=200):\n",
    "    \"\"\"Stream with real-time statistics.\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        stream=True,\n",
    "        stream_options={\"include_usage\": True}  # Get token counts\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    chunk_count = 0\n",
    "    first_token_time = None\n",
    "    usage = None\n",
    "    \n",
    "    for chunk in stream:\n",
    "        chunk_count += 1\n",
    "        \n",
    "        # Capture usage from final chunk\n",
    "        if chunk.usage:\n",
    "            usage = chunk.usage\n",
    "        \n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            if first_token_time is None:\n",
    "                first_token_time = time.time() - start_time\n",
    "            \n",
    "            content = chunk.choices[0].delta.content\n",
    "            full_response += content\n",
    "            print(content, end=\"\", flush=True)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\\n{'='*50}\")\n",
    "    print(\"üìä Streaming Statistics:\")\n",
    "    print(f\"   Chunks received: {chunk_count}\")\n",
    "    print(f\"   Time to first token: {first_token_time:.3f}s\")\n",
    "    print(f\"   Total time: {total_time:.3f}s\")\n",
    "    print(f\"   Characters: {len(full_response)}\")\n",
    "    \n",
    "    if usage:\n",
    "        print(f\"   Prompt tokens: {usage.prompt_tokens}\")\n",
    "        print(f\"   Completion tokens: {usage.completion_tokens}\")\n",
    "        tokens_per_second = usage.completion_tokens / total_time\n",
    "        print(f\"   Speed: {tokens_per_second:.1f} tokens/second\")\n",
    "    \n",
    "    # Display as rendered markdown for better readability\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"üìã Rendered Output:\\n\")\n",
    "    md(full_response)\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "stream_with_stats(\"Write a short poem about coding.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building a Chat Interface with Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ You: What is a function in programming?\n",
      "ü§ñ Assistant: "
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A function in programming is a reusable block of code that performs a specific task. It can take inputs, known as parameters, and may return a result. Functions help organize code, improve readability, and facilitate code reuse."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ You: Can you show a simple example in Python?\n",
      "ü§ñ Assistant: "
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Certainly! Here‚Äôs a simple example of a function in Python that adds two numbers:\n",
       "\n",
       "```python\n",
       "def add_numbers(a, b):\n",
       "    return a + b\n",
       "\n",
       "# Using the function\n",
       "result = add_numbers(3, 5)\n",
       "print(result)  # Output: 8\n",
       "```\n",
       "\n",
       "In this example, `add_numbers` is a function that takes two parameters, `a` and `b`, and returns their sum."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class StreamingChat:\n",
    "    \"\"\"Simple streaming chat interface.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt=\"You are a helpful assistant.\"):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        self.client = OpenAI()\n",
    "    \n",
    "    def chat(self, user_message):\n",
    "        \"\"\"Send message and stream response.\"\"\"\n",
    "        \n",
    "        # Add user message\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        print(f\"üë§ You: {user_message}\")\n",
    "        print(f\"ü§ñ Assistant: \", end=\"\")\n",
    "        \n",
    "        # Stream response\n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=self.messages,\n",
    "            max_tokens=300,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        assistant_response = \"\"\n",
    "        \n",
    "        # Create a display handle for live updates\n",
    "        display_handle = display(Markdown(\"\"), display_id=True)\n",
    "        \n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                assistant_response += content\n",
    "                \n",
    "                # Update the specific display without clearing other output\n",
    "                display_handle.update(Markdown(assistant_response + \"‚ñå\"))\n",
    "        \n",
    "        # Final update without cursor\n",
    "        display_handle.update(Markdown(assistant_response))\n",
    "        print()  # Add spacing after response\n",
    "        \n",
    "        # Add assistant response to history\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "        \n",
    "        return assistant_response\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history, keeping system prompt.\"\"\"\n",
    "        self.messages = [self.messages[0]]\n",
    "        print(\"üóëÔ∏è History cleared\")\n",
    "\n",
    "chat = StreamingChat(\"You are a helpful coding tutor. Keep responses concise.\")\n",
    "chat.chat(\"What is a function in programming?\");\n",
    "chat.chat(\"Can you show a simple example in Python?\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ You: What about functions with parameters?\n",
      "ü§ñ Assistant: "
     ]
    },
    {
     "data": {
      "text/markdown": [
       "It seems like you might be asking for a deeper exploration of functions with parameters. Here's a more detailed look at the topic:\n",
       "\n",
       "### Types of Parameters in Functions\n",
       "\n",
       "1. **Required Parameters**: These must be provided when the function is called.\n",
       "2. **Optional Parameters**: You can provide default values.\n",
       "3. **Variable-Length Parameters**: You can allow for an arbitrary number of arguments.\n",
       "\n",
       "### Examples\n",
       "\n",
       "1. **Required Parameters**:\n",
       "\n",
       "   ```python\n",
       "   def subtract(a, b):\n",
       "       return a - b\n",
       "\n",
       "   result = subtract(10, 4)  # Output: 6\n",
       "   ```\n",
       "\n",
       "2. **Optional Parameters** (with default values):\n",
       "\n",
       "   ```python\n",
       "   def multiply(a, b=2):\n",
       "       return a * b\n",
       "\n",
       "   print(multiply(5))    # Output: 10 (5 * 2)\n",
       "   print(multiply(5, 3)) # Output: 15 (5 * 3)\n",
       "   ```\n",
       "\n",
       "3. **Variable-Length Parameters**:\n",
       "\n",
       "   ```python\n",
       "   def summarize(*args):\n",
       "       return sum(args)\n",
       "\n",
       "   print(summarize(1, 2, 3))          # Output: 6\n",
       "   print(summarize(1, 2, 3, 4, 5))    # Output: 15\n",
       "   ```\n",
       "\n",
       "### Explanation\n",
       "\n",
       "- **Required Parameters**: The function must receive exactly the specified parameters.\n",
       "- **Optional Parameters**: If an"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chat.chat(\"What about functions with parameters?\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Streaming Benefits**\n",
    "   - Faster perceived response time\n",
    "   - Better user experience\n",
    "   - Ability to process/stop early\n",
    "\n",
    "2. **Implementation**\n",
    "   - Set `stream=True` in API call\n",
    "   - Iterate over chunks\n",
    "   - Access content via `chunk.choices[0].delta.content`\n",
    "\n",
    "3. **Common Patterns**\n",
    "   - Live markdown rendering\n",
    "   - Word/phrase detection\n",
    "   - Early termination\n",
    "   - Token counting with `stream_options`\n",
    "\n",
    "4. **Best Practices**\n",
    "   - Always use `flush=True` when printing\n",
    "   - Handle `None` content in chunks\n",
    "   - Track both chunks and content\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **mini-model-compare**: Compare streaming across models\n",
    "- **lab-llm-playground**: Build complete interactive playground\n",
    "- **Module 9**: Implement streaming in production APIs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_llm_agent",
   "language": "python",
   "name": "text_llm_agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
