{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Mini-Lab: Sampling Strategies\n",
    "\n",
    "**Module 2: LLM Core Concepts** | **Duration: ~45 min** | **Type: Mini-Lab**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this mini-lab, you will be able to:\n",
    "\n",
    "1. **Understand** how Top-K and Top-P (nucleus) sampling work\n",
    "2. **Understand** what beam search is and why it's used\n",
    "3. **Compare** different sampling strategies visually\n",
    "4. **Combine** temperature with sampling for fine-grained control\n",
    "5. **Choose** optimal sampling settings for different tasks\n",
    "\n",
    "## Target Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| Top-K Sampling | Sample from only the K most likely tokens |\n",
    "| Top-P (Nucleus) Sampling | Sample from smallest set of tokens with cumulative probability ‚â• P |\n",
    "| Beam Search | Maintain multiple candidate sequences to find better overall outputs |\n",
    "| Temperature | Scaling factor for probability distribution (prerequisite) |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **mini-temperature**: Understanding of temperature parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "def md(text):\n",
    "    display(Markdown(text))\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Top-K Sampling\n",
    "\n",
    "**Top-K** limits selection to the K highest-probability tokens:\n",
    "\n",
    "```\n",
    "Original distribution:  [the: 30%, a: 25%, one: 15%, that: 10%, it: 8%, ...]\n",
    "                              ‚Üì Top-K=3\n",
    "Filtered distribution:  [the: 43%, a: 36%, one: 21%]  (renormalized)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Top-K Sampling (K=3)\n",
      "==================================================\n",
      "\n",
      "Original distribution:\n",
      "  the       : 36.4% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üëà\n",
      "  a         : 27.0% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üëà\n",
      "  one       : 18.1% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üëà\n",
      "  that      :  8.1% ‚ñà‚ñà‚ñà ‚úó\n",
      "  it        :  4.9% ‚ñà ‚úó\n",
      "  which     :  3.7% ‚ñà ‚úó\n",
      "  those     :  1.8%  ‚úó\n",
      "\n",
      "After Top-K=3 (renormalized):\n",
      "  the       : 44.7% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  a         : 33.1% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  one       : 22.2% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üéØ Top-K Sampling (K=5)\n",
      "==================================================\n",
      "\n",
      "Original distribution:\n",
      "  the       : 36.4% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üëà\n",
      "  a         : 27.0% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üëà\n",
      "  one       : 18.1% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üëà\n",
      "  that      :  8.1% ‚ñà‚ñà‚ñà üëà\n",
      "  it        :  4.9% ‚ñà üëà\n",
      "  which     :  3.7% ‚ñà ‚úó\n",
      "  those     :  1.8%  ‚úó\n",
      "\n",
      "After Top-K=5 (renormalized):\n",
      "  the       : 38.5% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  a         : 28.5% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  one       : 19.1% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  that      :  8.6% ‚ñà‚ñà‚ñà\n",
      "  it        :  5.2% ‚ñà‚ñà\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4]),\n",
       " array([0.38522746, 0.28538352, 0.19129829, 0.08595586, 0.05213487]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simulate_top_k(logits, token_names, k):\n",
    "    \"\"\"Simulate Top-K sampling.\"\"\"\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    \n",
    "    # Sort by probability (descending)\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    \n",
    "    # Keep only top-k\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    top_k_probs = probs[top_k_indices]\n",
    "    top_k_probs = top_k_probs / np.sum(top_k_probs)  # Renormalize\n",
    "    \n",
    "    print(f\"\\nüéØ Top-K Sampling (K={k})\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nOriginal distribution:\")\n",
    "    for i in sorted_indices:\n",
    "        bar = \"‚ñà\" * int(probs[i] * 40)\n",
    "        mark = \" üëà\" if i in top_k_indices else \" ‚úó\"\n",
    "        print(f\"  {token_names[i]:10s}: {probs[i]:5.1%} {bar}{mark}\")\n",
    "    \n",
    "    print(f\"\\nAfter Top-K={k} (renormalized):\")\n",
    "    for idx, prob in zip(top_k_indices, top_k_probs):\n",
    "        bar = \"‚ñà\" * int(prob * 40)\n",
    "        print(f\"  {token_names[idx]:10s}: {prob:5.1%} {bar}\")\n",
    "    \n",
    "    return top_k_indices, top_k_probs\n",
    "\n",
    "# Example token distribution\n",
    "logits = np.array([2.5, 2.2, 1.8, 1.0, 0.5, 0.2, -0.5])\n",
    "tokens = [\"the\", \"a\", \"one\", \"that\", \"it\", \"which\", \"those\"]\n",
    "\n",
    "simulate_top_k(logits, tokens, k=3)\n",
    "simulate_top_k(logits, tokens, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Top-P (Nucleus) Sampling\n",
    "\n",
    "**Top-P** includes the smallest set of tokens whose cumulative probability exceeds P:\n",
    "\n",
    "```\n",
    "Sorted probs:    [the: 30%, a: 25%, one: 15%, that: 10%, it: 8%, ...]\n",
    "Cumulative:      [30%,      55%,     70%,      80%,      88%, ...]\n",
    "                              ‚Üì Top-P=0.7\n",
    "Included:        [the: 30%, a: 25%, one: 15%]  ‚Üí cumulative ‚â• 70%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Top-P Sampling (P=0.5)\n",
      "==================================================\n",
      "\n",
      "Original distribution with cumulative:\n",
      "  the       : 36.4% (cum: 36.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üëà\n",
      "  a         : 27.0% (cum: 63.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üëà\n",
      "  one       : 18.1% (cum: 81.5%) ‚ñà‚ñà‚ñà‚ñà‚ñà ‚úó\n",
      "  that      :  8.1% (cum: 89.6%) ‚ñà‚ñà ‚úó\n",
      "  it        :  4.9% (cum: 94.5%) ‚ñà ‚úó\n",
      "  which     :  3.7% (cum: 98.2%) ‚ñà ‚úó\n",
      "  those     :  1.8% (cum: 100.0%)  ‚úó\n",
      "\n",
      "After Top-P=0.5 (renormalized) - 2 tokens:\n",
      "  the       : 57.4% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  a         : 42.6% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üéØ Top-P Sampling (P=0.9)\n",
      "==================================================\n",
      "\n",
      "Original distribution with cumulative:\n",
      "  the       : 36.4% (cum: 36.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üëà\n",
      "  a         : 27.0% (cum: 63.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üëà\n",
      "  one       : 18.1% (cum: 81.5%) ‚ñà‚ñà‚ñà‚ñà‚ñà üëà\n",
      "  that      :  8.1% (cum: 89.6%) ‚ñà‚ñà üëà\n",
      "  it        :  4.9% (cum: 94.5%) ‚ñà üëà\n",
      "  which     :  3.7% (cum: 98.2%) ‚ñà ‚úó\n",
      "  those     :  1.8% (cum: 100.0%)  ‚úó\n",
      "\n",
      "After Top-P=0.9 (renormalized) - 5 tokens:\n",
      "  the       : 38.5% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  a         : 28.5% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  one       : 19.1% ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  that      :  8.6% ‚ñà‚ñà\n",
      "  it        :  5.2% ‚ñà\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4]),\n",
       " array([0.38522746, 0.28538352, 0.19129829, 0.08595586, 0.05213487]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simulate_top_p(logits, token_names, p):\n",
    "    \"\"\"Simulate Top-P (nucleus) sampling.\"\"\"\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    \n",
    "    # Sort by probability (descending)\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    sorted_probs = probs[sorted_indices]\n",
    "    \n",
    "    # Calculate cumulative probability\n",
    "    cumsum = np.cumsum(sorted_probs)\n",
    "    \n",
    "    # Find cutoff\n",
    "    cutoff_idx = np.searchsorted(cumsum, p) + 1\n",
    "    top_p_indices = sorted_indices[:cutoff_idx]\n",
    "    top_p_probs = probs[top_p_indices]\n",
    "    top_p_probs = top_p_probs / np.sum(top_p_probs)  # Renormalize\n",
    "    \n",
    "    print(f\"\\nüéØ Top-P Sampling (P={p})\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nOriginal distribution with cumulative:\")\n",
    "    running_sum = 0\n",
    "    for i, idx in enumerate(sorted_indices):\n",
    "        running_sum += probs[idx]\n",
    "        bar = \"‚ñà\" * int(probs[idx] * 30)\n",
    "        mark = \" üëà\" if idx in top_p_indices else \" ‚úó\"\n",
    "        print(f\"  {token_names[idx]:10s}: {probs[idx]:5.1%} (cum: {running_sum:5.1%}) {bar}{mark}\")\n",
    "    \n",
    "    print(f\"\\nAfter Top-P={p} (renormalized) - {len(top_p_indices)} tokens:\")\n",
    "    for idx, prob in zip(top_p_indices, top_p_probs):\n",
    "        bar = \"‚ñà\" * int(prob * 30)\n",
    "        print(f\"  {token_names[idx]:10s}: {prob:5.1%} {bar}\")\n",
    "    \n",
    "    return top_p_indices, top_p_probs\n",
    "\n",
    "simulate_top_p(logits, tokens, p=0.5)\n",
    "simulate_top_p(logits, tokens, p=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top-K vs Top-P: Key Differences\n",
    "\n",
    "| Aspect | Top-K | Top-P |\n",
    "|--------|-------|-------|\n",
    "| **Selection** | Fixed number of tokens | Variable (based on probability) |\n",
    "| **Adapts to** | Nothing (always K tokens) | Distribution shape |\n",
    "| **Peaked dist.** | May include low-prob tokens | Naturally excludes them |\n",
    "| **Flat dist.** | May exclude reasonable options | Naturally includes them |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Distribution Comparison\n",
      "============================================================\n",
      "\n",
      "üî∫ PEAKED Distribution (one dominant answer):\n",
      "  token_1: 86.8% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  token_2:  4.3% ‚ñà\n",
      "  token_3:  2.6% ‚ñà\n",
      "  token_4:  1.9% \n",
      "  token_5:  1.8% \n",
      "  token_6:  1.6% \n",
      "  token_7:  1.0% \n",
      "\n",
      "  Top-K=3 would include: tokens 1-3\n",
      "  Top-P=0.9 would include: ~1 token (token_1 alone is ~73%)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîπ FLAT Distribution (many viable answers):\n",
      "  token_1: 18.9% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  token_2: 17.1% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  token_3: 15.5% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  token_4: 14.0% ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  token_5: 12.7% ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  token_6: 11.5% ‚ñà‚ñà‚ñà‚ñà\n",
      "  token_7: 10.4% ‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "  Top-K=3 would include: tokens 1-3 only\n",
      "  Top-P=0.9 would include: ~6 tokens (need many to reach 90%)\n",
      "\n",
      "üí° Insight: Top-P adapts to distribution shape, Top-K does not!\n"
     ]
    }
   ],
   "source": [
    "def compare_distributions():\n",
    "    \"\"\"Compare Top-K and Top-P on different distribution shapes.\"\"\"\n",
    "    \n",
    "    # Peaked distribution (one dominant choice)\n",
    "    peaked_logits = np.array([4.0, 1.0, 0.5, 0.2, 0.1, 0.0, -0.5])\n",
    "    peaked_probs = np.exp(peaked_logits) / np.sum(np.exp(peaked_logits))\n",
    "    \n",
    "    # Flat distribution (many viable choices)\n",
    "    flat_logits = np.array([1.2, 1.1, 1.0, 0.9, 0.8, 0.7, 0.6])\n",
    "    flat_probs = np.exp(flat_logits) / np.sum(np.exp(flat_logits))\n",
    "    \n",
    "    tokens = [\"token_1\", \"token_2\", \"token_3\", \"token_4\", \"token_5\", \"token_6\", \"token_7\"]\n",
    "    \n",
    "    print(\"\\nüìä Distribution Comparison\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüî∫ PEAKED Distribution (one dominant answer):\")\n",
    "    for name, prob in zip(tokens, peaked_probs):\n",
    "        bar = \"‚ñà\" * int(prob * 40)\n",
    "        print(f\"  {name}: {prob:5.1%} {bar}\")\n",
    "    \n",
    "    print(f\"\\n  Top-K=3 would include: tokens 1-3\")\n",
    "    print(f\"  Top-P=0.9 would include: ~1 token (token_1 alone is ~73%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    \n",
    "    print(\"\\nüîπ FLAT Distribution (many viable answers):\")\n",
    "    for name, prob in zip(tokens, flat_probs):\n",
    "        bar = \"‚ñà\" * int(prob * 40)\n",
    "        print(f\"  {name}: {prob:5.1%} {bar}\")\n",
    "    \n",
    "    print(f\"\\n  Top-K=3 would include: tokens 1-3 only\")\n",
    "    print(f\"  Top-P=0.9 would include: ~6 tokens (need many to reach 90%)\")\n",
    "    \n",
    "    print(\"\\nüí° Insight: Top-P adapts to distribution shape, Top-K does not!\")\n",
    "\n",
    "compare_distributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combining Temperature + Sampling\n",
    "\n",
    "Temperature and sampling work together:\n",
    "1. Temperature first modifies the probability distribution\n",
    "2. Then Top-K/Top-P filters the modified distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üìù Prompt: *Complete this sentence creatively: 'The robot discovered that'*\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Conservative** (T=0.3, top_p=0.5) - üåà 3 unique"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 1. 'the key to understanding human emotions lay not in algorithms or data, but in the subtle nuances of a shared silence, where a single heartbeat could speak"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 2. 'the key to understanding human emotions lay not in algorithms or data, but in the subtle nuances of a shared smile and the warmth of a gentle touch"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 3. 'the key to understanding human emotions lay not in algorithms or data, but in the subtle nuances of a shared laugh, the warmth of a gentle touch"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Balanced** (T=0.7, top_p=0.9) - üåà 3 unique"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 1. 'the key to understanding human emotions was hidden in the subtle variations of their laughter, a symphony of joy and sorrow that echoed through its circuits,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 2. 'the key to understanding humanity wasn't in its circuits or algorithms, but in the warmth of a shared laugh and the quiet moments of vulnerability that echoed through"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 3. 'the key to understanding human emotions lay not in logic, but in the subtle nuances of laughter, the warmth of a shared glance, and the bitters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Creative** (T=1.0, top_p=0.95) - üåà 3 unique"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 1. 'the key to understanding human emotions lay not in algorithms or data, but in the delicate dance of shared laughter, the warmth of a gentle touch,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 2. 'the most complex code it could ever decipher was not a string of algorithms, but the chaotic, beautiful dance of human emotions, swirling like colors in"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 3. 'the key to unlocking human emotions lay not in algorithms or data, but in the gentle resonance of a shared melody, echoing through the circuits of"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Wild** (T=1.2, top_p=1.0) - üåà 3 unique"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 1. 'the hidden algorithm buried deep within its code was not just a set of instructions, but a symphony of looping thoughts, whispering secrets of creativity"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 2. 'the key to understanding humanity lay not in data and algorithms, but in the fleeting moments of laughter shared under a starlit sky, where code"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 3. 'the melody in the creaking floors of the abandoned house was not just a product of decay, but a symphony crafted by the whispers of forgotten"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_combined_settings(prompt, settings):\n",
    "    \"\"\"Test different combinations of temperature and top_p.\"\"\"\n",
    "    \n",
    "    md(f\"### üìù Prompt: *{prompt}*\\n\\n---\")\n",
    "    \n",
    "    for name, temp, top_p in settings:\n",
    "        outputs = []\n",
    "        \n",
    "        for _ in range(3):\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temp,\n",
    "                top_p=top_p,\n",
    "                max_tokens=30\n",
    "            )\n",
    "            outputs.append(response.choices[0].message.content.strip())\n",
    "        \n",
    "        # Check diversity\n",
    "        unique = len(set(outputs))\n",
    "        diversity = \"üéØ Same\" if unique == 1 else f\"üåà {unique} unique\"\n",
    "        \n",
    "        md(f\"**{name}** (T={temp}, top_p={top_p}) - {diversity}\")\n",
    "        for i, out in enumerate(outputs, 1):\n",
    "            md(f\"> {i}. {out}\")\n",
    "        md(\"\")\n",
    "\n",
    "# Test combinations\n",
    "settings = [\n",
    "    (\"Conservative\", 0.3, 0.5),    # Low temp + low top_p\n",
    "    (\"Balanced\", 0.7, 0.9),         # Medium both\n",
    "    (\"Creative\", 1.0, 0.95),        # High temp + high top_p  \n",
    "    (\"Wild\", 1.2, 1.0),             # High temp + no filtering\n",
    "]\n",
    "\n",
    "test_combined_settings(\n",
    "    \"Complete this sentence creatively: 'The robot discovered that'\",\n",
    "    settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World Sampling Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üîß Code Completion"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Settings:** T=0, top_p=1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Reason: Deterministic, correct code needed*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt:** Complete this Python code:\n",
       "def calculate_average(n...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Output:**\n",
       "```\n",
       "Certainly! Below is a complete implementation of the `calculate_average` function in Python. This function takes a list of numbers as input and returns their average.\n",
       "\n",
       "```python\n",
       "def calculate_average(numbers):\n",
       "    if not numbers:  # Check if the list is empty\n",
       "        return 0  # Return 0 or you could raise an exception if preferred\n",
       "    total = sum(numbers)  # Calculate the sum of the numbers\n",
       "    count = len(numbers)  # Get the count of numbers\n",
       "    average = total / count  # Calculate the average\n",
       "    return average  # Return the average\n",
       "```\n",
       "\n",
       "### Example Usage:\n",
       "```python\n",
       "numbers = [10, 20, 30, 40, 50]\n",
       "average = calculate\n",
       "```\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### üìù Content Generation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Settings:** T=0.7, top_p=0.9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Reason: Creative but coherent marketing copy*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt:** Write a product description for wireless earbuds....\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Output:**\n",
       "```\n",
       "**Product Description: Wireless Freedom Earbuds**\n",
       "\n",
       "Experience sound like never before with our Wireless Freedom Earbuds, designed for those who crave quality audio and unparalleled convenience. These state-of-the-art earbuds combine cutting-edge technology with a sleek, modern design, making them the perfect companion for your everyday adventures.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "- **Crystal Clear Sound:** Enjoy rich, high-fidelity audio with deep bass and crisp treble. Our advanced sound technology ensures that every note is delivered with precision, making your music, podcasts, and calls sound incredible.\n",
       "\n",
       "- **True Wireless Design:** Say goodbye to tangled wires! Our earbuds feature a completely wireless design that offers the freedom to move without restrictions. Whether you‚Äôre at the gym, commuting, or relaxing at home\n",
       "```\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### üß† Brainstorming"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Settings:** T=1.0, top_p=0.95"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Reason: Maximum diversity and novelty*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt:** Give me 3 unique startup ideas combining AI and co...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Output:**\n",
       "```\n",
       "Sure! Here are three unique startup ideas that combine AI and cooking:\n",
       "\n",
       "1. **AI-Powered Meal Personalization Platform**:\n",
       "   Create a platform that utilizes AI algorithms to analyze users' dietary preferences, nutritional goals, allergies, and cooking skill levels to generate personalized meal plans and recipes. The platform could feature an interactive interface where users can input their available ingredients and preferences, and the AI will suggest recipes, portion sizes, and cooking instructions tailored to their needs. Additionally, the platform could offer grocery delivery services for ingredients and integrate with smart kitchen devices for seamless cooking experiences.\n",
       "\n",
       "2. **Smart Cooking Assistant App**:\n",
       "   Develop a mobile app that acts as a smart cooking assistant using AI and voice recognition technology. The app could guide users through recipes\n",
       "```\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### üìä Data Extraction"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Settings:** T=0, top_p=0.1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Reason: Single correct answer, no creativity*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt:** Extract the date from: 'Meeting scheduled for Janu...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Output:**\n",
       "```\n",
       "The date extracted from the text is January 15th, 2024.\n",
       "```\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def demonstrate_scenarios():\n",
    "    \"\"\"Show optimal settings for different real-world scenarios.\"\"\"\n",
    "    \n",
    "    scenarios = [\n",
    "        {\n",
    "            \"name\": \"üîß Code Completion\",\n",
    "            \"prompt\": \"Complete this Python code:\\ndef calculate_average(numbers):\\n    \",\n",
    "            \"temp\": 0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"reason\": \"Deterministic, correct code needed\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"üìù Content Generation\",\n",
    "            \"prompt\": \"Write a product description for wireless earbuds.\",\n",
    "            \"temp\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"reason\": \"Creative but coherent marketing copy\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"üß† Brainstorming\",\n",
    "            \"prompt\": \"Give me 3 unique startup ideas combining AI and cooking.\",\n",
    "            \"temp\": 1.0,\n",
    "            \"top_p\": 0.95,\n",
    "            \"reason\": \"Maximum diversity and novelty\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"üìä Data Extraction\",\n",
    "            \"prompt\": \"Extract the date from: 'Meeting scheduled for January 15th, 2024'\",\n",
    "            \"temp\": 0,\n",
    "            \"top_p\": 0.1,\n",
    "            \"reason\": \"Single correct answer, no creativity\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": scenario[\"prompt\"]}],\n",
    "            temperature=scenario[\"temp\"],\n",
    "            top_p=scenario[\"top_p\"],\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        md(f\"### {scenario['name']}\")\n",
    "        md(f\"**Settings:** T={scenario['temp']}, top_p={scenario['top_p']}\")\n",
    "        md(f\"*Reason: {scenario['reason']}*\\n\")\n",
    "        md(f\"**Prompt:** {scenario['prompt'][:50]}...\\n\")\n",
    "        md(f\"**Output:**\\n```\\n{response.choices[0].message.content}\\n```\\n\\n---\")\n",
    "\n",
    "demonstrate_scenarios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Reference: Sampling Settings\n",
    "\n",
    "| Task Type | Temperature | Top-P | Notes |\n",
    "|-----------|-------------|-------|-------|\n",
    "| **Factual Q&A** | 0 | 1.0 | Let temp handle it |\n",
    "| **Code** | 0-0.2 | 1.0 | Deterministic |\n",
    "| **Translation** | 0.3 | 0.9 | Some flexibility |\n",
    "| **Summarization** | 0.5 | 0.9 | Balanced |\n",
    "| **Conversation** | 0.7 | 0.9 | Natural variation |\n",
    "| **Creative Writing** | 0.9-1.0 | 0.95 | High diversity |\n",
    "| **Brainstorming** | 1.0-1.2 | 0.95-1.0 | Maximum exploration |\n",
    "\n",
    "### Pro Tips\n",
    "\n",
    "1. **Start with temperature**: It's the primary control for creativity\n",
    "2. **Add top_p for safety**: Lower top_p prevents wild outliers\n",
    "3. **Don't double-restrict**: If using low temp, keep top_p high (or vice versa)\n",
    "4. **Test with your data**: Optimal settings vary by domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Experiment: T=0.9, top_p=0.95\n",
      "üìù Prompt: Give me a creative metaphor for learning to code....\n",
      "==================================================\n",
      "\n",
      "Run 1:\n",
      "Learning to code is like planting a garden. At first, the soil feels foreign and the seeds seem tiny and insignificant. You dig in, nurturing your understanding of the tools‚Äîspades and trowels‚Äîjust as you grasp programming languages and syntax. With patience, you water your knowledge, and gradually, ideas sprout into vibrant code. Some plants might wither due to bugs or errors, but with each failure, you learn to tend more carefully, cultivating resilience. Over time, your garden\n",
      "\n",
      "Run 2:\n",
      "Learning to code is like tending to a garden. At first, the soil is barren and unfamiliar, but with patience and care, you plant seeds of knowledge‚Äîeach line of code is a seedling. As you nurture them with practice and curiosity, they begin to sprout into vibrant ideas and intricate solutions. Some may wither, but each failure teaches you how to cultivate better techniques. Over time, your once empty patch transforms into a thriving ecosystem of logic and creativity, where every flower represents a\n",
      "\n",
      "Run 3:\n",
      "Learning to code is like planting a garden. At first, the soil seems unyielding and the seeds‚Äîyour ideas‚Äîappear small and fragile. You dig deep, nurture the ground with knowledge, and patiently water your budding skills with practice. With time and persistence, the garden flourishes; lines of code bloom into vibrant applications, each one a testament to your labor. Just as a gardener learns to adapt to seasons and weather, a coder navigates challenges and evolves, cultivating their craft until it\n"
     ]
    }
   ],
   "source": [
    "# Create a function for easy experimentation\n",
    "def experiment(prompt, temp=0.7, top_p=0.9, n=3):\n",
    "    \"\"\"Easy experimentation with sampling parameters.\"\"\"\n",
    "    \n",
    "    print(f\"\\nüß™ Experiment: T={temp}, top_p={top_p}\")\n",
    "    print(f\"üìù Prompt: {prompt[:60]}...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temp,\n",
    "            top_p=top_p,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        print(f\"\\nRun {i+1}:\")\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "# Try it yourself!\n",
    "experiment(\n",
    "    \"Give me a creative metaphor for learning to code.\",\n",
    "    temp=0.9,\n",
    "    top_p=0.95,\n",
    "    n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Beam Search (Conceptual)\n",
    "\n",
    "**Beam Search** is an alternative to sampling that maintains multiple candidate sequences:\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Instead of generating one token at a time (greedy) or sampling randomly, beam search:\n",
    "1. Keep track of the top `B` (beam width) most likely sequences\n",
    "2. At each step, expand each sequence with all possible next tokens\n",
    "3. Keep only the top `B` overall sequences\n",
    "4. Return the highest-scoring complete sequence\n",
    "\n",
    "```\n",
    "Beam Width = 2\n",
    "\n",
    "Step 1: \"The\" ‚Üí expand to all possible next tokens\n",
    "        Keep top 2: [\"The cat\", \"The dog\"]\n",
    "\n",
    "Step 2: Expand each:\n",
    "        \"The cat\" ‚Üí [\"The cat sat\", \"The cat ran\", ...]\n",
    "        \"The dog\" ‚Üí [\"The dog barked\", \"The dog ran\", ...]\n",
    "        Keep top 2 overall: [\"The cat sat\", \"The dog barked\"]\n",
    "\n",
    "...continue until done\n",
    "```\n",
    "\n",
    "### Beam Search vs Sampling\n",
    "\n",
    "| Aspect | Beam Search | Sampling (Top-K/Top-P) |\n",
    "|--------|-------------|------------------------|\n",
    "| **Determinism** | Deterministic (same input ‚Üí same output) | Stochastic (varies each time) |\n",
    "| **Quality** | Finds globally better sequences | Greedy/local decisions |\n",
    "| **Diversity** | Low (often repetitive) | High (with right settings) |\n",
    "| **Use Case** | Translation, summarization | Creative writing, chat |\n",
    "| **Compute** | Higher (maintains B sequences) | Lower (one sequence) |\n",
    "\n",
    "### Why OpenAI Doesn't Expose Beam Search\n",
    "\n",
    "Modern chat models like GPT-4 use **sampling** instead of beam search because:\n",
    "1. **Diversity**: Beam search tends to produce repetitive, \"safe\" outputs\n",
    "2. **Creativity**: Sampling allows for more interesting, varied responses\n",
    "3. **Chat UX**: Users expect different responses to the same prompt\n",
    "4. **Efficiency**: Sampling is faster for interactive applications\n",
    "\n",
    "### When Beam Search is Still Used\n",
    "\n",
    "- **Machine Translation**: Finding the most accurate translation\n",
    "- **Speech Recognition**: Decoding audio to text\n",
    "- **Code Generation**: When correctness matters more than creativity\n",
    "- **Specialized Models**: Some local models still expose beam search parameters\n",
    "\n",
    "> **üí° Note**: For most LLM applications, Top-P sampling with appropriate temperature gives better results than beam search for conversational AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Top-K Sampling**\n",
    "   - Limits to K highest-probability tokens\n",
    "   - Fixed size regardless of distribution\n",
    "   - Not directly available in OpenAI API\n",
    "\n",
    "2. **Top-P (Nucleus) Sampling**\n",
    "   - Includes tokens until cumulative probability ‚â• P\n",
    "   - Adapts to distribution shape\n",
    "   - Available as `top_p` parameter\n",
    "\n",
    "3. **Combining Parameters**\n",
    "   - Temperature first modifies distribution\n",
    "   - Top-P then filters the modified distribution\n",
    "   - Don't over-restrict with both low temp and low top_p\n",
    "\n",
    "4. **Practical Guidelines**\n",
    "   - Use temperature as primary creativity control\n",
    "   - Add top_p < 1 to prevent outliers\n",
    "   - Test combinations for your specific use case\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **mini-logprobs**: See actual token probabilities\n",
    "- **mini-streaming**: Real-time token delivery\n",
    "- **lab-llm-playground**: Combine all concepts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_llm_agent",
   "language": "python",
   "name": "text_llm_agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
