{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Mini-Lab: Tokenizer Explorer\n",
    "\n",
    "**Module 2: LLM Core Concepts** | **Duration: ~30 min** | **Type: Mini-Lab**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this mini-lab, you will be able to:\n",
    "\n",
    "1. **Understand** how tokenization converts text into tokens that LLMs process\n",
    "2. **Explore** different tokenization algorithms (BPE, WordPiece, SentencePiece)\n",
    "3. **Compare** tokenizers across different models (GPT-4, GPT-3.5, Claude)\n",
    "4. **Predict** token counts for cost estimation and context planning\n",
    "5. **Identify** edge cases that affect token counts unexpectedly\n",
    "\n",
    "## Target Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| Tokenization | Converting text into discrete units (tokens) for model processing |\n",
    "| Byte-Pair Encoding (BPE) | Algorithm that iteratively merges frequent character pairs |\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "- **Cost Control**: API costs are based on tokens, not characters\n",
    "- **Context Limits**: Understanding token counts helps you fit content within limits\n",
    "- **Prompt Design**: Knowing how text tokenizes helps write efficient prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tokenizers initialized\n",
      "  - GPT-4o tokenizer: o200k_base\n",
      "  - GPT-3.5 tokenizer: cl100k_base\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize tokenizers for different models\n",
    "gpt4_enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "gpt35_enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "print(\"‚úì Tokenizers initialized\")\n",
    "print(f\"  - GPT-4o tokenizer: {gpt4_enc.name}\")\n",
    "print(f\"  - GPT-3.5 tokenizer: {gpt35_enc.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE is the algorithm used by GPT models. It works by:\n",
    "1. Starting with a vocabulary of individual characters\n",
    "2. Iteratively merging the most frequent adjacent pairs\n",
    "3. Building a vocabulary of subwords that balance frequency and meaning\n",
    "\n",
    "Let's visualize how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìù Input: \"Hello, world!\"\n",
      "üî¢ GPT-4o: 4 tokens\n",
      "üìä Ratio: 3.25 chars/token\n",
      "\n",
      "üß© Token breakdown:\n",
      "   [0] ID:  13225 ‚Üí 'Hello'\n",
      "   [1] ID:     11 ‚Üí ','\n",
      "   [2] ID:   2375 ‚Üí ' world'\n",
      "   [3] ID:      0 ‚Üí '!'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13225, 11, 2375, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualize_tokenization(text, encoder, name=\"Tokenizer\"):\n",
    "    \"\"\"Visualize how text is broken into tokens.\"\"\"\n",
    "    tokens = encoder.encode(text)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìù Input: \\\"{text}\\\"\")\n",
    "    print(f\"üî¢ {name}: {len(tokens)} tokens\")\n",
    "    print(f\"üìä Ratio: {len(text)/len(tokens):.2f} chars/token\")\n",
    "    print(f\"\\nüß© Token breakdown:\")\n",
    "    \n",
    "    for i, token_id in enumerate(tokens):\n",
    "        token_text = encoder.decode([token_id])\n",
    "        # Escape special characters for display\n",
    "        display_text = token_text.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "        print(f\"   [{i}] ID: {token_id:6d} ‚Üí '{display_text}'\")\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Simple example\n",
    "visualize_tokenization(\"Hello, world!\", gpt4_enc, \"GPT-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Token Patterns: What Gets Merged?\n",
    "\n",
    "BPE creates tokens based on frequency in the training data. Common patterns become single tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìù Input: \"The quick brown fox jumps over the lazy dog\"\n",
      "üî¢ GPT-4o: 9 tokens\n",
      "üìä Ratio: 4.78 chars/token\n",
      "\n",
      "üß© Token breakdown:\n",
      "   [0] ID:    976 ‚Üí 'The'\n",
      "   [1] ID:   4853 ‚Üí ' quick'\n",
      "   [2] ID:  19705 ‚Üí ' brown'\n",
      "   [3] ID:  68347 ‚Üí ' fox'\n",
      "   [4] ID:  65613 ‚Üí ' jumps'\n",
      "   [5] ID:   1072 ‚Üí ' over'\n",
      "   [6] ID:    290 ‚Üí ' the'\n",
      "   [7] ID:  29082 ‚Üí ' lazy'\n",
      "   [8] ID:   6446 ‚Üí ' dog'\n",
      "\n",
      "============================================================\n",
      "üìù Input: \"TensorFlow PyTorch LangChain embeddings\"\n",
      "üî¢ GPT-4o: 7 tokens\n",
      "üìä Ratio: 5.57 chars/token\n",
      "\n",
      "üß© Token breakdown:\n",
      "   [0] ID:  40994 ‚Üí 'Tensor'\n",
      "   [1] ID:  18017 ‚Üí 'Flow'\n",
      "   [2] ID:  15993 ‚Üí ' Py'\n",
      "   [3] ID: 162709 ‚Üí 'Torch'\n",
      "   [4] ID:  27830 ‚Üí ' Lang'\n",
      "   [5] ID:  20848 ‚Üí 'Chain'\n",
      "   [6] ID: 174989 ‚Üí ' embeddings'\n",
      "\n",
      "============================================================\n",
      "üìù Input: \"2024 1234567890 3.14159 $1,000,000\"\n",
      "üî¢ GPT-4o: 18 tokens\n",
      "üìä Ratio: 1.89 chars/token\n",
      "\n",
      "üß© Token breakdown:\n",
      "   [0] ID:   1323 ‚Üí '202'\n",
      "   [1] ID:     19 ‚Üí '4'\n",
      "   [2] ID:    220 ‚Üí ' '\n",
      "   [3] ID:   7633 ‚Üí '123'\n",
      "   [4] ID:  19354 ‚Üí '456'\n",
      "   [5] ID:  29338 ‚Üí '789'\n",
      "   [6] ID:     15 ‚Üí '0'\n",
      "   [7] ID:    220 ‚Üí ' '\n",
      "   [8] ID:     18 ‚Üí '3'\n",
      "   [9] ID:     13 ‚Üí '.'\n",
      "   [10] ID:  16926 ‚Üí '141'\n",
      "   [11] ID:   4621 ‚Üí '59'\n",
      "   [12] ID:    548 ‚Üí ' $'\n",
      "   [13] ID:     16 ‚Üí '1'\n",
      "   [14] ID:     11 ‚Üí ','\n",
      "   [15] ID:   1302 ‚Üí '000'\n",
      "   [16] ID:     11 ‚Üí ','\n",
      "   [17] ID:   1302 ‚Üí '000'\n",
      "\n",
      "============================================================\n",
      "üìù Input: \"def calculate_embedding(text: str) -> list[float]:\"\n",
      "üî¢ GPT-4o: 12 tokens\n",
      "üìä Ratio: 4.17 chars/token\n",
      "\n",
      "üß© Token breakdown:\n",
      "   [0] ID:   1314 ‚Üí 'def'\n",
      "   [1] ID:  17950 ‚Üí ' calculate'\n",
      "   [2] ID: 122618 ‚Üí '_embedding'\n",
      "   [3] ID:  13414 ‚Üí '(text'\n",
      "   [4] ID:     25 ‚Üí ':'\n",
      "   [5] ID:    989 ‚Üí ' str'\n",
      "   [6] ID:      8 ‚Üí ')'\n",
      "   [7] ID:   2747 ‚Üí ' ->'\n",
      "   [8] ID:   1562 ‚Üí ' list'\n",
      "   [9] ID:     58 ‚Üí '['\n",
      "   [10] ID:   7829 ‚Üí 'float'\n",
      "   [11] ID:  12433 ‚Üí ']:'\n",
      "\n",
      "============================================================\n",
      "üìù Input: \"word    word\n",
      "word\tword\"\n",
      "üî¢ GPT-4o: 6 tokens\n",
      "üìä Ratio: 3.67 chars/token\n",
      "\n",
      "üß© Token breakdown:\n",
      "   [0] ID:   1801 ‚Üí 'word'\n",
      "   [1] ID:    271 ‚Üí '   '\n",
      "   [2] ID:   2195 ‚Üí ' word'\n",
      "   [3] ID:    198 ‚Üí '\\n'\n",
      "   [4] ID:   1801 ‚Üí 'word'\n",
      "   [5] ID: 138068 ‚Üí '\\tword'\n"
     ]
    }
   ],
   "source": [
    "# Explore different types of content\n",
    "examples = [\n",
    "    # Common English words (often single tokens)\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \n",
    "    # Technical terms (might be split)\n",
    "    \"TensorFlow PyTorch LangChain embeddings\",\n",
    "    \n",
    "    # Numbers (different patterns)\n",
    "    \"2024 1234567890 3.14159 $1,000,000\",\n",
    "    \n",
    "    # Code (special tokenization)\n",
    "    \"def calculate_embedding(text: str) -> list[float]:\",\n",
    "    \n",
    "    # Whitespace variations\n",
    "    \"word    word\\nword\\tword\",\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    visualize_tokenization(text, gpt4_enc, \"GPT-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Model Comparison\n",
    "\n",
    "Different models use different tokenizers. This affects:\n",
    "- Token counts (and thus costs)\n",
    "- How meaning is captured at the token level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenizers(text):\n",
    "    \"\"\"Compare how different tokenizers handle the same text.\"\"\"\n",
    "    tokenizers = [\n",
    "        (\"GPT-4o (o200k_base)\", gpt4_enc),\n",
    "        (\"GPT-3.5 (cl100k_base)\", gpt35_enc),\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìù Text: \\\"{text}\\\"\")\n",
    "    print(f\"üìè Length: {len(text)} characters\")\n",
    "    print(f\"\\nüìä Token counts:\")\n",
    "    \n",
    "    for name, enc in tokenizers:\n",
    "        tokens = enc.encode(text)\n",
    "        print(f\"   {name}: {len(tokens)} tokens\")\n",
    "    \n",
    "    print(\"\\nüîç Detailed breakdown:\")\n",
    "    for name, enc in tokenizers:\n",
    "        tokens = enc.encode(text)\n",
    "        decoded = [enc.decode([t]) for t in tokens]\n",
    "        print(f\"\\n   {name}:\")\n",
    "        print(f\"   {decoded}\")\n",
    "\n",
    "# Test with various content types\n",
    "test_texts = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"The embedding dimension is 1536\",\n",
    "    \"async def fetch_data(): await api.call()\",\n",
    "    \"„Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå\",  # Japanese: \"Hello World\"\n",
    "    \"ü§ñüí°üöÄ\",  # Emojis\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    compare_tokenizers(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Token Count Estimation Tool\n",
    "\n",
    "Build a practical tool for estimating API costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing per million tokens (approximate, as of 2025)\n",
    "PRICING = {\n",
    "    \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
    "    \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "    \"gpt-3.5-turbo\": {\"input\": 0.50, \"output\": 1.50},\n",
    "}\n",
    "\n",
    "def estimate_cost(prompt, expected_output_tokens=500, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"Estimate API cost for a prompt.\"\"\"\n",
    "    # Use appropriate tokenizer\n",
    "    if model.startswith(\"gpt-4o\"):\n",
    "        enc = gpt4_enc\n",
    "    else:\n",
    "        enc = gpt35_enc\n",
    "    \n",
    "    input_tokens = len(enc.encode(prompt))\n",
    "    prices = PRICING.get(model, PRICING[\"gpt-4o-mini\"])\n",
    "    \n",
    "    input_cost = (input_tokens / 1_000_000) * prices[\"input\"]\n",
    "    output_cost = (expected_output_tokens / 1_000_000) * prices[\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    print(f\"\\nüí∞ Cost Estimate for {model}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"üì• Input tokens:  {input_tokens:,}\")\n",
    "    print(f\"üì§ Output tokens: {expected_output_tokens:,} (estimated)\")\n",
    "    print(f\"\\nüíµ Costs:\")\n",
    "    print(f\"   Input:  ${input_cost:.6f}\")\n",
    "    print(f\"   Output: ${output_cost:.6f}\")\n",
    "    print(f\"   Total:  ${total_cost:.6f}\")\n",
    "    print(f\"\\nüìä Per 1000 calls: ${total_cost * 1000:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": expected_output_tokens,\n",
    "        \"total_cost\": total_cost\n",
    "    }\n",
    "\n",
    "# Example: RAG system prompt\n",
    "rag_prompt = \"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "Large Language Models (LLMs) are AI systems trained on vast amounts of text data.\n",
    "They can understand and generate human-like text. Common architectures include\n",
    "transformer-based models like GPT, which use attention mechanisms to process\n",
    "sequential data efficiently.\n",
    "\n",
    "Question: What is an LLM?\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "estimate_cost(rag_prompt, expected_output_tokens=100, model=\"gpt-4o-mini\")\n",
    "estimate_cost(rag_prompt, expected_output_tokens=100, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Edge Cases & Gotchas\n",
    "\n",
    "Understanding edge cases helps avoid surprises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_edge_cases():\n",
    "    \"\"\"Explore tokenization edge cases.\"\"\"\n",
    "    \n",
    "    edge_cases = [\n",
    "        # Repeated characters\n",
    "        (\"aaaaaaaaaa\", \"Repeated letters\"),\n",
    "        (\"...........\", \"Repeated punctuation\"),\n",
    "        \n",
    "        # Case sensitivity\n",
    "        (\"Hello hello HELLO\", \"Case variations\"),\n",
    "        \n",
    "        # Leading/trailing spaces\n",
    "        (\" word\", \"Leading space\"),\n",
    "        (\"word \", \"Trailing space\"),\n",
    "        (\" word \", \"Both spaces\"),\n",
    "        \n",
    "        # Special characters\n",
    "        (\"user@email.com\", \"Email\"),\n",
    "        (\"https://example.com/path?query=value\", \"URL\"),\n",
    "        \n",
    "        # Code patterns\n",
    "        (\"{{variable}}\", \"Template syntax\"),\n",
    "        (\"/* comment */\", \"Comment block\"),\n",
    "        \n",
    "        # Unicode\n",
    "        (\"caf√© r√©sum√© na√Øve\", \"Accented characters\"),\n",
    "        (\"‚Üí ‚Üê ‚Üî ‚áí\", \"Arrows\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüî¨ Tokenization Edge Cases\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for text, description in edge_cases:\n",
    "        tokens = gpt4_enc.encode(text)\n",
    "        ratio = len(text) / len(tokens) if len(tokens) > 0 else 0\n",
    "        print(f\"\\n{description}:\")\n",
    "        print(f\"  Text: \\\"{text}\\\"\")\n",
    "        print(f\"  Chars: {len(text)}, Tokens: {len(tokens)}, Ratio: {ratio:.2f}\")\n",
    "        decoded = [gpt4_enc.decode([t]) for t in tokens]\n",
    "        print(f\"  Breakdown: {decoded}\")\n",
    "\n",
    "analyze_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Exercise: Optimize a Prompt\n",
    "\n",
    "Apply what you've learned to optimize a prompt for token efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original verbose prompt\n",
    "verbose_prompt = \"\"\"\n",
    "You are an extremely helpful and knowledgeable AI assistant. Your job is to help\n",
    "users with their questions. Please make sure to be thorough, accurate, and helpful\n",
    "in all of your responses. When answering questions, please consider the context\n",
    "carefully and provide comprehensive answers.\n",
    "\n",
    "The user has the following question that they would like you to answer:\n",
    "\n",
    "What is machine learning?\n",
    "\n",
    "Please provide your response below:\n",
    "\"\"\"\n",
    "\n",
    "# Optimized prompt (same meaning, fewer tokens)\n",
    "optimized_prompt = \"\"\"\n",
    "You are a helpful AI assistant. Answer accurately and thoroughly.\n",
    "\n",
    "Question: What is machine learning?\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìä Prompt Optimization Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "verbose_tokens = len(gpt4_enc.encode(verbose_prompt))\n",
    "optimized_tokens = len(gpt4_enc.encode(optimized_prompt))\n",
    "savings = verbose_tokens - optimized_tokens\n",
    "percent_saved = (savings / verbose_tokens) * 100\n",
    "\n",
    "print(f\"\\nüìù Verbose prompt: {verbose_tokens} tokens\")\n",
    "print(f\"‚ú® Optimized prompt: {optimized_tokens} tokens\")\n",
    "print(f\"üí∞ Tokens saved: {savings} ({percent_saved:.1f}%)\")\n",
    "print(f\"\\nüî¢ Over 1M API calls, this saves ~{savings * 1_000_000:,} tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Tokenization Basics**\n",
    "   - Tokens are the units LLMs process\n",
    "   - 1 token ‚âà 4 characters ‚âà 0.75 words (in English)\n",
    "   - BPE builds vocabulary from frequent patterns\n",
    "\n",
    "2. **Model Differences**\n",
    "   - Different models use different tokenizers\n",
    "   - GPT-4o uses `o200k_base`, GPT-3.5 uses `cl100k_base`\n",
    "   - Same text can have different token counts across models\n",
    "\n",
    "3. **Cost Implications**\n",
    "   - API pricing is per token, not per character\n",
    "   - Efficient prompts save money at scale\n",
    "   - Input/output tokens often have different prices\n",
    "\n",
    "4. **Edge Cases**\n",
    "   - Non-English text often uses more tokens\n",
    "   - Code may tokenize unexpectedly\n",
    "   - Whitespace handling varies\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **mini-context**: Learn about context window limits\n",
    "- **mini-temperature**: Explore generation parameters\n",
    "- **lab-llm-playground**: Combine all LLM core concepts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_llm_agent",
   "language": "python",
   "name": "text_llm_agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
