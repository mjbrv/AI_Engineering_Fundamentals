{
  "notebook": "lab-llm-playground.ipynb",
  "module": "Module 2: LLM Core Concepts",
  "total_duration": "~1 hour",
  "transcripts": [
    {
      "cell_index": 0,
      "cell_type": "markdown",
      "section": "Introduction",
      "transcript": "Alright, welcome everyone to our LLM Playground lab! So, this is where things get really fun because we're gonna get our hands dirty with the actual mechanics of how large language models work. You know, we've talked about all these concepts like tokenization, temperature, sampling strategies... but now? Now we're actually going to see them in action. And honestly, I think this is the best way to really understand what's happening under the hood. So we've got about an hour here, and we'll go through everything from how text gets broken into tokens, to playing with temperature to see how it affects creativity, and even implementing streaming responses. By the end of this, you're gonna have a much better intuition for how to tune these models for your specific use cases. Sound good? Let's dive in!"
    },
    {
      "cell_index": 1,
      "cell_type": "markdown",
      "section": "1. Setup",
      "transcript": "Okay, so first things first - we need to get our environment set up. This should only take like five minutes, maybe less if you've already got your API keys configured."
    },
    {
      "cell_index": 2,
      "cell_type": "code",
      "section": "1. Setup",
      "concepts": ["OpenAI client", "Anthropic client", "environment variables"],
      "transcript": "Alright, let me run this setup cell here. So what we're doing is, uh, importing our usual suspects - os for environment stuff, dotenv to load our API keys, and of course the OpenAI client. We're also bringing in tiktoken which, if you remember, is OpenAI's tokenizer library - we'll need that in just a bit. Now here's a nice little helper - we've got this md() function that renders markdown. This is super useful because when the LLM outputs markdown-formatted text, instead of seeing raw asterisks and hashtags, we get beautifully rendered headers, bold text, lists - the whole thing. And then we've got this optional Anthropic setup wrapped in a try-except... because not everyone has an Anthropic API key, and that's totally fine. It's optional for the model comparison section later. Let's see... boom! Both clients initialized successfully. Perfect, we're good to go!"
    },
    {
      "cell_index": 3,
      "cell_type": "markdown",
      "section": "2. Tokenization Explorer",
      "transcript": "Okay, now this is where it gets interesting. Tokenization! So, you guys already know what tokenization is conceptually, right? It's this process of breaking text into these discrete chunks - tokens - that the model can actually work with. But here's the thing that I find really cool... different models use different tokenizers with completely different vocabularies. And there's this rough rule of thumb - about one token equals four characters, or roughly three-quarters of a word in English. But, uh, let's actually see what that looks like in practice."
    },
    {
      "cell_index": 4,
      "cell_type": "code",
      "section": "2. Tokenization Explorer",
      "concepts": ["tokenization", "token IDs", "tiktoken", "GPT-4o tokenizer"],
      "transcript": "So here I've got this explore_tokens function that's gonna show us exactly how text gets broken down. We're using the GPT-4o tokenizer specifically with tiktoken. Let me run this and... okay, look at this! So 'Hello, world!' - a super simple string - becomes just 4 tokens. 'Hello' is one token, the comma is its own token, then we've got ' world' with the space included - that's important - and then the exclamation mark. Now here's where it gets more interesting - look at 'The transformer architecture revolutionized NLP'. See how 'revolutionized' gets split into 'revolution' and 'ized'? That's the byte-pair encoding at work, breaking down less common words into subword units. And then the code example - notice how the tokenizer handles things like the colon-parenthesis-newline sequence as one token? That's because it's seen this pattern so many times in training data. Pretty neat, right?"
    },
    {
      "cell_index": 5,
      "cell_type": "markdown",
      "section": "3. Temperature Effects",
      "transcript": "Alright, let's move on to one of my favorite parameters to play with - temperature. So you already know that temperature controls the randomness of outputs by scaling the logits before the softmax. Low temperature makes things more deterministic, high temperature makes things more... let's say 'creative'. But actually seeing the difference? That's what we're gonna do right now."
    },
    {
      "cell_index": 6,
      "cell_type": "code",
      "section": "3. Temperature Effects",
      "concepts": ["temperature", "logit scaling", "output randomness", "creativity control"],
      "transcript": "So I've got this test_temperature function that's gonna call the model multiple times with different temperature values. We're asking it to write a one-sentence story about a robot - simple creative task. Notice we're also asking for markdown format so the output looks nice and rendered. And let's run it with temperatures 0, 0.7, and 1.2. Okay... check this out! At temperature 0, we get this nicely formatted output about a lonely robot discovering poetry - very structured, very coherent. At 0.7, it's similar but with slight variations in the wording. But at 1.2? Now we've got something more unexpected, more creative. This is exactly why temperature is so powerful for tuning your outputs. For factual tasks, keep it low. For creative stuff, crank it up a bit. And look how clean these markdown outputs look compared to raw text! Make sense?"
    },
    {
      "cell_index": 7,
      "cell_type": "markdown",
      "section": "4. Sampling Strategies",
      "transcript": "Now let's talk about sampling strategies. So temperature is one way to control randomness, but we've also got Top-P, also called nucleus sampling, and Top-K sampling. Top-P is the one we can actually use with OpenAI's API - it selects from the smallest set of tokens whose cumulative probability exceeds some threshold P. Top-K just picks from the K most likely tokens. The OpenAI API doesn't directly expose Top-K, but it's still important to understand the concept."
    },
    {
      "cell_index": 8,
      "cell_type": "code",
      "section": "4. Sampling Strategies",
      "concepts": ["Top-P sampling", "nucleus sampling", "probability thresholds"],
      "transcript": "Alright, so here we're testing Top-P at different values - 0.1, 0.5, and 0.95. I'm keeping temperature at 1.0 so we can really see the Top-P effect. We're asking for 3 unique hobbies in markdown format - let's see what happens. Okay so at Top-P 0.1, super restricted - we get these nicely formatted hobbies with bold headers and descriptions. At 0.5... actually pretty similar hobbies! But look at Top-P 0.95 - now we've got different options entirely. More variety! See, with lower Top-P values, you're basically telling the model 'only consider the most probable tokens', so you get more... predictable results. Higher Top-P means 'consider more options from the probability distribution'. It's subtle but it matters, especially when you want more diverse outputs. And these markdown-rendered lists look so much cleaner than plain text, right?"
    },
    {
      "cell_index": 9,
      "cell_type": "markdown",
      "section": "5. Stop Sequences",
      "transcript": "Okay, quick but super useful topic - stop sequences. These are like, uh, exit signals that tell the model 'hey, stop generating when you see this'. Really handy for controlling output format."
    },
    {
      "cell_index": 10,
      "cell_type": "code",
      "section": "5. Stop Sequences",
      "concepts": ["stop sequences", "output control", "generation termination"],
      "transcript": "So check this out - we're asking for 5 fruits, one per line. But in the first call, I've set a stop sequence of '\\n4.' - meaning stop before the fourth item. And look at the result - Apple, Banana, Cherry... and it stops! Only three items. Without the stop sequence? We get all five - Apple, Banana, Orange, Mango, Grapes. This is really practical stuff, like... imagine you're building something where you only want a certain number of items, or you want to stop at a specific keyword or pattern. Stop sequences give you that fine-grained control without having to post-process the output yourself."
    },
    {
      "cell_index": 11,
      "cell_type": "markdown",
      "section": "6. Streaming Responses",
      "transcript": "Alright, next up - streaming responses. This is huge for user experience. Instead of waiting for the entire response to generate and then showing it all at once, streaming lets you display tokens as they come in. It makes everything feel so much faster and more interactive."
    },
    {
      "cell_index": 12,
      "cell_type": "code",
      "section": "6. Streaming Responses",
      "concepts": ["streaming", "real-time output", "chunk processing", "perceived latency"],
      "transcript": "So here's our streaming function - and this one's pretty cool. The key thing is setting stream=True in the API call, and then we iterate over the chunks as they come in. But here's the neat part - instead of showing raw text, we're using clear_output and our md() helper to re-render the markdown on every single chunk. Let me run this and... watch! See how it's streaming but the output is beautifully formatted the whole time? That little cursor shows it's still generating. This is exactly what you see in ChatGPT and Claude's interfaces - live markdown rendering as tokens come in. The perceived latency is way lower because you're seeing progress the whole time, and it looks professional from the start. When you're building production applications, this streaming-with-markdown approach is the way to go for chat-style interfaces. Users don't want to stare at a loading spinner for 5 seconds, you know?"
    },
    {
      "cell_index": 13,
      "cell_type": "markdown",
      "section": "7. Model Comparison",
      "transcript": "Okay, last but definitely not least - model comparison. This is actually super important in the real world because, you know, not all models are created equal. Different models have different strengths, different personalities almost. And as AI engineers, we need to understand these differences so we can pick the right tool for the job."
    },
    {
      "cell_index": 14,
      "cell_type": "code",
      "section": "7. Model Comparison",
      "concepts": ["model comparison", "GPT-4o vs GPT-4o-mini", "Claude", "model selection"],
      "transcript": "So here we've got a compare_models function that sends the exact same prompt to different models and shows us their responses side by side - all beautifully rendered as markdown. We're comparing GPT-4o-mini, the full GPT-4o, and if you've got Anthropic set up, Claude as well. The code tries a few different Claude model versions since they update these pretty frequently. Let's ask them all about the most important skill for an AI engineer in 2026... and there we go! Look at this - each model's response is nicely formatted with headers, and you can see each model has its own style of answering. GPT-4o-mini tends to be concise, GPT-4o gives a bit more depth, and Claude often has a slightly different perspective. This is why model comparison matters - in production, you might want the cheaper GPT-4o-mini for simple tasks, but reach for GPT-4o or Claude when you need more nuanced reasoning. It's all about matching the model to the task. And honestly? The only way to really know which model works best for YOUR specific use case is to test them like this."
    },
    {
      "cell_index": 15,
      "cell_type": "markdown",
      "section": "Summary",
      "transcript": "And that's a wrap for this lab! So let's do a quick recap of everything we covered. We started with tokenization - actually seeing how text gets chopped up into tokens and why that matters for cost and context limits. Then we played with temperature and saw how it controls creativity - low for factual, high for creative. We explored Top-P sampling which gives us another knob for controlling output diversity. Stop sequences for precise control over when generation stops. Streaming for that smooth real-time experience. And finally, model comparison to understand that different models really do behave differently. These are all fundamental tools in your AI engineering toolkit now. I'd encourage you to go experiment with these parameters on your own projects - there's really no substitute for hands-on experience. Alright, any questions? Otherwise, see you in the next module!"
    }
  ]
}
