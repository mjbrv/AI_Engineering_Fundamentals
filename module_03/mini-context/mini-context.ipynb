{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Mini-Lab: Context Window Limits\n",
    "\n",
    "**Module 2: LLM Core Concepts** | **Duration: ~30 min** | **Type: Mini-Lab**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this mini-lab, you will be able to:\n",
    "\n",
    "1. **Understand** what a context window is and why it matters\n",
    "2. **Compare** context window sizes across different models\n",
    "3. **Implement** strategies for handling long content\n",
    "4. **Recognize** the \"lost in the middle\" phenomenon\n",
    "5. **Plan** context allocation for prompts, content, and responses\n",
    "\n",
    "## Target Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| Context Window | Maximum number of tokens a model can process in one request |\n",
    "| Tokenization | Converting text to tokens (prerequisite from mini-tokenizer) |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **mini-tokenizer**: Understanding of tokens and token counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Helper function to render LLM output as formatted markdown\n",
    "def md(text):\n",
    "    \"\"\"Display text as rendered markdown.\"\"\"\n",
    "    display(Markdown(text))\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text.\"\"\"\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Context Windows\n",
    "\n",
    "A **context window** is the maximum number of tokens a model can process in a single request. This includes:\n",
    "- System prompt\n",
    "- Conversation history\n",
    "- User input\n",
    "- Retrieved context (RAG)\n",
    "- **AND** the generated response\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ           CONTEXT WINDOW (128K)             ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  [System]  [History]  [Context]  [Response] ‚îÇ\n",
    "‚îÇ    500       2000       8000        4000    ‚îÇ\n",
    "‚îÇ                                             ‚îÇ\n",
    "‚îÇ  Total: 14,500 tokens used of 128,000       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Context Window Comparison\n",
      "============================================================\n",
      "\n",
      "gemini-1.5-pro        1,000,000 tokens (~2,000 pages)\n",
      "                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "gemini-1.5-flash      1,000,000 tokens (~2,000 pages)\n",
      "                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "claude-3-opus           200,000 tokens (~400 pages)\n",
      "                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "claude-3-sonnet         200,000 tokens (~400 pages)\n",
      "                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "claude-3-haiku          200,000 tokens (~400 pages)\n",
      "                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "gpt-4o                  128,000 tokens (~256 pages)\n",
      "                     ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "gpt-4o-mini             128,000 tokens (~256 pages)\n",
      "                     ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "gpt-4-turbo             128,000 tokens (~256 pages)\n",
      "                     ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "mistral-7b               32,768 tokens (~65 pages)\n",
      "                     ‚ñà\n",
      "\n",
      "gpt-3.5-turbo            16,385 tokens (~32 pages)\n",
      "                     \n",
      "\n",
      "gpt-3.5-turbo-16k        16,385 tokens (~32 pages)\n",
      "                     \n",
      "\n",
      "llama-3-70b               8,192 tokens (~16 pages)\n",
      "                     \n"
     ]
    }
   ],
   "source": [
    "# Model context window sizes (as of 2024)\n",
    "MODEL_CONTEXTS = {\n",
    "    # OpenAI\n",
    "    \"gpt-4o\": 128_000,\n",
    "    \"gpt-4o-mini\": 128_000,\n",
    "    \"gpt-4-turbo\": 128_000,\n",
    "    \"gpt-3.5-turbo\": 16_385,\n",
    "    \"gpt-3.5-turbo-16k\": 16_385,\n",
    "    \n",
    "    # Anthropic\n",
    "    \"claude-3-opus\": 200_000,\n",
    "    \"claude-3-sonnet\": 200_000,\n",
    "    \"claude-3-haiku\": 200_000,\n",
    "    \n",
    "    # Google\n",
    "    \"gemini-1.5-pro\": 1_000_000,\n",
    "    \"gemini-1.5-flash\": 1_000_000,\n",
    "    \n",
    "    # Open Source\n",
    "    \"llama-3-70b\": 8_192,\n",
    "    \"mistral-7b\": 32_768,\n",
    "}\n",
    "\n",
    "def visualize_context_sizes():\n",
    "    \"\"\"Visualize context window sizes.\"\"\"\n",
    "    print(\"\\nüìä Context Window Comparison\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    max_ctx = max(MODEL_CONTEXTS.values())\n",
    "    \n",
    "    for model, ctx_size in sorted(MODEL_CONTEXTS.items(), key=lambda x: -x[1]):\n",
    "        bar_length = int((ctx_size / max_ctx) * 40)\n",
    "        bar = \"‚ñà\" * bar_length\n",
    "        \n",
    "        # Estimate pages (assuming ~500 tokens/page)\n",
    "        pages = ctx_size // 500\n",
    "        \n",
    "        print(f\"\\n{model:20} {ctx_size:>10,} tokens (~{pages:,} pages)\")\n",
    "        print(f\"                     {bar}\")\n",
    "\n",
    "visualize_context_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context Budget Planning\n",
    "\n",
    "When designing prompts, you need to allocate your context budget wisely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìê Context Budget Plan for gpt-4o-mini\n",
      "==================================================\n",
      "Context limit: 128,000 tokens\n",
      "\n",
      "üì• INPUT ALLOCATION:\n",
      "   System prompt:         14 tokens\n",
      "   User message:          12 tokens\n",
      "   Retrieved context:    980 tokens\n",
      "   History:                0 tokens\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   Input subtotal:     1,006 tokens\n",
      "\n",
      "üì§ OUTPUT RESERVATION:\n",
      "   Max response:       2,000 tokens\n",
      "\n",
      "üìä SUMMARY:\n",
      "   Total needed:       3,006 tokens\n",
      "   Remaining buffer:  124,994 tokens\n",
      "\n",
      "‚úÖ OK: Using 2.3% of context\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 1006,\n",
       " 'reserved_output': 2000,\n",
       " 'remaining': 124994,\n",
       " 'usage_percent': 2.3484374999999997}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plan_context_budget(model, system_prompt, user_message, context=\"\", \n",
    "                        max_response_tokens=4000, conversation_history=\"\"):\n",
    "    \"\"\"Plan context budget for an API call.\"\"\"\n",
    "    \n",
    "    context_limit = MODEL_CONTEXTS.get(model, 128_000)\n",
    "    \n",
    "    # Count tokens for each component\n",
    "    system_tokens = count_tokens(system_prompt)\n",
    "    user_tokens = count_tokens(user_message)\n",
    "    context_tokens = count_tokens(context)\n",
    "    history_tokens = count_tokens(conversation_history)\n",
    "    \n",
    "    # Calculate totals\n",
    "    input_total = system_tokens + user_tokens + context_tokens + history_tokens\n",
    "    reserved_for_response = max_response_tokens\n",
    "    total_needed = input_total + reserved_for_response\n",
    "    remaining = context_limit - total_needed\n",
    "    \n",
    "    print(f\"\\nüìê Context Budget Plan for {model}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Context limit: {context_limit:,} tokens\")\n",
    "    print(f\"\\nüì• INPUT ALLOCATION:\")\n",
    "    print(f\"   System prompt:     {system_tokens:>6,} tokens\")\n",
    "    print(f\"   User message:      {user_tokens:>6,} tokens\")\n",
    "    print(f\"   Retrieved context: {context_tokens:>6,} tokens\")\n",
    "    print(f\"   History:           {history_tokens:>6,} tokens\")\n",
    "    print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(f\"   Input subtotal:    {input_total:>6,} tokens\")\n",
    "    print(f\"\\nüì§ OUTPUT RESERVATION:\")\n",
    "    print(f\"   Max response:      {reserved_for_response:>6,} tokens\")\n",
    "    print(f\"\\nüìä SUMMARY:\")\n",
    "    print(f\"   Total needed:      {total_needed:>6,} tokens\")\n",
    "    print(f\"   Remaining buffer:  {remaining:>6,} tokens\")\n",
    "    \n",
    "    usage_percent = (total_needed / context_limit) * 100\n",
    "    \n",
    "    if remaining < 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Over budget by {-remaining:,} tokens!\")\n",
    "    elif usage_percent > 90:\n",
    "        print(f\"\\n‚ö†Ô∏è  CAUTION: Using {usage_percent:.1f}% of context\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ OK: Using {usage_percent:.1f}% of context\")\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": input_total,\n",
    "        \"reserved_output\": reserved_for_response,\n",
    "        \"remaining\": remaining,\n",
    "        \"usage_percent\": usage_percent\n",
    "    }\n",
    "\n",
    "# Example: RAG application\n",
    "system = \"You are a helpful assistant that answers questions based on the provided context.\"\n",
    "user = \"What are the key benefits of using transformers for NLP tasks?\"\n",
    "retrieved_context = \"\\n\".join([\n",
    "    \"Transformers introduced self-attention mechanisms that allow parallel processing.\",\n",
    "    \"Unlike RNNs, transformers can capture long-range dependencies efficiently.\",\n",
    "    \"The architecture enables transfer learning through pre-trained models.\",\n",
    "    \"BERT and GPT demonstrated state-of-the-art results across NLP benchmarks.\",\n",
    "] * 20)  # Simulate more context\n",
    "\n",
    "plan_context_budget(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system_prompt=system,\n",
    "    user_message=user,\n",
    "    context=retrieved_context,\n",
    "    max_response_tokens=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The \"Lost in the Middle\" Problem\n",
    "\n",
    "Research shows that LLMs pay less attention to information in the middle of long contexts. Let's demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Lost-in-the-Middle Test\n",
      "============================================================\n",
      "üìö Using 60 real AI Engineering documents as filler\n",
      "üìÑ Total document size: 15,494 tokens\n",
      "\n",
      "üîë Secret codes placed at:\n",
      "   START:  SECRET-START-1234\n",
      "   MIDDLE: SECRET-MIDDLE-5678\n",
      "   END:    SECRET-END-9012\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìç Code at START:\n",
      "   Question: What is the FIRST secret code?\n",
      "   Expected: SECRET-START-1234\n",
      "   Answer: The first secret code is SECRET-START-1234.\n",
      "   Found correctly: ‚úÖ Yes\n",
      "\n",
      "üìç Code at MIDDLE:\n",
      "   Question: What is the SECOND secret code?\n",
      "   Expected: SECRET-MIDDLE-5678\n",
      "   Answer: The second secret code is SECRET-MIDDLE-5678.\n",
      "   Found correctly: ‚úÖ Yes\n",
      "\n",
      "üìç Code at END:\n",
      "   Question: What is the THIRD secret code?\n",
      "   Expected: SECRET-END-9012\n",
      "   Answer: The third secret code is SECRET-END-9012.\n",
      "   Found correctly: ‚úÖ Yes\n",
      "\n",
      "============================================================\n",
      "üìä SUMMARY:\n",
      "   START code found:  ‚úÖ\n",
      "   MIDDLE code found: ‚úÖ\n",
      "   END code found:    ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../data')\n",
    "from load_corpus import load_documents\n",
    "\n",
    "def test_lost_in_middle():\n",
    "    \"\"\"Demonstrate the lost-in-the-middle phenomenon.\"\"\"\n",
    "    \n",
    "    # Load real documents from the corpus as filler\n",
    "    docs = load_documents()\n",
    "    filler_texts = [doc['content'] for doc in docs]\n",
    "    filler = \"\\n\\n\".join(filler_texts)  # Combine all documents\n",
    "    \n",
    "    # Split filler into two halves for positioning\n",
    "    filler_tokens = enc.encode(filler)\n",
    "    mid_point = len(filler_tokens) // 3 # to fot the ncet\n",
    "    filler_part1 = enc.decode(filler_tokens[:mid_point])\n",
    "    filler_part2 = enc.decode(filler_tokens[-mid_point:])\n",
    "    \n",
    "    # Three different secret codes - one for each position\n",
    "    code_start = \"SECRET-START-1234\"\n",
    "    code_middle = \"SECRET-MIDDLE-5678\"\n",
    "    code_end = \"SECRET-END-9012\"\n",
    "    \n",
    "    fact_start = f\"IMPORTANT: The first secret code is {code_start}.\"\n",
    "    fact_middle = f\"IMPORTANT: The second secret code is {code_middle}.\"\n",
    "    fact_end = f\"IMPORTANT: The third secret code is {code_end}.\"\n",
    "    \n",
    "    # Build ONE document with all three codes at START, MIDDLE, and END\n",
    "    document = f\"{fact_start}\\n\\n{filler_part1}\\n\\n{fact_middle}\\n\\n{filler_part2}\\n\\n{fact_end}\"\n",
    "    total_tokens = count_tokens(document)\n",
    "    \n",
    "    # Questions to test retrieval from each position\n",
    "    test_cases = [\n",
    "        (\"START\", \"What is the FIRST secret code?\", code_start),\n",
    "        (\"MIDDLE\", \"What is the SECOND secret code?\", code_middle),\n",
    "        (\"END\", \"What is the THIRD secret code?\", code_end),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüî¨ Lost-in-the-Middle Test\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìö Using {len(docs)} real AI Engineering documents as filler\")\n",
    "    print(f\"üìÑ Total document size: {total_tokens:,} tokens\")\n",
    "    print(f\"\\nüîë Secret codes placed at:\")\n",
    "    print(f\"   START:  {code_start}\")\n",
    "    print(f\"   MIDDLE: {code_middle}\")\n",
    "    print(f\"   END:    {code_end}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    results = []\n",
    "    for position, question, expected_code in test_cases:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Answer based only on the provided document. Be concise.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Document:\\n{document}\\n\\nQuestion: {question}\"}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        found = expected_code in answer\n",
    "        results.append(found)\n",
    "        \n",
    "        print(f\"\\nüìç Code at {position}:\")\n",
    "        print(f\"   Question: {question}\")\n",
    "        print(f\"   Expected: {expected_code}\")\n",
    "        print(f\"   Answer: {answer}\")\n",
    "        print(f\"   Found correctly: {'‚úÖ Yes' if found else '‚ùå No'}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä SUMMARY:\")\n",
    "    print(f\"   START code found:  {'‚úÖ' if results[0] else '‚ùå'}\")\n",
    "    print(f\"   MIDDLE code found: {'‚úÖ' if results[1] else '‚ùå'}\")\n",
    "    print(f\"   END code found:    {'‚úÖ' if results[2] else '‚ùå'}\")\n",
    "    \n",
    "    if not results[1] and (results[0] or results[2]):\n",
    "        print(\"\\nüí° This demonstrates the 'Lost in the Middle' phenomenon!\")\n",
    "        print(\"   The model found codes at the start/end but missed the middle.\")\n",
    "\n",
    "test_lost_in_middle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Strategies for Long Content\n",
    "\n",
    "When content exceeds context limits, use these strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Original document: 2,401 tokens\n",
      "\n",
      "üì¶ Chunking with overlap:\n",
      "\n",
      "Chunk 1: 500 tokens (positions 0-500)\n",
      "   Preview: \n",
      "Chapter 1: Introduction to AI\n",
      "\n",
      "Artificial Intelligence (AI) has transformed numerous industries ove...\n",
      "\n",
      "Chunk 2: 500 tokens (positions 450-950)\n",
      "   Preview: , and game playing.\n",
      "\n",
      "The transformer architecture, introduced in 2017, revolutionized NLP by enablin...\n",
      "\n",
      "Chunk 3: 500 tokens (positions 900-1400)\n",
      "   Preview:  data without explicit\n",
      "programming. Deep learning, using neural networks with many layers, has achie...\n",
      "\n",
      "Chunk 4: 500 tokens (positions 1350-1850)\n",
      "   Preview:  AI applications continue to expand.\n",
      "This chapter explores the fundamentals of AI and machine learni...\n",
      "\n",
      "Chunk 5: 500 tokens (positions 1800-2300)\n",
      "   Preview: .\n",
      "\n",
      "Chapter 1: Introduction to AI\n",
      "\n",
      "Artificial Intelligence (AI) has transformed numerous industries o...\n",
      "\n",
      "Chunk 6: 151 tokens (positions 2250-2750)\n",
      "   Preview: , and game playing.\n",
      "\n",
      "The transformer architecture, introduced in 2017, revolutionized NLP by enablin...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, max_tokens=1000, overlap_tokens=100):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks that fit within token limits.\n",
    "    Overlap helps maintain context between chunks.\n",
    "    \"\"\"\n",
    "    tokens = enc.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + max_tokens\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = enc.decode(chunk_tokens)\n",
    "        chunks.append({\n",
    "            \"text\": chunk_text,\n",
    "            \"tokens\": len(chunk_tokens),\n",
    "            \"start_idx\": start,\n",
    "            \"end_idx\": end\n",
    "        })\n",
    "        start = end - overlap_tokens  # Overlap for continuity\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example: Long document\n",
    "long_document = \"\"\"\n",
    "Chapter 1: Introduction to AI\n",
    "\n",
    "Artificial Intelligence (AI) has transformed numerous industries over the past decade.\n",
    "From healthcare diagnostics to autonomous vehicles, AI applications continue to expand.\n",
    "This chapter explores the fundamentals of AI and machine learning.\n",
    "\n",
    "Machine learning, a subset of AI, enables computers to learn from data without explicit\n",
    "programming. Deep learning, using neural networks with many layers, has achieved\n",
    "remarkable success in image recognition, natural language processing, and game playing.\n",
    "\n",
    "The transformer architecture, introduced in 2017, revolutionized NLP by enabling\n",
    "parallel processing of sequences through self-attention mechanisms.\n",
    "\"\"\" * 20  # Simulate a longer document\n",
    "\n",
    "print(f\"üìÑ Original document: {count_tokens(long_document):,} tokens\")\n",
    "print(\"\\nüì¶ Chunking with overlap:\\n\")\n",
    "\n",
    "chunks = chunk_text(long_document, max_tokens=500, overlap_tokens=50)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk['tokens']} tokens (positions {chunk['start_idx']}-{chunk['end_idx']})\")\n",
    "    print(f\"   Preview: {chunk['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Processing 2 chunks...\n",
      "   Chunk 1: Found relevant content\n",
      "   Chunk 2: Found relevant content\n",
      "\n",
      "‚úÖ Final Answer:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The transformer architecture is a model introduced in 2017 that revolutionized natural language processing (NLP) by enabling parallel processing of sequences through self-attention mechanisms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_and_process(long_text, question, max_context=4000):\n",
    "    \"\"\"\n",
    "    Process long content by:\n",
    "    1. Chunking the content\n",
    "    2. Extracting relevant parts per chunk\n",
    "    3. Combining and answering\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = chunk_text(long_text, max_tokens=2000, overlap_tokens=100)\n",
    "    \n",
    "    print(f\"\\nüìã Processing {len(chunks)} chunks...\")\n",
    "    \n",
    "    # Extract relevant info from each chunk\n",
    "    relevant_extracts = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Extract only the sentences relevant to this question: \"{question}\"\n",
    "                \n",
    "From this text:\n",
    "{chunk['text']}\n",
    "\n",
    "Return only relevant sentences, or 'Nothing relevant' if none found.\"\"\"\n",
    "            }],\n",
    "            temperature=0,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        extract = response.choices[0].message.content\n",
    "        if \"nothing relevant\" not in extract.lower():\n",
    "            relevant_extracts.append(extract)\n",
    "            print(f\"   Chunk {i+1}: Found relevant content\")\n",
    "        else:\n",
    "            print(f\"   Chunk {i+1}: No relevant content\")\n",
    "    \n",
    "    # Combine and answer\n",
    "    combined_context = \"\\n---\\n\".join(relevant_extracts)\n",
    "    \n",
    "    if count_tokens(combined_context) > max_context:\n",
    "        print(f\"\\n‚ö†Ô∏è  Combined context too large, truncating...\")\n",
    "        combined_tokens = enc.encode(combined_context)[:max_context]\n",
    "        combined_context = enc.decode(combined_tokens)\n",
    "    \n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Based on the following information:\n",
    "\n",
    "{combined_context}\n",
    "\n",
    "Answer this question: {question}\"\"\"\n",
    "        }],\n",
    "        temperature=0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Final Answer:\")\n",
    "    md(final_response.choices[0].message.content)\n",
    "    \n",
    "    return final_response.choices[0].message.content\n",
    "\n",
    "# Test with our long document\n",
    "summarize_and_process(\n",
    "    long_document,\n",
    "    \"What is the transformer architecture and when was it introduced?\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Advanced Topic: Context Manager Class\n",
    "\n",
    "A **ContextManager** is a utility class that helps manage context window allocation for LLM API calls. It provides methods to:\n",
    "\n",
    "- Calculate available tokens for context after accounting for system prompts and expected response length\n",
    "- Select and fit context items (like RAG retrieval results) within the available token budget  \n",
    "- Prepare optimized requests that maximize the use of available context\n",
    "\n",
    "### Key Features:\n",
    "- **Token counting**: Calculate exact token usage for different text inputs\n",
    "- **Budget allocation**: Determine how much space is available for retrieved context\n",
    "- **Smart truncation**: Select as many context items as possible that fit within the limit\n",
    "- **Request preparation**: Format everything into a ready-to-use prompt structure\n",
    "\n",
    "### Example Use Cases:\n",
    "- RAG (Retrieval-Augmented Generation) systems that need to fit multiple retrieved documents\n",
    "- Chat applications with long conversation histories\n",
    "- Multi-document summarization tasks\n",
    "- Any scenario where you need to optimize context usage\n",
    "\n",
    "> **üìö Note**: Implementing a production-ready ContextManager involves additional considerations like token estimation accuracy, chunk prioritization strategies, and error handling. This advanced topic will be covered in depth in more advanced courses on LLM application development and RAG system architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Context Manager is an advanced topic for production LLM applications\n",
      "üìö It will be covered in detail in advanced courses on RAG and LLM app development\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"üí° Context Manager is an advanced topic for production LLM applications\")\n",
    "print(\"üìö It will be covered in detail in advanced courses on RAG and LLM app development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Context Window Basics**\n",
    "   - Context includes: system + history + user + context + response\n",
    "   - Different models have vastly different limits (8K to 1M tokens)\n",
    "   - Always reserve space for the response\n",
    "\n",
    "2. **Budget Planning**\n",
    "   - Calculate token usage before API calls\n",
    "   - Prioritize most relevant content\n",
    "   - Leave buffer for safety\n",
    "\n",
    "3. **Lost in the Middle**\n",
    "   - Models attend more to beginning and end\n",
    "   - Place important information strategically\n",
    "   - Consider reranking retrieved content\n",
    "\n",
    "4. **Long Content Strategies**\n",
    "   - Chunking with overlap\n",
    "   - Extract-then-synthesize\n",
    "   - Use context manager\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **mini-temperature**: Learn about generation parameters\n",
    "- **mini-sampling**: Explore Top-K and Top-P\n",
    "- **Module 4**: Apply context management in RAG systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_llm_agent",
   "language": "python",
   "name": "text_llm_agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
