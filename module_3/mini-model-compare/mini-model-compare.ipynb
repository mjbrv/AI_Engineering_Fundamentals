{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Mini-Lab: Comparing LLM Models\n",
    "\n",
    "**Module 2: LLM Core Concepts** | **Duration: ~45 min** | **Type: Mini-Lab**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this mini-lab, you will be able to:\n",
    "\n",
    "1. **Compare** responses from different LLM providers and models\n",
    "2. **Evaluate** model performance across different task types\n",
    "3. **Analyze** cost-performance trade-offs\n",
    "4. **Choose** the right model for specific use cases\n",
    "5. **Use** both cloud APIs and local models (Ollama) for comparison\n",
    "\n",
    "## Target Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| Model Comparison | Evaluating different LLMs on the same tasks |\n",
    "| Model Selection | Decision framework for choosing models |\n",
    "| Open vs Closed Models | Trade-offs between local open-source and cloud APIs |\n",
    "| Context Window | Maximum input size varies by model |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **mini-ollama-setup** (Module 1): For local model comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Anthropic client initialized\n",
      "‚úì OpenAI client initialized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Optional: Anthropic client\n",
    "try:\n",
    "    import anthropic\n",
    "    anthropic_client = anthropic.Anthropic()\n",
    "    HAS_ANTHROPIC = True\n",
    "    print(\"‚úì Anthropic client initialized\")\n",
    "except:\n",
    "    HAS_ANTHROPIC = False\n",
    "    print(\"‚úó Anthropic not available (optional)\")\n",
    "\n",
    "def md(text):\n",
    "    display(Markdown(text))\n",
    "\n",
    "print(\"‚úì OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Landscape Overview\n",
    "\n",
    "Understanding the current LLM landscape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üìä Model Comparison Overview\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Provider | Model | Context | Input $/M | Output $/M | Speed |\n",
       "|----------|-------|---------|-----------|------------|-------|\n",
       "| openai | gpt-4o | 128,000 | $2.50 | $10.00 | fast |\n",
       "| openai | gpt-4o-mini | 128,000 | $0.15 | $0.60 | very fast |\n",
       "| openai | o1-preview | 128,000 | $15.00 | $60.00 | slow (thinks) |\n",
       "| anthropic | claude-3-opus | 200,000 | $15.00 | $75.00 | moderate |\n",
       "| anthropic | claude-3-5-sonnet | 200,000 | $3.00 | $15.00 | fast |\n",
       "| anthropic | claude-3-haiku | 200,000 | $0.25 | $1.25 | very fast |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model specifications (as of 2024)\n",
    "MODELS = {\n",
    "    \"openai\": {\n",
    "        \"gpt-4o\": {\n",
    "            \"context\": 128_000,\n",
    "            \"cost_input\": 2.50,  # per million tokens\n",
    "            \"cost_output\": 10.00,\n",
    "            \"strengths\": [\"reasoning\", \"code\", \"multimodal\"],\n",
    "            \"speed\": \"fast\"\n",
    "        },\n",
    "        \"gpt-4o-mini\": {\n",
    "            \"context\": 128_000,\n",
    "            \"cost_input\": 0.15,\n",
    "            \"cost_output\": 0.60,\n",
    "            \"strengths\": [\"general\", \"fast\", \"cost-effective\"],\n",
    "            \"speed\": \"very fast\"\n",
    "        },\n",
    "        \"o1-preview\": {\n",
    "            \"context\": 128_000,\n",
    "            \"cost_input\": 15.00,\n",
    "            \"cost_output\": 60.00,\n",
    "            \"strengths\": [\"complex reasoning\", \"math\", \"science\"],\n",
    "            \"speed\": \"slow (thinks)\"\n",
    "        },\n",
    "    },\n",
    "    \"anthropic\": {\n",
    "        \"claude-3-opus\": {\n",
    "            \"context\": 200_000,\n",
    "            \"cost_input\": 15.00,\n",
    "            \"cost_output\": 75.00,\n",
    "            \"strengths\": [\"analysis\", \"writing\", \"reasoning\"],\n",
    "            \"speed\": \"moderate\"\n",
    "        },\n",
    "        \"claude-3-5-sonnet\": {\n",
    "            \"context\": 200_000,\n",
    "            \"cost_input\": 3.00,\n",
    "            \"cost_output\": 15.00,\n",
    "            \"strengths\": [\"balanced\", \"code\", \"analysis\"],\n",
    "            \"speed\": \"fast\"\n",
    "        },\n",
    "        \"claude-3-haiku\": {\n",
    "            \"context\": 200_000,\n",
    "            \"cost_input\": 0.25,\n",
    "            \"cost_output\": 1.25,\n",
    "            \"strengths\": [\"speed\", \"cost\", \"simple tasks\"],\n",
    "            \"speed\": \"very fast\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "def print_model_comparison():\n",
    "    \"\"\"Print model comparison table.\"\"\"\n",
    "    \n",
    "    md(\"## üìä Model Comparison Overview\\n\")\n",
    "    \n",
    "    table = \"| Provider | Model | Context | Input $/M | Output $/M | Speed |\\n\"\n",
    "    table += \"|----------|-------|---------|-----------|------------|-------|\\n\"\n",
    "    \n",
    "    for provider, models in MODELS.items():\n",
    "        for model_name, specs in models.items():\n",
    "            ctx = f\"{specs['context']:,}\"\n",
    "            table += f\"| {provider} | {model_name} | {ctx} | ${specs['cost_input']:.2f} | ${specs['cost_output']:.2f} | {specs['speed']} |\\n\"\n",
    "    \n",
    "    md(table)\n",
    "\n",
    "print_model_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Direct Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üìù Prompt\n",
       "> Explain the concept of recursion in programming. Use a simple example.\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying openai/gpt-4o-mini... ‚úì (6.83s)\n",
      "\n",
      "Querying openai/gpt-4o... ‚úì (7.16s)\n",
      "\n",
      "Querying anthropic/claude-3-5-sonnet-latest... ‚ùå Error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-sonnet-latest'}, 'request_id': 'req_011CXBqyQuE9xGU4qZ2wZSH2'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ü§ñ openai/gpt-4o-mini"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚è±Ô∏è Time: 6.83s | üìä Tokens: 20‚Üí300 | üí∞ Cost: $0.000183\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Recursion in programming is a technique where a function calls itself in order to solve a problem. This approach is often used to break down complex problems into simpler subproblems. A recursive function typically has two main components:\n",
       "\n",
       "1. **Base Case**: This is the condition under which the recursion stops. It prevents the function from calling itself indefinitely.\n",
       "2. **Recursive Case**: This is where the function calls itself with a modified argument, moving towards the base case.\n",
       "\n",
       "### Simple Example: Factorial Calculation\n",
       "\n",
       "A classic example of recursion is the calculation of the factorial of a number. The factorial of a non-negative integer \\( n \\) (denoted as \\( n! \\)) is the product of all positive integers less than or equal to \\( n \\). The factorial can be defined recursively as follows:\n",
       "\n",
       "- **Base Case**: \\( 0! = 1 \\) (by definition)\n",
       "- **Recursive Case**: \\( n! = n \\times (n-1)! \\) for \\( n > 0 \\)\n",
       "\n",
       "Here‚Äôs how you can implement this in Python:\n",
       "\n",
       "```python\n",
       "def factorial(n):\n",
       "    # Base case\n",
       "    if n == 0:\n",
       "        return 1\n",
       "    # Recursive case\n",
       "    else:\n",
       "        return n * factorial(n - 1)\n",
       "\n",
       "# Example usage\n",
       "print(factorial(5))  # Output: 120\n",
       "```\n",
       "\n",
       "### Explanation of the Example:\n",
       "\n",
       "1. **Base Case**: When `n` is\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ü§ñ openai/gpt-4o"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚è±Ô∏è Time: 7.16s | üìä Tokens: 20‚Üí300 | üí∞ Cost: $0.003050\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Recursion in programming is a technique where a function calls itself in order to solve a problem. This approach is often used to break down complex problems into simpler, more manageable sub-problems. A recursive function typically has two main components: a base case and a recursive case. The base case is a condition that stops the recursion, preventing it from continuing indefinitely. The recursive case is where the function calls itself with a modified argument, gradually working towards the base case.\n",
       "\n",
       "A classic example of recursion is the calculation of the factorial of a number. The factorial of a non-negative integer \\( n \\) (denoted as \\( n! \\)) is the product of all positive integers less than or equal to \\( n \\). The recursive definition of a factorial is:\n",
       "\n",
       "- Base case: \\( 0! = 1 \\)\n",
       "- Recursive case: \\( n! = n \\times (n-1)! \\) for \\( n > 0 \\)\n",
       "\n",
       "Here's a simple implementation of a recursive function to calculate the factorial of a number in Python:\n",
       "\n",
       "```python\n",
       "def factorial(n):\n",
       "    # Base case: if n is 0, return 1\n",
       "    if n == 0:\n",
       "        return 1\n",
       "    # Recursive case: n * factorial of (n-1)\n",
       "    else:\n",
       "        return n * factorial(n - 1)\n",
       "\n",
       "# Example usage\n",
       "print(factorial(5))  # Output: 120\n",
       "```\n",
       "\n",
       "In this example, the `factorial` function\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'openai/gpt-4o-mini': {'content': 'Recursion in programming is a technique where a function calls itself in order to solve a problem. This approach is often used to break down complex problems into simpler subproblems. A recursive function typically has two main components:\\n\\n1. **Base Case**: This is the condition under which the recursion stops. It prevents the function from calling itself indefinitely.\\n2. **Recursive Case**: This is where the function calls itself with a modified argument, moving towards the base case.\\n\\n### Simple Example: Factorial Calculation\\n\\nA classic example of recursion is the calculation of the factorial of a number. The factorial of a non-negative integer \\\\( n \\\\) (denoted as \\\\( n! \\\\)) is the product of all positive integers less than or equal to \\\\( n \\\\). The factorial can be defined recursively as follows:\\n\\n- **Base Case**: \\\\( 0! = 1 \\\\) (by definition)\\n- **Recursive Case**: \\\\( n! = n \\\\times (n-1)! \\\\) for \\\\( n > 0 \\\\)\\n\\nHere‚Äôs how you can implement this in Python:\\n\\n```python\\ndef factorial(n):\\n    # Base case\\n    if n == 0:\\n        return 1\\n    # Recursive case\\n    else:\\n        return n * factorial(n - 1)\\n\\n# Example usage\\nprint(factorial(5))  # Output: 120\\n```\\n\\n### Explanation of the Example:\\n\\n1. **Base Case**: When `n` is',\n",
       "  'time': 6.833609580993652,\n",
       "  'input_tokens': 20,\n",
       "  'output_tokens': 300},\n",
       " 'openai/gpt-4o': {'content': \"Recursion in programming is a technique where a function calls itself in order to solve a problem. This approach is often used to break down complex problems into simpler, more manageable sub-problems. A recursive function typically has two main components: a base case and a recursive case. The base case is a condition that stops the recursion, preventing it from continuing indefinitely. The recursive case is where the function calls itself with a modified argument, gradually working towards the base case.\\n\\nA classic example of recursion is the calculation of the factorial of a number. The factorial of a non-negative integer \\\\( n \\\\) (denoted as \\\\( n! \\\\)) is the product of all positive integers less than or equal to \\\\( n \\\\). The recursive definition of a factorial is:\\n\\n- Base case: \\\\( 0! = 1 \\\\)\\n- Recursive case: \\\\( n! = n \\\\times (n-1)! \\\\) for \\\\( n > 0 \\\\)\\n\\nHere's a simple implementation of a recursive function to calculate the factorial of a number in Python:\\n\\n```python\\ndef factorial(n):\\n    # Base case: if n is 0, return 1\\n    if n == 0:\\n        return 1\\n    # Recursive case: n * factorial of (n-1)\\n    else:\\n        return n * factorial(n - 1)\\n\\n# Example usage\\nprint(factorial(5))  # Output: 120\\n```\\n\\nIn this example, the `factorial` function\",\n",
       "  'time': 7.163080215454102,\n",
       "  'input_tokens': 20,\n",
       "  'output_tokens': 300}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_openai(model, prompt, max_tokens=300):\n",
    "    \"\"\"Query OpenAI model.\"\"\"\n",
    "    start = time.time()\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.choices[0].message.content,\n",
    "        \"time\": elapsed,\n",
    "        \"input_tokens\": response.usage.prompt_tokens,\n",
    "        \"output_tokens\": response.usage.completion_tokens\n",
    "    }\n",
    "\n",
    "def query_anthropic(model, prompt, max_tokens=300):\n",
    "    \"\"\"Query Anthropic model.\"\"\"\n",
    "    if not HAS_ANTHROPIC:\n",
    "        return None\n",
    "    \n",
    "    start = time.time()\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.content[0].text,\n",
    "        \"time\": elapsed,\n",
    "        \"input_tokens\": response.usage.input_tokens,\n",
    "        \"output_tokens\": response.usage.output_tokens\n",
    "    }\n",
    "\n",
    "def compare_models(prompt, models_to_test):\n",
    "    \"\"\"Compare multiple models on the same prompt.\"\"\"\n",
    "    \n",
    "    md(f\"### üìù Prompt\\n> {prompt}\\n\\n---\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_spec in models_to_test:\n",
    "        provider = model_spec[\"provider\"]\n",
    "        model = model_spec[\"model\"]\n",
    "        \n",
    "        print(f\"\\nQuerying {provider}/{model}...\", end=\" \")\n",
    "        \n",
    "        try:\n",
    "            if provider == \"openai\":\n",
    "                result = query_openai(model, prompt)\n",
    "            elif provider == \"anthropic\":\n",
    "                result = query_anthropic(model, prompt)\n",
    "                if result is None:\n",
    "                    print(\"‚ùå Anthropic not available\")\n",
    "                    continue\n",
    "            \n",
    "            results[f\"{provider}/{model}\"] = result\n",
    "            print(f\"‚úì ({result['time']:.2f}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    # Display results\n",
    "    for model_name, result in results.items():\n",
    "        specs = None\n",
    "        provider, model = model_name.split(\"/\")\n",
    "        if provider in MODELS and model in MODELS[provider]:\n",
    "            specs = MODELS[provider][model]\n",
    "        \n",
    "        cost = 0\n",
    "        if specs:\n",
    "            cost = (result['input_tokens'] / 1_000_000 * specs['cost_input'] +\n",
    "                   result['output_tokens'] / 1_000_000 * specs['cost_output'])\n",
    "        \n",
    "        md(f\"### ü§ñ {model_name}\")\n",
    "        md(f\"‚è±Ô∏è Time: {result['time']:.2f}s | üìä Tokens: {result['input_tokens']}‚Üí{result['output_tokens']} | üí∞ Cost: ${cost:.6f}\\n\")\n",
    "        md(f\"{result['content']}\\n\\n---\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with multiple models\n",
    "models_to_test = [\n",
    "    {\"provider\": \"openai\", \"model\": \"gpt-4o-mini\"},\n",
    "    {\"provider\": \"openai\", \"model\": \"gpt-4o\"},\n",
    "]\n",
    "\n",
    "if HAS_ANTHROPIC:\n",
    "    models_to_test.append({\"provider\": \"anthropic\", \"model\": \"claude-3-5-sonnet-latest\"})\n",
    "\n",
    "compare_models(\n",
    "    \"Explain the concept of recursion in programming. Use a simple example.\",\n",
    "    models_to_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Task-Specific Model Evaluation\n",
    "\n",
    "Different models excel at different tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üìã Task: REASONING"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Prompt:* A farmer has 17 sheep. All but 9 die. How many sheep are left? \n",
       "Think step by step and explain your ...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gpt-4o-mini** ‚úì (3.10s, 135 tokens)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> Let's break down the problem step by step:\n",
       "\n",
       "1. **Understanding the total number of sheep**: The farmer starts with a total of 17 sheep.\n",
       "\n",
       "2. **Interpreting \"all but 9 die\"**: The phrase \"all but 9 die\"...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gpt-4o** ‚úì (1.88s, 115 tokens)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> To solve this problem, let's break it down step by step:\n",
       "\n",
       "1. **Initial Count**: The farmer starts with 17 sheep.\n",
       "\n",
       "2. **Understanding \"All but 9 die\"**: The phrase \"all but 9 die\" means that out of the...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## üìã Task: CODE"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Prompt:* Write a Python function to check if a string is a palindrome....\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gpt-4o-mini** ‚úì (3.86s, 200 tokens)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> Certainly! A palindrome is a string that reads the same forwards and backwards. Here‚Äôs a simple Python function to check if a given string is a palindrome:\n",
       "\n",
       "```python\n",
       "def is_palindrome(s):\n",
       "    # Remov...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gpt-4o** ‚úì (6.55s, 200 tokens)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> Certainly! A palindrome is a string that reads the same forward and backward. Here's a Python function to check if a given string is a palindrome:\n",
       "\n",
       "```python\n",
       "def is_palindrome(s):\n",
       "    # Remove any non...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## üìã Task: CREATIVE"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Prompt:* Write a haiku about artificial intelligence....\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gpt-4o-mini** ‚úì (0.70s, 20 tokens)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> Silent circuits hum,  \n",
       "Thoughts woven in code and light,  \n",
       "Dreams of minds awake....\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gpt-4o** ‚úì (1.27s, 17 tokens)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> Silent circuits hum,  \n",
       "Thoughts emerge from coded light‚Äî  \n",
       "Machine dreams awake....\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## üìã Task: EXTRACTION"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Prompt:* Extract the following from this text:\n",
       "- Name\n",
       "- Company\n",
       "- Email\n",
       "\n",
       "Text: \"Hi, I'm Sarah Johnson from Te...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gpt-4o-mini** ‚úì (1.20s, 35 tokens)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ```json\n",
       "{\n",
       "  \"Name\": \"Sarah Johnson\",\n",
       "  \"Company\": \"TechCorp\",\n",
       "  \"Email\": \"sarah.j@techcorp.com\"\n",
       "}\n",
       "```...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gpt-4o** ‚úì (0.70s, 35 tokens)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ```json\n",
       "{\n",
       "  \"Name\": \"Sarah Johnson\",\n",
       "  \"Company\": \"TechCorp\",\n",
       "  \"Email\": \"sarah.j@techcorp.com\"\n",
       "}\n",
       "```...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## üìã Task: SUMMARIZATION"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Prompt:* Summarize this in one sentence:\n",
       "Machine learning is a subset of artificial intelligence that enables...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gpt-4o-mini** ‚úì (1.28s, 28 tokens)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> Machine learning, a subset of artificial intelligence, allows systems to autonomously learn and improve from experience by accessing and utilizing data without explicit programming....\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gpt-4o** ‚úì (0.60s, 23 tokens)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> Machine learning is a branch of artificial intelligence that allows systems to autonomously learn and improve from data without explicit programming....\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_task_types():\n",
    "    \"\"\"Evaluate models across different task types.\"\"\"\n",
    "    \n",
    "    tasks = {\n",
    "        \"reasoning\": {\n",
    "            \"prompt\": \"\"\"A farmer has 17 sheep. All but 9 die. How many sheep are left? \n",
    "Think step by step and explain your reasoning.\"\"\",\n",
    "            \"expected_contains\": \"9\"\n",
    "        },\n",
    "        \"code\": {\n",
    "            \"prompt\": \"Write a Python function to check if a string is a palindrome.\",\n",
    "            \"expected_contains\": \"def\"\n",
    "        },\n",
    "        \"creative\": {\n",
    "            \"prompt\": \"Write a haiku about artificial intelligence.\",\n",
    "            \"expected_contains\": None  # Subjective\n",
    "        },\n",
    "        \"extraction\": {\n",
    "            \"prompt\": \"\"\"Extract the following from this text:\n",
    "- Name\n",
    "- Company\n",
    "- Email\n",
    "\n",
    "Text: \"Hi, I'm Sarah Johnson from TechCorp. You can reach me at sarah.j@techcorp.com\"\n",
    "\n",
    "Return as JSON.\"\"\",\n",
    "            \"expected_contains\": \"sarah\"\n",
    "        },\n",
    "        \"summarization\": {\n",
    "            \"prompt\": \"\"\"Summarize this in one sentence:\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn \n",
    "and improve from experience without being explicitly programmed. It focuses on developing \n",
    "computer programs that can access data and use it to learn for themselves.\"\"\",\n",
    "            \"expected_contains\": None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    models = [\n",
    "        (\"openai\", \"gpt-4o-mini\"),\n",
    "        (\"openai\", \"gpt-4o\"),\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for task_name, task_info in tasks.items():\n",
    "        md(f\"## üìã Task: {task_name.upper()}\")\n",
    "        md(f\"*Prompt:* {task_info['prompt'][:100]}...\\n\")\n",
    "        \n",
    "        results[task_name] = {}\n",
    "        \n",
    "        for provider, model in models:\n",
    "            try:\n",
    "                result = query_openai(model, task_info['prompt'], max_tokens=200)\n",
    "                \n",
    "                # Check if expected content is present\n",
    "                success = \"‚úì\" if (task_info['expected_contains'] is None or \n",
    "                                  task_info['expected_contains'].lower() in result['content'].lower()) else \"?\"\n",
    "                \n",
    "                results[task_name][model] = {\n",
    "                    \"time\": result['time'],\n",
    "                    \"tokens\": result['output_tokens'],\n",
    "                    \"content\": result['content']\n",
    "                }\n",
    "                \n",
    "                md(f\"**{model}** {success} ({result['time']:.2f}s, {result['output_tokens']} tokens)\")\n",
    "                md(f\"> {result['content'][:200]}...\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                md(f\"**{model}** ‚ùå Error: {e}\\n\")\n",
    "        \n",
    "        md(\"---\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "task_results = evaluate_task_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cost-Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üí∞ Cost-Performance Analysis\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Prompt:* Write a brief explanation of what an API is and why it's important....\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Model | Avg Time | Avg Tokens | Cost/Call | Cost/1K Calls |\n",
       "|-------|----------|------------|-----------|---------------|\n",
       "| gpt-4o-mini | 4.30s | 200 | $0.000123 | $0.123 |\n",
       "| gpt-4o | 2.21s | 137 | $0.001419 | $1.419 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### üìä Comparison"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **gpt-4o** costs **11.5x** more than gpt-4o-mini"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Speed difference: 48.6% faster"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- For 1M API calls: $123.15 vs $1419.17"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'model': 'gpt-4o-mini',\n",
       "  'avg_time': 4.298131783803304,\n",
       "  'avg_tokens': 200.0,\n",
       "  'avg_cost': 0.00012315,\n",
       "  'cost_per_1k': 0.12315000000000001},\n",
       " {'model': 'gpt-4o',\n",
       "  'avg_time': 2.2095280488332114,\n",
       "  'avg_tokens': 136.66666666666666,\n",
       "  'avg_cost': 0.0014191666666666669,\n",
       "  'cost_per_1k': 1.419166666666667}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_cost_performance(prompt, num_runs=3):\n",
    "    \"\"\"Analyze cost vs performance for different models.\"\"\"\n",
    "    \n",
    "    models = [\n",
    "        (\"gpt-4o-mini\", 0.15, 0.60),\n",
    "        (\"gpt-4o\", 2.50, 10.00),\n",
    "    ]\n",
    "    \n",
    "    md(\"## üí∞ Cost-Performance Analysis\\n\")\n",
    "    md(f\"*Prompt:* {prompt[:80]}...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model, input_cost, output_cost in models:\n",
    "        times = []\n",
    "        token_counts = []\n",
    "        costs = []\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            result = query_openai(model, prompt, max_tokens=200)\n",
    "            times.append(result['time'])\n",
    "            token_counts.append(result['output_tokens'])\n",
    "            \n",
    "            cost = (result['input_tokens'] / 1_000_000 * input_cost +\n",
    "                   result['output_tokens'] / 1_000_000 * output_cost)\n",
    "            costs.append(cost)\n",
    "        \n",
    "        avg_time = sum(times) / len(times)\n",
    "        avg_tokens = sum(token_counts) / len(token_counts)\n",
    "        avg_cost = sum(costs) / len(costs)\n",
    "        \n",
    "        results.append({\n",
    "            \"model\": model,\n",
    "            \"avg_time\": avg_time,\n",
    "            \"avg_tokens\": avg_tokens,\n",
    "            \"avg_cost\": avg_cost,\n",
    "            \"cost_per_1k\": avg_cost * 1000\n",
    "        })\n",
    "    \n",
    "    # Display results\n",
    "    table = \"| Model | Avg Time | Avg Tokens | Cost/Call | Cost/1K Calls |\\n\"\n",
    "    table += \"|-------|----------|------------|-----------|---------------|\\n\"\n",
    "    \n",
    "    for r in results:\n",
    "        table += f\"| {r['model']} | {r['avg_time']:.2f}s | {r['avg_tokens']:.0f} | ${r['avg_cost']:.6f} | ${r['cost_per_1k']:.3f} |\\n\"\n",
    "    \n",
    "    md(table)\n",
    "    \n",
    "    # Calculate relative comparisons\n",
    "    if len(results) >= 2:\n",
    "        base = results[0]  # gpt-4o-mini as baseline\n",
    "        premium = results[1]  # gpt-4o\n",
    "        \n",
    "        cost_ratio = premium['avg_cost'] / base['avg_cost']\n",
    "        time_diff = ((base['avg_time'] - premium['avg_time']) / base['avg_time']) * 100\n",
    "        \n",
    "        md(f\"\\n### üìä Comparison\")\n",
    "        md(f\"- **{premium['model']}** costs **{cost_ratio:.1f}x** more than {base['model']}\")\n",
    "        md(f\"- Speed difference: {abs(time_diff):.1f}% {'faster' if time_diff > 0 else 'slower'}\")\n",
    "        md(f\"- For 1M API calls: ${base['cost_per_1k']*1000:.2f} vs ${premium['cost_per_1k']*1000:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "analyze_cost_performance(\n",
    "    \"Write a brief explanation of what an API is and why it's important.\",\n",
    "    num_runs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Selection Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCENARIO 1: High-volume customer support chatbot\n",
      "============================================================\n",
      "\n",
      "üéØ Model Recommendation\n",
      "==================================================\n",
      "Task Type: general\n",
      "Budget Sensitivity: high\n",
      "Quality Requirement: good\n",
      "Latency Requirement: realtime\n",
      "\n",
      "‚ú® Recommended Model: gpt-4o-mini\n",
      "\n",
      "üìù Additional Notes:\n",
      "   ‚Ä¢ Consider batching requests to reduce costs\n",
      "   ‚Ä¢ Enable streaming for better perceived latency\n",
      "\n",
      "============================================================\n",
      "SCENARIO 2: Code review assistant for developers\n",
      "============================================================\n",
      "\n",
      "üéØ Model Recommendation\n",
      "==================================================\n",
      "Task Type: code\n",
      "Budget Sensitivity: medium\n",
      "Quality Requirement: best\n",
      "Latency Requirement: fast\n",
      "\n",
      "‚ú® Recommended Model: gpt-4o-mini\n",
      "\n",
      "üìù Additional Notes:\n",
      "   ‚Ä¢ Consider adding code review/testing step\n",
      "\n",
      "============================================================\n",
      "SCENARIO 3: Data extraction from documents\n",
      "============================================================\n",
      "\n",
      "üéØ Model Recommendation\n",
      "==================================================\n",
      "Task Type: extraction\n",
      "Budget Sensitivity: high\n",
      "Quality Requirement: good\n",
      "Latency Requirement: flexible\n",
      "\n",
      "‚ú® Recommended Model: gpt-4o-mini\n",
      "\n",
      "üìù Additional Notes:\n",
      "   ‚Ä¢ Consider batching requests to reduce costs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gpt-4o-mini'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recommend_model(task_type, budget_sensitivity, quality_requirement, latency_requirement):\n",
    "    \"\"\"\n",
    "    Recommend a model based on requirements.\n",
    "    \n",
    "    Args:\n",
    "        task_type: \"general\", \"reasoning\", \"code\", \"creative\", \"extraction\"\n",
    "        budget_sensitivity: \"low\", \"medium\", \"high\"\n",
    "        quality_requirement: \"basic\", \"good\", \"best\"\n",
    "        latency_requirement: \"realtime\", \"fast\", \"flexible\"\n",
    "    \"\"\"\n",
    "    \n",
    "    recommendations = {\n",
    "        # (quality, budget, latency) -> model\n",
    "        (\"basic\", \"high\", \"realtime\"): \"gpt-4o-mini\",\n",
    "        (\"basic\", \"high\", \"fast\"): \"gpt-4o-mini\",\n",
    "        (\"basic\", \"medium\", \"realtime\"): \"gpt-4o-mini\",\n",
    "        (\"good\", \"medium\", \"fast\"): \"gpt-4o-mini\",\n",
    "        (\"good\", \"low\", \"fast\"): \"gpt-4o\",\n",
    "        (\"good\", \"low\", \"flexible\"): \"gpt-4o\",\n",
    "        (\"best\", \"low\", \"fast\"): \"gpt-4o\",\n",
    "        (\"best\", \"low\", \"flexible\"): \"gpt-4o\",\n",
    "    }\n",
    "    \n",
    "    # Special case for complex reasoning\n",
    "    if task_type == \"reasoning\" and quality_requirement == \"best\":\n",
    "        recommended = \"gpt-4o (or o1-preview for complex math/science)\"\n",
    "    else:\n",
    "        key = (quality_requirement, budget_sensitivity, latency_requirement)\n",
    "        recommended = recommendations.get(key, \"gpt-4o-mini\")\n",
    "    \n",
    "    print(f\"\\nüéØ Model Recommendation\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Task Type: {task_type}\")\n",
    "    print(f\"Budget Sensitivity: {budget_sensitivity}\")\n",
    "    print(f\"Quality Requirement: {quality_requirement}\")\n",
    "    print(f\"Latency Requirement: {latency_requirement}\")\n",
    "    print(f\"\\n‚ú® Recommended Model: {recommended}\")\n",
    "    \n",
    "    # Additional notes\n",
    "    notes = []\n",
    "    if budget_sensitivity == \"high\":\n",
    "        notes.append(\"Consider batching requests to reduce costs\")\n",
    "    if quality_requirement == \"best\" and task_type == \"code\":\n",
    "        notes.append(\"Consider adding code review/testing step\")\n",
    "    if latency_requirement == \"realtime\":\n",
    "        notes.append(\"Enable streaming for better perceived latency\")\n",
    "    \n",
    "    if notes:\n",
    "        print(\"\\nüìù Additional Notes:\")\n",
    "        for note in notes:\n",
    "            print(f\"   ‚Ä¢ {note}\")\n",
    "    \n",
    "    return recommended\n",
    "\n",
    "# Example scenarios\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCENARIO 1: High-volume customer support chatbot\")\n",
    "print(\"=\"*60)\n",
    "recommend_model(\"general\", \"high\", \"good\", \"realtime\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCENARIO 2: Code review assistant for developers\")\n",
    "print(\"=\"*60)\n",
    "recommend_model(\"code\", \"medium\", \"best\", \"fast\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCENARIO 3: Data extraction from documents\")\n",
    "print(\"=\"*60)\n",
    "recommend_model(\"extraction\", \"high\", \"good\", \"flexible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Reference: Model Selection Guide\n",
    "\n",
    "| Use Case | Primary Choice | Alternative | Why |\n",
    "|----------|---------------|-------------|-----|\n",
    "| **Simple Q&A** | gpt-4o-mini | - | Cost-effective, fast |\n",
    "| **Customer Support** | gpt-4o-mini | claude-3-haiku | High volume, cost matters |\n",
    "| **Code Generation** | gpt-4o | claude-3-5-sonnet | Quality critical |\n",
    "| **Complex Reasoning** | gpt-4o / o1 | claude-3-opus | Accuracy paramount |\n",
    "| **Creative Writing** | gpt-4o | claude-3-5-sonnet | Quality and style |\n",
    "| **Data Extraction** | gpt-4o-mini | - | Structured output, fast |\n",
    "| **Long Documents** | claude-3-5-sonnet | gpt-4o | 200K context |\n",
    "| **Image Analysis** | gpt-4o | claude-3-5-sonnet | Multimodal |\n",
    "\n",
    "### Key Decision Factors\n",
    "\n",
    "1. **Quality vs Cost**: Higher-tier models cost 10-40x more\n",
    "2. **Latency**: Mini/Haiku models are 2-3x faster\n",
    "3. **Context Length**: Claude has 200K, GPT-4 has 128K\n",
    "4. **Specialization**: o1 for math/science, Claude for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÄ Model Router Demo\n",
      "============================================================\n",
      "\n",
      "üìù Task: \"What is the capital of France?...\"\n",
      "   Type: simple ‚Üí Model: gpt-4o-mini\n",
      "\n",
      "üìù Task: \"Write a Python function to sort a list...\"\n",
      "   Type: code ‚Üí Model: gpt-4o\n",
      "\n",
      "üìù Task: \"Calculate the derivative of x^3 + 2x^2...\"\n",
      "   Type: math ‚Üí Model: gpt-4o\n",
      "\n",
      "üìù Task: \"Write a creative story about a robot...\"\n",
      "   Type: creative ‚Üí Model: gpt-4o\n",
      "\n",
      "üìù Task: \"Analyze the pros and cons of microservices archite...\"\n",
      "   Type: complex ‚Üí Model: gpt-4o\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def create_model_router(task_descriptions):\n",
    "    \"\"\"Create a simple model router based on task.\"\"\"\n",
    "    \n",
    "    routing_rules = {\n",
    "        \"simple\": \"gpt-4o-mini\",\n",
    "        \"complex\": \"gpt-4o\",\n",
    "        \"creative\": \"gpt-4o\",\n",
    "        \"code\": \"gpt-4o\",\n",
    "        \"math\": \"gpt-4o\",  # or o1 for complex math\n",
    "    }\n",
    "    \n",
    "    def classify_task(description):\n",
    "        \"\"\"Simple task classification.\"\"\"\n",
    "        desc_lower = description.lower()\n",
    "        \n",
    "        if any(word in desc_lower for word in [\"code\", \"function\", \"program\", \"debug\"]):\n",
    "            return \"code\"\n",
    "        elif any(word in desc_lower for word in [\"calculate\", \"math\", \"solve\", \"equation\"]):\n",
    "            return \"math\"\n",
    "        elif any(word in desc_lower for word in [\"write\", \"story\", \"creative\", \"poem\"]):\n",
    "            return \"creative\"\n",
    "        elif any(word in desc_lower for word in [\"analyze\", \"explain\", \"compare\", \"reason\"]):\n",
    "            return \"complex\"\n",
    "        else:\n",
    "            return \"simple\"\n",
    "    \n",
    "    print(\"\\nüîÄ Model Router Demo\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for desc in task_descriptions:\n",
    "        task_type = classify_task(desc)\n",
    "        model = routing_rules[task_type]\n",
    "        print(f\"\\nüìù Task: \\\"{desc[:50]}...\\\"\")\n",
    "        print(f\"   Type: {task_type} ‚Üí Model: {model}\")\n",
    "\n",
    "# Test the router\n",
    "test_tasks = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Write a Python function to sort a list\",\n",
    "    \"Calculate the derivative of x^3 + 2x^2\",\n",
    "    \"Write a creative story about a robot\",\n",
    "    \"Analyze the pros and cons of microservices architecture\",\n",
    "]\n",
    "\n",
    "create_model_router(test_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Local vs Cloud Models (Ollama Integration)\n",
    "\n",
    "If you completed **mini-ollama-setup** in Module 1, you can also compare local open-source models!\n",
    "\n",
    "### Open Source vs Proprietary Trade-offs\n",
    "\n",
    "| Aspect | Local (Ollama) | Cloud (OpenAI/Anthropic) |\n",
    "|--------|----------------|--------------------------|\n",
    "| **Privacy** | ‚úÖ Data stays local | ‚ùå Data sent to API |\n",
    "| **Cost** | ‚úÖ Free after setup | ‚ùå Per-token pricing |\n",
    "| **Quality** | ‚ö†Ô∏è Varies by model | ‚úÖ State-of-the-art |\n",
    "| **Setup** | ‚ö†Ô∏è Requires installation | ‚úÖ Just API key |\n",
    "| **Offline** | ‚úÖ Works offline | ‚ùå Requires internet |\n",
    "| **Customization** | ‚úÖ Can fine-tune | ‚ùå Limited options |\n",
    "\n",
    "### Popular Open Source Models\n",
    "\n",
    "| Model | Size | Comparable To | Best For |\n",
    "|-------|------|---------------|----------|\n",
    "| `llama3.2:8b` | 4.7GB | GPT-3.5 | General tasks |\n",
    "| `mistral:7b` | 4.1GB | GPT-3.5 | Fast inference |\n",
    "| `qwen2.5-coder:7b` | 4.4GB | GPT-4o-mini (code) | Code generation |\n",
    "| `phi3:mini` | 2.3GB | - | Edge/mobile |\n",
    "\n",
    "### Using Local Models in This Lab\n",
    "\n",
    "```python\n",
    "# If you have Ollama running (from Module 1):\n",
    "from openai import OpenAI\n",
    "\n",
    "# Cloud client (default)\n",
    "cloud_client = OpenAI()\n",
    "\n",
    "# Local client (Ollama)\n",
    "local_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "# Same code works for both!\n",
    "response = local_client.chat.completions.create(\n",
    "    model=\"llama3.2:8b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "```\n",
    "\n",
    "> **üí° Tip**: For development and learning, use local models to save API costs. Switch to cloud models for production or when you need the best quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Model Landscape**\n",
    "   - Multiple providers (OpenAI, Anthropic, Google)\n",
    "   - Significant cost differences (10-40x)\n",
    "   - Different strengths per model\n",
    "\n",
    "2. **Selection Criteria**\n",
    "   - Task complexity\n",
    "   - Budget constraints\n",
    "   - Latency requirements\n",
    "   - Context length needs\n",
    "\n",
    "3. **Cost Optimization**\n",
    "   - Start with gpt-4o-mini for most tasks\n",
    "   - Upgrade to gpt-4o only when needed\n",
    "   - Consider routing based on task type\n",
    "\n",
    "4. **Best Practices**\n",
    "   - Test with your actual data\n",
    "   - Monitor quality and costs\n",
    "   - Build model routing for production\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **lab-llm-playground**: Build interactive playground combining all concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_llm_agent",
   "language": "python",
   "name": "text_llm_agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
