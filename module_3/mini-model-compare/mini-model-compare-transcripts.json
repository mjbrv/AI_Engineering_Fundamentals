{
  "notebook": "mini-model-compare.ipynb",
  "module": "Module 2: LLM Core Concepts",
  "total_duration": "~45 min",
  "transcripts": [
    {
      "cell_index": 0,
      "cell_type": "markdown",
      "section": "Introduction",
      "transcript": "Welcome to Comparing LLM Models! This is super practical because, honestly, one of the most common questions I get is 'which model should I use?' And the answer is always 'it depends.' Today we're gonna figure out what it depends on. We'll compare different providers and models, evaluate them across task types, analyze cost versus performance tradeoffs, and build some mental models for making good choices. Let's get to it!"
    },
    {
      "cell_index": 1,
      "cell_type": "markdown",
      "section": "1. Setup",
      "transcript": "Let's set up our clients - we'll use both OpenAI and optionally Anthropic."
    },
    {
      "cell_index": 2,
      "cell_type": "code",
      "section": "1. Setup",
      "concepts": ["OpenAI client", "Anthropic client", "multi-provider setup"],
      "transcript": "Both clients initialized successfully - OpenAI and Anthropic. The Anthropic one is optional, wrapped in a try-except. If you don't have an Anthropic API key, that's fine, you can still follow along with the OpenAI comparisons."
    },
    {
      "cell_index": 3,
      "cell_type": "markdown",
      "section": "2. Model Landscape Overview",
      "transcript": "Let's start with the lay of the land. What models are out there and what do they offer?"
    },
    {
      "cell_index": 4,
      "cell_type": "code",
      "section": "2. Model Landscape Overview",
      "concepts": ["model specifications", "pricing comparison", "context window sizes", "model strengths"],
      "transcript": "This table tells you a lot! Look at the price differences - GPT-4o-mini is $0.15 per million input tokens, while GPT-4o is $2.50, and o1-preview is a whopping $15. That's 100x price range! Context windows vary too - 128K for OpenAI, 200K for Claude, and a million for Gemini. And each model has different strengths. GPT-4o-mini is fast and cheap, great for general tasks. GPT-4o is better for reasoning and code. o1-preview is the thinking model for complex problems. Claude is known for analysis and writing. Knowing this landscape helps you make informed choices."
    },
    {
      "cell_index": 5,
      "cell_type": "markdown",
      "section": "3. Direct Model Comparison",
      "transcript": "Let's actually run the same prompt through different models and see what we get."
    },
    {
      "cell_index": 6,
      "cell_type": "code",
      "section": "3. Direct Model Comparison",
      "concepts": ["query functions", "response timing", "token counting", "cost calculation", "side-by-side comparison"],
      "transcript": "We're asking both models to explain recursion with a simple example. GPT-4o-mini takes about 6.8 seconds, GPT-4o takes 7.2 seconds - pretty similar. The Anthropic call failed here due to a model name issue, but that's okay. What I want you to notice is the structure of comparison - same prompt, measure time, count tokens, calculate cost. This is the framework for evaluating models. Both give good answers, but one costs significantly more."
    },
    {
      "cell_index": 7,
      "cell_type": "markdown",
      "section": "4. Task-Specific Model Evaluation",
      "transcript": "Different tasks favor different models. Let's test this systematically."
    },
    {
      "cell_index": 8,
      "cell_type": "code",
      "section": "4. Task-Specific Model Evaluation",
      "concepts": ["task types", "reasoning tasks", "code generation", "creative tasks", "data extraction", "summarization"],
      "transcript": "We're running 5 different task types: reasoning, code, creative, extraction, and summarization. For the reasoning task - 'All but 9 sheep die' - both models get it right, this is a classic trick question. Code generation - both produce valid Python. Creative haiku - subjective but both sound good. Extraction - both correctly pull out the structured data. Summarization - both capture the key points. The interesting thing is timing and tokens. For simple tasks, mini often does just as well but faster and cheaper."
    },
    {
      "cell_index": 9,
      "cell_type": "markdown",
      "section": "5. Cost-Performance Analysis",
      "transcript": "Let's quantify the cost-performance tradeoff."
    },
    {
      "cell_index": 10,
      "cell_type": "code",
      "section": "5. Cost-Performance Analysis",
      "concepts": ["cost analysis", "performance metrics", "scaling costs", "model economics"],
      "transcript": "We run the same prompt 3 times each to get averages. GPT-4o-mini: average time around 1 second, cost of about $0.000043 per call. GPT-4o: similar time, but cost of about $0.000700 per call. That's... let me do the math... about 16x more expensive! Per 1,000 calls: roughly $0.04 vs $0.70. At a million calls, that's $40 versus $700. If the output quality is sufficient for your use case, mini saves you a LOT of money."
    },
    {
      "cell_index": 11,
      "cell_type": "markdown",
      "section": "6. Model Selection Framework",
      "transcript": "Let's build a decision framework to make this easier."
    },
    {
      "cell_index": 12,
      "cell_type": "code",
      "section": "6. Model Selection Framework",
      "concepts": ["model selection", "decision framework", "budget sensitivity", "quality requirements", "latency requirements"],
      "transcript": "This recommend_model function takes your requirements and suggests a model. Scenario 1: high-volume chatbot with high budget sensitivity, good quality, realtime latency - recommendation is gpt-4o-mini. Makes sense, you need speed and cost-effectiveness at scale. Scenario 2: code review assistant, medium budget, best quality - still recommends mini actually, but adds a note about code review testing steps. Scenario 3: data extraction, high budget sensitivity - mini again. The pattern is clear: start with mini, only upgrade when you genuinely need it."
    },
    {
      "cell_index": 13,
      "cell_type": "markdown",
      "section": "7. Quick Reference: Model Selection Guide",
      "transcript": "Here's a quick reference table you can bookmark."
    },
    {
      "cell_index": 14,
      "cell_type": "code",
      "section": "7. Quick Reference: Model Selection Guide",
      "concepts": ["model routing", "task classification", "automatic selection", "intelligent routing"],
      "transcript": "This model router demo shows automatic model selection based on task. 'What is the capital of France?' - simple task, routes to mini. 'Write a Python function' - code task, routes to gpt-4o. 'Calculate a derivative' - math, routes to gpt-4o. 'Write a creative story' - creative, routes to gpt-4o. 'Analyze pros and cons' - complex reasoning, routes to gpt-4o. This kind of routing is what you'd build for production - use the expensive model only when the task requires it."
    },
    {
      "cell_index": 15,
      "cell_type": "markdown",
      "section": "Summary",
      "transcript": "Key takeaways! The model landscape has multiple providers with significant cost differences - 10 to 40x between tiers. Selection criteria: task complexity, budget, latency needs, context length. Cost optimization strategy: start with gpt-4o-mini for most tasks, upgrade only when quality demands it. Consider intelligent routing based on task type. Best practices: test with your actual data, monitor quality and costs in production, and build that routing logic. From here, check out the main LLM playground lab to combine everything we've learned. Great work!"
    },
    {
      "cell_index": 16,
      "cell_type": "code",
      "section": "Summary",
      "concepts": [],
      "transcript": ""
    }
  ]
}
