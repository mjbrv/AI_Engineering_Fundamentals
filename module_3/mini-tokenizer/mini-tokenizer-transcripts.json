{
  "notebook": "mini-tokenizer.ipynb",
  "module": "Module 2: LLM Core Concepts",
  "total_duration": "~30 min",
  "transcripts": [
    {
      "cell_index": 0,
      "cell_type": "markdown",
      "section": "Introduction",
      "transcript": "Alright everyone, welcome to our Tokenizer Explorer mini-lab! So this one is super important because tokenization is really the foundation of how LLMs work. I mean, the model doesn't see text the way we do - it sees tokens. And understanding this has some really practical implications for your work. We're going to look at how text gets broken down into tokens, play with different tokenizers, compare how different models handle the same text, and even build a little cost estimation tool. Plus we'll explore some edge cases that might catch you off guard. By the end of this, you'll have a much better intuition for why your prompts cost what they cost and how to make them more efficient. Let's get into it!"
    },
    {
      "cell_index": 1,
      "cell_type": "markdown",
      "section": "1. Setup",
      "transcript": "Okay, let's get our environment ready. This is pretty quick - just need to import tiktoken, which is OpenAI's tokenizer library."
    },
    {
      "cell_index": 2,
      "cell_type": "code",
      "section": "1. Setup",
      "concepts": ["tiktoken", "tokenizer initialization", "GPT-4o tokenizer", "GPT-3.5 tokenizer"],
      "transcript": "So here we're importing tiktoken and setting up two different tokenizers - one for GPT-4o and one for GPT-3.5. And you can see in the output they have different names: o200k_base for GPT-4o and cl100k_base for GPT-3.5. These names refer to their vocabulary sizes and versions. The 'o200k' means roughly 200,000 tokens in the vocabulary, and 'cl100k' is about 100,000. Alright, both initialized successfully, we're good!"
    },
    {
      "cell_index": 3,
      "cell_type": "markdown",
      "section": "2. Understanding Byte-Pair Encoding (BPE)",
      "transcript": "Now let's talk about Byte-Pair Encoding - BPE. This is the algorithm that GPT models use for tokenization. The idea is pretty clever: you start with individual characters, then you iteratively merge the most common pairs together. Over time, you build up a vocabulary of subwords that balances frequency and meaning. So common words become single tokens, while rare words get split into pieces. Let's actually see this in action."
    },
    {
      "cell_index": 4,
      "cell_type": "code",
      "section": "2. Understanding Byte-Pair Encoding (BPE)",
      "concepts": ["tokenization visualization", "token IDs", "character-to-token ratio", "BPE"],
      "transcript": "Alright, so I've got this visualize_tokenization function that shows us exactly how text gets broken down. Let's try it with a simple example: 'Hello, world!'. And look at this - it's only 4 tokens! 'Hello' is one token, the comma is its own token, ' world' with the leading space is one token, and the exclamation mark is separate. See that ratio there? 3.25 characters per token. That's pretty close to the rule of thumb that one token is about four characters. And notice how each token has a unique ID - like 'Hello' is 13225. That ID is what the model actually works with internally."
    },
    {
      "cell_index": 5,
      "cell_type": "markdown",
      "section": "3. Token Patterns: What Gets Merged?",
      "transcript": "Now here's where it gets interesting. BPE creates tokens based on what was frequent in the training data. So common patterns become single tokens, but rare or unusual text gets split up more."
    },
    {
      "cell_index": 6,
      "cell_type": "code",
      "section": "3. Token Patterns: What Gets Merged?",
      "concepts": ["token patterns", "technical terms tokenization", "numbers tokenization", "code tokenization", "whitespace handling"],
      "transcript": "Let's look at different types of content and see how they tokenize differently. First, 'The quick brown fox' - common English words, and we get about 4.78 characters per token. Each word is basically its own token, nice and efficient. Now look at the technical terms: 'TensorFlow PyTorch LangChain'. These get split - 'Tensor' and 'Flow' are separate, 'Py' and 'Torch' are separate. Makes sense, these are newer terms that weren't as common in training. The numbers are really interesting! See how '1234567890' gets broken into chunks of three digits? And the dollar amount with commas - each comma is its own token. This is why numbers can be surprisingly expensive. Code tokenizes pretty well actually - look at 'def calculate_embedding', the underscore helps group things nicely. And finally, whitespace - tabs and newlines each need their own tokens. Something to keep in mind when formatting prompts!"
    },
    {
      "cell_index": 7,
      "cell_type": "markdown",
      "section": "4. Cross-Model Comparison",
      "transcript": "Okay, here's something that's really important for production work. Different models use different tokenizers, and that affects both your costs and how the model understands text."
    },
    {
      "cell_index": 8,
      "cell_type": "code",
      "section": "4. Cross-Model Comparison",
      "concepts": ["cross-model comparison", "vocabulary differences", "multilingual tokenization", "emoji tokenization"],
      "transcript": "Check this out - we're comparing GPT-4o and GPT-3.5 on the same text. 'Artificial Intelligence' - GPT-4o makes it 2 tokens, but GPT-3.5 splits it into 3! It breaks 'Artificial' into 'Art' and 'ificial'. So same text, different token counts. Now look at the Japanese text - this is really striking. GPT-4o handles it in just 2 tokens, but GPT-3.5 needs 4 and you can see those question mark symbols in the breakdown - that's actually bytes being shown. GPT-4o's newer tokenizer is much better at non-English languages. And emojis? Both struggle honestly - they get broken into multiple byte tokens. So if you're building something with lots of emoji, keep that cost in mind!"
    },
    {
      "cell_index": 9,
      "cell_type": "markdown",
      "section": "5. Token Count Estimation Tool",
      "transcript": "Let's build something practical now - a tool to estimate API costs. Because understanding token counts is directly tied to understanding your bill!"
    },
    {
      "cell_index": 10,
      "cell_type": "code",
      "section": "5. Token Count Estimation Tool",
      "concepts": ["cost estimation", "API pricing", "input vs output tokens", "production cost planning"],
      "transcript": "So I've got the current pricing here - GPT-4o is $2.50 per million input tokens and $10 for output. GPT-4o-mini is way cheaper at $0.15 and $0.60. This estimate_cost function takes a prompt and tells you exactly what it'll cost. Let's try it with a typical RAG prompt. For gpt-4o-mini: 78 input tokens, estimated 100 output tokens, total cost is... 0.00007 dollars. Per thousand calls that's only 7 cents! But look at gpt-4o - same prompt costs about $1.20 per thousand calls. That's about 17 times more expensive. This is why model selection really matters at scale."
    },
    {
      "cell_index": 11,
      "cell_type": "markdown",
      "section": "6. Edge Cases & Gotchas",
      "transcript": "Now let's look at some edge cases that can trip you up. These are the things that don't tokenize the way you might expect."
    },
    {
      "cell_index": 12,
      "cell_type": "code",
      "section": "6. Edge Cases & Gotchas",
      "concepts": ["tokenization edge cases", "repeated characters", "case sensitivity", "whitespace tokens", "URL tokenization", "Unicode handling"],
      "transcript": "Alright, this is really useful stuff. Repeated letters like 'aaaaaaa' - they don't become individual tokens, they get grouped. Case matters! 'Hello hello HELLO' - they're actually different tokens. The uppercase version gets split into 'HEL' and 'LO'. Watch out for leading versus trailing spaces too - a leading space often gets merged with the word, but a trailing space is its own token. URLs and emails tokenize pretty efficiently, which is nice. Template syntax like double curly braces gets recognized as a pattern. And then accented characters - look at 'café', the 'c' is separate from 'afé'. The arrows are really token-heavy. So if you're working with non-ASCII characters, definitely check how they tokenize before you deploy."
    },
    {
      "cell_index": 13,
      "cell_type": "markdown",
      "section": "7. Practical Exercise: Optimize a Prompt",
      "transcript": "Let's put this knowledge to use with a real prompt optimization exercise."
    },
    {
      "cell_index": 14,
      "cell_type": "code",
      "section": "7. Practical Exercise: Optimize a Prompt",
      "concepts": ["prompt optimization", "token efficiency", "cost reduction", "production scaling"],
      "transcript": "So here's a verbose prompt - lots of filler words like 'extremely helpful and knowledgeable' and 'please make sure to be thorough'. It's 81 tokens. Now look at the optimized version - same meaning, but we've cut the fluff. Just 22 tokens! That's a 72.8% reduction! Think about this at scale: over a million API calls, you'd save 59 million tokens. At GPT-4o-mini prices, that's real money. And at GPT-4o prices? Even more significant. This is why it pays to be intentional about your prompts."
    },
    {
      "cell_index": 15,
      "cell_type": "markdown",
      "section": "Summary",
      "transcript": "Alright, let's wrap up what we've learned. Tokens are the units LLMs actually process - roughly 4 characters or 0.75 words in English. BPE builds vocabulary from frequent patterns in training data. Different models use different tokenizers, so token counts vary. Costs are per token, not per character, so efficient prompts save money at scale. And watch out for edge cases - non-English text, numbers, and code can surprise you. From here, you should check out mini-context to learn about context window limits, and mini-temperature for generation parameters. Nice work everyone!"
    }
  ]
}
