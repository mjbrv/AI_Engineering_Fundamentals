{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß± Lab: LLM Playground\n",
        "\n",
        "**Module 2: LLM Core Concepts** | **Duration: ~1 hour** | **Type: Wall Lab**\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "1. **Understand** how tokenization works with different models\n",
        "2. **Experiment** with temperature and its effect on outputs\n",
        "3. **Compare** Top-K and Top-P sampling strategies\n",
        "4. **Implement** streaming responses for real-time output\n",
        "5. **Analyze** how different models respond to the same prompt\n",
        "\n",
        "## Concepts Covered\n",
        "\n",
        "| Concept | Section |\n",
        "|---------|---------|\n",
        "| Tokenization | 2 |\n",
        "| Token IDs | 2 |\n",
        "| Temperature | 3 |\n",
        "| Top-K Sampling | 4 |\n",
        "| Top-P Sampling | 4 |\n",
        "| Stop Sequences | 5 |\n",
        "| Streaming | 6 |\n",
        "| Model Comparison | 7 |\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- OpenAI API key\n",
        "- (Optional) Anthropic API key for model comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup (~5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Anthropic client initialized\n",
            "‚úì OpenAI client initialized\n",
            "‚úì Markdown helper md() ready\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import tiktoken\n",
        "import time\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Helper function to render LLM output as formatted markdown\n",
        "def md(text):\n",
        "    \"\"\"Display text as rendered markdown.\"\"\"\n",
        "    display(Markdown(text))\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Optional: Anthropic client\n",
        "try:\n",
        "    import anthropic\n",
        "    anthropic_client = anthropic.Anthropic()\n",
        "    HAS_ANTHROPIC = True\n",
        "    print(\"‚úì Anthropic client initialized\")\n",
        "except:\n",
        "    HAS_ANTHROPIC = False\n",
        "    print(\"‚úó Anthropic not available (optional)\")\n",
        "\n",
        "print(\"‚úì OpenAI client initialized\")\n",
        "print(\"‚úì Markdown helper md() ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tokenization Explorer (~10 min)\n",
        "\n",
        "Tokenization converts text into discrete units (tokens) that models can process. Different models use different tokenizers with different vocabularies.\n",
        "\n",
        "**Key insight**: 1 token ‚âà 4 characters or ~0.75 words in English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Text: 'Hello, world!'\n",
            "Token count: 4\n",
            "Tokens: [13225, 11, 2375, 0]\n",
            "\n",
            "Token breakdown:\n",
            "   13225 ‚Üí 'Hello'\n",
            "      11 ‚Üí ','\n",
            "    2375 ‚Üí ' world'\n",
            "       0 ‚Üí '!'\n",
            "\n",
            "==================================================\n",
            "Text: 'The transformer architecture revolutionized NLP'\n",
            "Token count: 6\n",
            "Tokens: [976, 59595, 24022, 25284, 2110, 161231]\n",
            "\n",
            "Token breakdown:\n",
            "     976 ‚Üí 'The'\n",
            "   59595 ‚Üí ' transformer'\n",
            "   24022 ‚Üí ' architecture'\n",
            "   25284 ‚Üí ' revolution'\n",
            "    2110 ‚Üí 'ized'\n",
            "  161231 ‚Üí ' NLP'\n",
            "\n",
            "==================================================\n",
            "Text: 'def hello():\n",
            "    print('hi')'\n",
            "Token count: 8\n",
            "Tokens: [1314, 40617, 8595, 271, 2123, 706, 3686, 1542]\n",
            "\n",
            "Token breakdown:\n",
            "    1314 ‚Üí 'def'\n",
            "   40617 ‚Üí ' hello'\n",
            "    8595 ‚Üí '():\n",
            "'\n",
            "     271 ‚Üí '   '\n",
            "    2123 ‚Üí ' print'\n",
            "     706 ‚Üí '(''\n",
            "    3686 ‚Üí 'hi'\n",
            "    1542 ‚Üí '')'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1314, 40617, 8595, 271, 2123, 706, 3686, 1542]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get tokenizer for GPT-4o\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "\n",
        "def explore_tokens(text):\n",
        "    \"\"\"Visualize how text is tokenized.\"\"\"\n",
        "    tokens = enc.encode(text)\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"Token count: {len(tokens)}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"\\nToken breakdown:\")\n",
        "    for token_id in tokens:\n",
        "        token_text = enc.decode([token_id])\n",
        "        print(f\"  {token_id:6d} ‚Üí '{token_text}'\")\n",
        "    return tokens\n",
        "\n",
        "# Explore different types of text\n",
        "print(\"=\" * 50)\n",
        "explore_tokens(\"Hello, world!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "explore_tokens(\"The transformer architecture revolutionized NLP\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "explore_tokens(\"def hello():\\n    print('hi')\")  # Code tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Temperature Effects (~10 min)\n",
        "\n",
        "Temperature controls the randomness of model outputs by scaling the logits before softmax:\n",
        "- **Low temperature (0-0.3)**: More deterministic, focused outputs\n",
        "- **Medium temperature (0.5-0.7)**: Balanced creativity and coherence\n",
        "- **High temperature (0.8-1.5)**: More creative, varied outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Prompt:** *Write a one-sentence story about a robot.*\n",
              "\n",
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### üå°Ô∏è Temperature = 0\n",
              "\n",
              "In a world where emotions were forbidden, a lonely robot discovered an old book of poetry and, for the first time, felt the warmth of longing in its metallic heart."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### üå°Ô∏è Temperature = 0.7\n",
              "\n",
              "In a world where emotions were forbidden, a forgotten robot discovered an old book of poetry and, for the first time, felt the warmth of hope blooming in its metallic heart."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### üå°Ô∏è Temperature = 1.2\n",
              "\n",
              "In a world where emotions were considered outdated, a forgotten robot in a dusty corner of the museum reactivated to the sound of laughter, awakening an ancient curiosity that led it on a quest to understand what it truly meant to feel."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def test_temperature(prompt, temperatures=[0, 0.5, 1.0, 1.5]):\n",
        "    \"\"\"Compare outputs at different temperatures.\"\"\"\n",
        "    md(f\"**Prompt:** *{prompt}*\\n\\n---\")\n",
        "    \n",
        "    for temp in temperatures:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt + \"\\n\\nRespond in markdown format.\"}],\n",
        "            temperature=temp,\n",
        "            max_tokens=100\n",
        "        )\n",
        "        md(f\"### üå°Ô∏è Temperature = {temp}\\n\\n{response.choices[0].message.content}\")\n",
        "\n",
        "# Test with creative prompt\n",
        "test_temperature(\"Write a one-sentence story about a robot.\", temperatures=[0, 0.7, 1.2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Sampling Strategies (~15 min)\n",
        "\n",
        "### Top-P (Nucleus Sampling)\n",
        "Selects from the smallest set of tokens whose cumulative probability exceeds P.\n",
        "\n",
        "### Top-K Sampling\n",
        "Selects from only the K most likely tokens (not directly supported in OpenAI API but important to understand)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: 'List 3 unique hobbies:'\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìä Top-P = 0.1\n",
            "----------------------------------------\n",
            "Sure! Here are three unique hobbies:\n",
            "\n",
            "1. **Geocaching**: This is a real-world outdoor treasure hunting game where participants use GPS devices or mobile apps to hide and seek containers, called \"geocaches\" or \"caches,\" at specific locations marked by coordinates.\n",
            "\n",
            "2. **Soap Making**: This creative hobby involves crafting your own soap from scratch using various oils, lye, and additives. It allows for customization in scents, colors, and shapes, making it both a\n",
            "\n",
            "üìä Top-P = 0.5\n",
            "----------------------------------------\n",
            "Sure! Here are three unique hobbies:\n",
            "\n",
            "1. **Geocaching**: This is a real-world outdoor treasure hunting game where participants use GPS devices or mobile apps to hide and seek containers, called \"geocaches\" or \"caches,\" at specific locations marked by coordinates.\n",
            "\n",
            "2. **Soap Making**: This creative hobby involves the process of making soap from scratch using oils, lye, and various additives like fragrances and colors. It allows for customization and can be both an art and\n",
            "\n",
            "üìä Top-P = 0.95\n",
            "----------------------------------------\n",
            "Here are three unique hobbies you might find interesting:\n",
            "\n",
            "1. **Urban Exploration (Urbex)**: This hobby involves exploring abandoned buildings, tunnels, and other urban structures. Enthusiasts often document their adventures through photography and writing, uncovering the history and stories behind these forgotten places.\n",
            "\n",
            "2. **Geocaching**: This outdoor recreational activity involves using GPS devices to find hidden containers, called \"geocaches,\" which can be located all over the world. Geocachers often leave small\n"
          ]
        }
      ],
      "source": [
        "def test_top_p(prompt, top_p_values=[0.1, 0.5, 0.9, 1.0]):\n",
        "    \"\"\"Compare outputs at different top_p values.\"\"\"\n",
        "    md(f\"**Prompt:** *{prompt}*\\n\\n---\")\n",
        "    \n",
        "    for top_p in top_p_values:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt + \"\\n\\nRespond in markdown format.\"}],\n",
        "            temperature=1.0,  # Use temperature=1 to see top_p effect\n",
        "            top_p=top_p,\n",
        "            max_tokens=100\n",
        "        )\n",
        "        md(f\"### üìä Top-P = {top_p}\\n\\n{response.choices[0].message.content}\")\n",
        "\n",
        "# Test top_p\n",
        "test_top_p(\"List 3 unique hobbies:\", top_p_values=[0.1, 0.5, 0.95])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Stop Sequences (~5 min)\n",
        "\n",
        "Stop sequences tell the model when to stop generating. Useful for controlling output format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With stop sequence '\\n4.':\n",
            "1. Apple  \n",
            "2. Banana  \n",
            "3. Cherry  \n",
            "\n",
            "==================================================\n",
            "\n",
            "Without stop sequence:\n",
            "1. Apple  \n",
            "2. Banana  \n",
            "3. Orange  \n",
            "4. Mango  \n",
            "5. Grapes  \n"
          ]
        }
      ],
      "source": [
        "# Stop sequences example\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"List 5 fruits, one per line:\"}],\n",
        "    stop=[\"\\n4.\"],  # Stop before the 4th item\n",
        "    max_tokens=100\n",
        ")\n",
        "print(\"With stop sequence '\\\\n4.':\")\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "# Without stop sequence\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"List 5 fruits, one per line:\"}],\n",
        "    max_tokens=100\n",
        ")\n",
        "print(\"\\nWithout stop sequence:\")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Streaming Responses (~10 min)\n",
        "\n",
        "Streaming allows you to receive tokens as they're generated, improving perceived latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Prompt:** *Explain what makes a good AI engineer in 2-3 sentences.*\n",
              "\n",
              "---\n",
              "\n",
              "### üìù Streaming Response\n",
              "\n",
              "A good AI engineer possesses a strong foundation in mathematics and programming, allowing them to design and implement complex algorithms efficiently. Additionally, they should be adept at problem-solving and have a deep understanding of machine learning frameworks and data structures, enabling them to adapt solutions to diverse challenges in the AI landscape. Effective communication skills are also crucial, as they often need to collaborate with cross-functional teams and explain technical concepts to non-technical stakeholders.‚ñå"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def stream_response(prompt):\n",
        "    \"\"\"Demonstrate streaming response with live markdown rendering.\"\"\"\n",
        "    full_response = \"\"\n",
        "    \n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt + \"\\n\\nRespond in markdown format.\"}],\n",
        "        stream=True,\n",
        "        max_tokens=150\n",
        "    )\n",
        "    \n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content:\n",
        "            content = chunk.choices[0].delta.content\n",
        "            full_response += content\n",
        "            # Clear and re-render markdown on each chunk for live streaming effect\n",
        "            clear_output(wait=True)\n",
        "            md(f\"**Prompt:** *{prompt}*\\n\\n---\\n\\n### üìù Streaming Response\\n\\n{full_response}‚ñå\")\n",
        "    \n",
        "\n",
        "# Test streaming\n",
        "stream_response(\"Explain what makes a good AI engineer in 2-3 sentences.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Comparison (~10 min)\n",
        "\n",
        "Compare how different models respond to the same prompt. This helps you understand model characteristics and choose the right model for your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Prompt:** *What's the most important skill for an AI engineer to develop in 2026? Answer in 2 sentences.*\n",
              "\n",
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### ü§ñ GPT-4o-mini\n",
              "\n",
              "In 2026, the most important skill for an AI engineer will be the ability to integrate ethical considerations into AI system development, ensuring that models are fair, transparent, and accountable. Additionally, proficiency in advanced machine learning techniques, particularly in areas like explainability and bias reduction, will be crucial to navigate the evolving landscape of AI applications."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### ü§ñ GPT-4o\n",
              "\n",
              "In 2026, the most important skill for an AI engineer to develop will be advanced proficiency in ethical AI design, ensuring responsible and fair implementation of AI technologies. Additionally, staying adept in the latest machine learning frameworks, especially those geared towards edge computing and quantum computing, will be crucial for innovation and efficiency."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### ü§ñ Claude Sonnet\n",
              "\n",
              "**Adaptability and continuous learning** will be the most critical skill for AI engineers in 2026, as the field is evolving at an unprecedented pace with new architectures, frameworks, and paradigms emerging constantly. The ability to quickly understand, evaluate, and implement novel AI techniques while maintaining ethical considerations and system reliability will separate exceptional engineers from those who fall behind the rapidly advancing curve."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def compare_models(prompt):\n",
        "    \"\"\"Compare responses from different models.\"\"\"\n",
        "    md(f\"**Prompt:** *{prompt}*\\n\\n---\")\n",
        "    \n",
        "    # Add markdown instruction to prompt\n",
        "    full_prompt = prompt + \"\\n\\nRespond in markdown format.\"\n",
        "    \n",
        "    # OpenAI GPT-4o-mini\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    md(f\"### ü§ñ GPT-4o-mini\\n\\n{response.choices[0].message.content}\")\n",
        "    \n",
        "    # OpenAI GPT-4o\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    md(f\"### ü§ñ GPT-4o\\n\\n{response.choices[0].message.content}\")\n",
        "    \n",
        "    # Anthropic Claude (if available)\n",
        "    if HAS_ANTHROPIC:\n",
        "        try:\n",
        "            # Try latest Claude model first, fall back to alternatives\n",
        "            claude_models = [\"claude-sonnet-4-20250514\", \"claude-3-5-sonnet-latest\", \"claude-3-sonnet-20240229\"]\n",
        "            for model_name in claude_models:\n",
        "                try:\n",
        "                    response = anthropic_client.messages.create(\n",
        "                        model=model_name,\n",
        "                        max_tokens=150,\n",
        "                        messages=[{\"role\": \"user\", \"content\": full_prompt}]\n",
        "                    )\n",
        "                    md(f\"### ü§ñ Claude Sonnet\\n\\n{response.content[0].text}\")\n",
        "                    break\n",
        "                except Exception:\n",
        "                    continue\n",
        "            else:\n",
        "                md(\"### ü§ñ Claude Sonnet\\n\\n‚ö†Ô∏è No compatible Claude model found\")\n",
        "        except Exception as e:\n",
        "            md(f\"### ü§ñ Claude Sonnet\\n\\n‚ö†Ô∏è Claude API error: {e}\")\n",
        "    else:\n",
        "        md(\"### ü§ñ Claude Sonnet\\n\\n‚ö†Ô∏è Anthropic not available - skipping Claude comparison\")\n",
        "\n",
        "# Compare models on a reasoning task\n",
        "compare_models(\"What's the most important skill for an AI engineer to develop in 2026? Answer in 2 sentences.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Summary\n",
        "\n",
        "In this lab, you explored:\n",
        "\n",
        "1. **Tokenization** - How text gets broken into tokens and why it matters for cost and context limits\n",
        "2. **Temperature** - Controlling randomness and creativity in outputs\n",
        "3. **Top-P Sampling** - Another way to control output diversity\n",
        "4. **Stop Sequences** - Precise control over when generation stops\n",
        "5. **Streaming** - Real-time token delivery for better UX\n",
        "6. **Model Comparison** - Understanding different model characteristics\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- Use **low temperature** (0-0.3) for factual, deterministic tasks\n",
        "- Use **higher temperature** (0.7-1.2) for creative tasks\n",
        "- **Streaming** significantly improves perceived latency\n",
        "- Different models have different strengths - always test your specific use case!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "text_llm_agent",
      "language": "python",
      "name": "text_llm_agent"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
