{
  "notebook": "mini-sampling.ipynb",
  "module": "Module 2: LLM Core Concepts",
  "total_duration": "~45 min",
  "transcripts": [
    {
      "cell_index": 0,
      "cell_type": "markdown",
      "section": "Introduction",
      "transcript": "Hey everyone! Welcome to Sampling Strategies. So if you've done the temperature mini-lab, you know temperature controls randomness. But there's more to the story! Top-K and Top-P sampling give you additional ways to shape the model's output. They work together with temperature for really fine-grained control. By the end of this, you'll understand exactly how to tune these parameters for any use case. Let's dive in!"
    },
    {
      "cell_index": 1,
      "cell_type": "markdown",
      "section": "1. Setup",
      "transcript": "Quick setup as usual."
    },
    {
      "cell_index": 2,
      "cell_type": "code",
      "section": "1. Setup",
      "concepts": ["OpenAI client", "numpy", "environment setup"],
      "transcript": "Standard imports - OpenAI, numpy for the probability math, and our markdown helper. Ready to go!"
    },
    {
      "cell_index": 3,
      "cell_type": "markdown",
      "section": "2. Understanding Top-K Sampling",
      "transcript": "Let's start with Top-K. The idea is simple: instead of sampling from all possible tokens, you only consider the K most likely ones."
    },
    {
      "cell_index": 4,
      "cell_type": "code",
      "section": "2. Understanding Top-K Sampling",
      "concepts": ["Top-K sampling", "probability filtering", "token selection", "renormalization"],
      "transcript": "Let me simulate this. We have 7 tokens with different probabilities. 'the' is at 36.4%, 'a' at 27%, and so on. With Top-K=3, we only keep 'the', 'a', and 'one' - marked with the arrows. Then we renormalize so they sum to 100%. Now 'the' is 44.7%, 'a' is 33.1%, 'one' is 22.2%. The less likely options like 'those' at 1.8% are completely excluded. With Top-K=5, we include more options - 'that' and 'it' also get a chance. So K controls how many candidates you consider. Higher K means more diversity, lower K means more focused."
    },
    {
      "cell_index": 5,
      "cell_type": "markdown",
      "section": "3. Understanding Top-P (Nucleus) Sampling",
      "transcript": "Now Top-P is a bit different and honestly more elegant for most use cases."
    },
    {
      "cell_index": 6,
      "cell_type": "code",
      "section": "3. Understanding Top-P (Nucleus) Sampling",
      "concepts": ["Top-P sampling", "nucleus sampling", "cumulative probability", "adaptive selection"],
      "transcript": "Top-P - also called nucleus sampling - includes tokens until their cumulative probability reaches P. So with Top-P=0.5, we start adding tokens by probability: 'the' at 36.4%, cumulative 36.4%. Then 'a' at 27%, cumulative is now 63.4%. That exceeds 50%, so we stop! Only 2 tokens included. With Top-P=0.9, we need to reach 90% cumulative, so we include 'the', 'a', 'one', 'that', 'it' - five tokens. See how it adapts to the distribution? That's the key insight."
    },
    {
      "cell_index": 7,
      "cell_type": "markdown",
      "section": "4. Top-K vs Top-P: Key Differences",
      "transcript": "So what's the actual difference in practice?"
    },
    {
      "cell_index": 8,
      "cell_type": "code",
      "section": "4. Top-K vs Top-P: Key Differences",
      "concepts": ["distribution adaptation", "peaked vs flat distributions", "adaptive sampling"],
      "transcript": "This comparison is really illuminating. Look at the peaked distribution - 'token_1' has 86.8% probability. If you use Top-K=3, you're including two more tokens that together have like 7% probability. Kind of wasteful. But Top-P=0.9? It would just include token_1 since it already exceeds 90% on its own! Now look at the flat distribution - each token has around 15-19% probability. Top-K=3 only includes the top three. But Top-P=0.9 needs to include almost all of them to reach 90%! So Top-P adapts to the distribution shape, Top-K doesn't. That's why Top-P is generally preferred."
    },
    {
      "cell_index": 9,
      "cell_type": "markdown",
      "section": "5. Combining Temperature + Sampling",
      "transcript": "The real power comes from combining these parameters."
    },
    {
      "cell_index": 10,
      "cell_type": "code",
      "section": "5. Combining Temperature + Sampling",
      "concepts": ["parameter combination", "temperature and top_p", "output diversity", "creative control"],
      "transcript": "Temperature first modifies the distribution, then Top-P filters it. Let's see different combos on a creative prompt. Conservative - T=0.3, top_p=0.5: outputs are similar across runs, very focused. Balanced - T=0.7, top_p=0.9: some variety but coherent. Creative - T=1.0, top_p=0.95: now we're getting genuinely different completions each run. Wild - T=1.2, top_p=1.0: maximum exploration, can get pretty out there. The diversity indicator shows how many unique outputs we got. You can see the progression from 'Same' to multiple unique variations."
    },
    {
      "cell_index": 11,
      "cell_type": "markdown",
      "section": "6. Real-World Sampling Scenarios",
      "transcript": "Let's look at optimal settings for real tasks."
    },
    {
      "cell_index": 12,
      "cell_type": "code",
      "section": "6. Real-World Sampling Scenarios",
      "concepts": ["practical settings", "code completion settings", "content generation settings", "brainstorming settings", "data extraction settings"],
      "transcript": "Four real scenarios here. Code generation: T=0, top_p=1.0 - we want deterministic, correct code, so no randomness. The factorial function comes out clean and consistent. Content generation for marketing: T=0.7, top_p=0.9 - creative but coherent, you get nice product copy. Brainstorming startup ideas: T=1.0, top_p=0.95 - maximum diversity, because the whole point is to get varied ideas. Data extraction: T=0, top_p=0.1 - ultra-focused, there's one right answer. Notice how we're not double-restricting - if temp is 0, we keep top_p reasonable, and vice versa."
    },
    {
      "cell_index": 13,
      "cell_type": "markdown",
      "section": "7. Quick Reference: Sampling Settings",
      "transcript": "Here's your cheat sheet for sampling settings."
    },
    {
      "cell_index": 14,
      "cell_type": "code",
      "section": "7. Quick Reference: Sampling Settings",
      "concepts": ["experimentation", "parameter tuning", "iterative testing"],
      "transcript": "This experiment function is great for playing around. We're generating metaphors for learning to code at T=0.9, top_p=0.95. Three runs, three different metaphors! One about planting a garden, another about tending a garden... actually they're similar in theme but different in execution. The point is, these settings give you useful variety. Try changing the parameters and see how the outputs change!"
    },
    {
      "cell_index": 15,
      "cell_type": "markdown",
      "section": "Summary",
      "transcript": "Alright, let's recap! Top-K limits to K highest-probability tokens - fixed size regardless of distribution. Not directly available in OpenAI API but good to understand. Top-P includes tokens until cumulative probability reaches P - adapts to distribution shape, available as the top_p parameter. When combining: temperature first modifies the distribution, then Top-P filters. Don't over-restrict with both low temp and low top_p - pick one to be the main control. Practical guidelines: use temperature as primary creativity control, add top_p below 1 to prevent outliers, and always test with your specific use case. Next up, mini-logprobs to see actual probability distributions, mini-streaming for real-time delivery, and the main playground lab to combine everything!"
    }
  ]
}
