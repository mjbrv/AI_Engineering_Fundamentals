{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Mini-Lab: Token Probabilities\n",
    "\n",
    "**Module 2: LLM Core Concepts** | **Duration: ~30 min** | **Type: Mini-Lab**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this mini-lab, you will be able to:\n",
    "\n",
    "1. **Understand** what log probabilities (logprobs) represent\n",
    "2. **Access** token-level probabilities from OpenAI API\n",
    "3. **Analyze** model confidence for each generated token\n",
    "4. **Use** logprobs for classification and uncertainty detection\n",
    "\n",
    "## Target Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| Log Probabilities | Natural log of token probability, measuring model confidence |\n",
    "| Tokenization | Converting text to tokens (prerequisite) |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **mini-tokenizer**: Understanding of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "def md(text):\n",
    "    display(Markdown(text))\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Log Probabilities\n",
    "\n",
    "**Log probability (logprob)** = natural log of probability\n",
    "\n",
    "| Probability | Log Probability | Interpretation |\n",
    "|-------------|-----------------|----------------|\n",
    "| 100% (1.0) | 0.0 | Certain |\n",
    "| 50% (0.5) | -0.69 | Uncertain |\n",
    "| 10% (0.1) | -2.30 | Unlikely |\n",
    "| 1% (0.01) | -4.61 | Very unlikely |\n",
    "\n",
    "**Why log probabilities?**\n",
    "- Numerical stability (avoid underflow)\n",
    "- Addition instead of multiplication for sequences\n",
    "- Standard in ML/statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Probability ‚Üî Log Probability Conversion\n",
      "==================================================\n",
      "P = 99.00%  ‚Üí  logprob = -0.010  (üü¢ High)\n",
      "P = 90.00%  ‚Üí  logprob = -0.105  (üü¢ High)\n",
      "P = 75.00%  ‚Üí  logprob = -0.288  (üü° Medium)\n",
      "P = 50.00%  ‚Üí  logprob = -0.693  (üî¥ Low)\n",
      "P = 25.00%  ‚Üí  logprob = -1.386  (üî¥ Low)\n",
      "P = 10.00%  ‚Üí  logprob = -2.303  (üî¥ Low)\n",
      "P = 1.00%  ‚Üí  logprob = -4.605  (üî¥ Low)\n"
     ]
    }
   ],
   "source": [
    "def prob_to_logprob(p):\n",
    "    \"\"\"Convert probability to log probability.\"\"\"\n",
    "    return math.log(p)\n",
    "\n",
    "def logprob_to_prob(lp):\n",
    "    \"\"\"Convert log probability to probability.\"\"\"\n",
    "    return math.exp(lp)\n",
    "\n",
    "# Interactive conversion\n",
    "print(\"üìä Probability ‚Üî Log Probability Conversion\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "probabilities = [0.99, 0.90, 0.75, 0.50, 0.25, 0.10, 0.01]\n",
    "for p in probabilities:\n",
    "    lp = prob_to_logprob(p)\n",
    "    confidence = \"üü¢ High\" if p > 0.8 else \"üü° Medium\" if p > 0.5 else \"üî¥ Low\"\n",
    "    print(f\"P = {p:.2%}  ‚Üí  logprob = {lp:.3f}  ({confidence})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting Logprobs from OpenAI API\n",
    "\n",
    "Use `logprobs=True` to get probability information for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of France is Paris.\n",
      "\n",
      "Logprobs available: True\n"
     ]
    }
   ],
   "source": [
    "def get_response_with_logprobs(prompt, max_tokens=50):\n",
    "    \"\"\"Get response with token-level log probabilities.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0,\n",
    "        logprobs=True,\n",
    "        top_logprobs=5  # Get top 5 alternatives for each position\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test\n",
    "response = get_response_with_logprobs(\"The capital of France is\")\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(f\"\\nLogprobs available: {response.choices[0].logprobs is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Token-by-Token Analysis\n",
      "======================================================================\n",
      "\n",
      "[0] 'The'\n",
      "    Prob: 99.98% (logprob: -0.000) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üü¢ Certain\n",
      "    Alternatives:\n",
      "      'The': 99.98%\n",
      "      'Paris': 0.01%\n",
      "      'the': 0.00%\n",
      "\n",
      "[1] ' capital'\n",
      "    Prob: 100.00% (logprob: 0.000) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üü¢ Certain\n",
      "    Alternatives:\n",
      "      ' capital': 100.00%\n",
      "      'capital': 0.00%\n",
      "      ' Capital': 0.00%\n",
      "\n",
      "[2] ' of'\n",
      "    Prob: 100.00% (logprob: 0.000) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üü¢ Certain\n",
      "    Alternatives:\n",
      "      ' of': 100.00%\n",
      "      ' city': 0.00%\n",
      "      'of': 0.00%\n",
      "\n",
      "[3] ' France'\n",
      "    Prob: 100.00% (logprob: 0.000) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üü¢ Certain\n",
      "    Alternatives:\n",
      "      ' France': 100.00%\n",
      "      'France': 0.00%\n",
      "      ' Paris': 0.00%\n",
      "\n",
      "[4] ' is'\n",
      "    Prob: 100.00% (logprob: 0.000) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üü¢ Certain\n",
      "    Alternatives:\n",
      "      ' is': 100.00%\n",
      "      ' ŸáŸà': 0.00%\n",
      "      'is': 0.00%\n",
      "\n",
      "[5] ' Paris'\n",
      "    Prob: 100.00% (logprob: -0.000) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üü¢ Certain\n",
      "    Alternatives:\n",
      "      ' Paris': 100.00%\n",
      "      'Paris': 0.00%\n",
      "      ' –ü–∞—Ä–∏': 0.00%\n",
      "\n",
      "[6] '.'\n",
      "    Prob: 100.00% (logprob: -0.000) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üü¢ Certain\n",
      "    Alternatives:\n",
      "      '.': 100.00%\n",
      "      '.\\n': 0.00%\n",
      "      '.\\n\\n': 0.00%\n"
     ]
    }
   ],
   "source": [
    "def visualize_logprobs(response):\n",
    "    \"\"\"Visualize token probabilities from response.\"\"\"\n",
    "    \n",
    "    if not response.choices[0].logprobs:\n",
    "        print(\"No logprobs in response!\")\n",
    "        return\n",
    "    \n",
    "    content = response.choices[0].logprobs.content\n",
    "    \n",
    "    print(\"\\nüìä Token-by-Token Analysis\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, token_data in enumerate(content):\n",
    "        token = token_data.token\n",
    "        logprob = token_data.logprob\n",
    "        prob = logprob_to_prob(logprob)\n",
    "        \n",
    "        # Confidence indicator\n",
    "        if prob > 0.95:\n",
    "            conf = \"üü¢ Certain\"\n",
    "        elif prob > 0.75:\n",
    "            conf = \"üü° Confident\"\n",
    "        elif prob > 0.50:\n",
    "            conf = \"üü† Moderate\"\n",
    "        else:\n",
    "            conf = \"üî¥ Uncertain\"\n",
    "        \n",
    "        # Visual bar\n",
    "        bar = \"‚ñà\" * int(prob * 20)\n",
    "        \n",
    "        # Display token (escape special chars)\n",
    "        display_token = token.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "        \n",
    "        print(f\"\\n[{i}] '{display_token}'\")\n",
    "        print(f\"    Prob: {prob:6.2%} (logprob: {logprob:.3f}) {bar} {conf}\")\n",
    "        \n",
    "        # Show top alternatives\n",
    "        if token_data.top_logprobs:\n",
    "            print(f\"    Alternatives:\")\n",
    "            for alt in token_data.top_logprobs[:3]:\n",
    "                alt_prob = logprob_to_prob(alt.logprob)\n",
    "                alt_token = alt.token.replace('\\n', '\\\\n')\n",
    "                print(f\"      '{alt_token}': {alt_prob:.2%}\")\n",
    "\n",
    "visualize_logprobs(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing Model Confidence\n",
    "\n",
    "Use logprobs to understand when the model is confident vs uncertain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FACTUAL PROMPT\n",
      "======================================================================\n",
      "\n",
      "üìù Prompt: What is 2 + 2?\n",
      "üì§ Response: 2 + 2 equals 4.\n",
      "\n",
      "üìä Confidence Statistics:\n",
      "   Average confidence: 99.9%\n",
      "   Minimum confidence: 99.1% at token ' equals'\n",
      "   Total tokens: 8\n",
      "\n",
      "‚úÖ All tokens above 50% confidence\n",
      "\n",
      "======================================================================\n",
      "CREATIVE PROMPT\n",
      "======================================================================\n",
      "\n",
      "üìù Prompt: Write a creative word that describes happiness:\n",
      "üì§ Response: \"Joyglow\" - a radiant state of happiness that illuminates one's spirit and brightens the world around them.\n",
      "\n",
      "üìä Confidence Statistics:\n",
      "   Average confidence: 77.2%\n",
      "   Minimum confidence: 27.4% at token 'gl'\n",
      "   Total tokens: 24\n",
      "\n",
      "‚ö†Ô∏è Uncertain tokens (< 50% confidence):\n",
      "   [2] 'gl': 27.4%\n",
      "   [5] ' -': 43.1%\n",
      "   [7] ' radiant': 46.8%\n",
      "   [8] ' state': 35.7%\n",
      "   [12] ' illumin': 39.7%\n",
      "   [14] ' one's': 34.1%\n",
      "   [17] ' bright': 31.9%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.611696846736773,\n",
       " 0.9819413556270229,\n",
       " 0.2741468890900173,\n",
       " 0.9912431846933826,\n",
       " 0.9997252355748864,\n",
       " 0.4311681117146683,\n",
       " 0.927823599740573,\n",
       " 0.4683588870072995,\n",
       " 0.3569191407471089,\n",
       " 0.9990793160283619,\n",
       " 0.9308707953959569,\n",
       " 0.9989319161051103,\n",
       " 0.3974039176757734,\n",
       " 1.0,\n",
       " 0.34078324763253887,\n",
       " 0.8061138397206027,\n",
       " 0.9397901664200573,\n",
       " 0.3192055297680872,\n",
       " 1.0,\n",
       " 0.9996458892352142,\n",
       " 0.7737587027519198,\n",
       " 0.9999932502087602,\n",
       " 0.9852807830011578,\n",
       " 0.9875489701375568]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_confidence(prompt):\n",
    "    \"\"\"Analyze model confidence across the response.\"\"\"\n",
    "    \n",
    "    response = get_response_with_logprobs(prompt, max_tokens=30)\n",
    "    content = response.choices[0].logprobs.content\n",
    "    \n",
    "    tokens = []\n",
    "    probs = []\n",
    "    \n",
    "    for token_data in content:\n",
    "        tokens.append(token_data.token)\n",
    "        probs.append(logprob_to_prob(token_data.logprob))\n",
    "    \n",
    "    avg_prob = np.mean(probs)\n",
    "    min_prob = np.min(probs)\n",
    "    min_idx = np.argmin(probs)\n",
    "    \n",
    "    print(f\"\\nüìù Prompt: {prompt}\")\n",
    "    print(f\"üì§ Response: {response.choices[0].message.content}\")\n",
    "    print(f\"\\nüìä Confidence Statistics:\")\n",
    "    print(f\"   Average confidence: {avg_prob:.1%}\")\n",
    "    print(f\"   Minimum confidence: {min_prob:.1%} at token '{tokens[min_idx]}'\")\n",
    "    print(f\"   Total tokens: {len(tokens)}\")\n",
    "    \n",
    "    # Identify uncertain tokens\n",
    "    uncertain = [(i, t, p) for i, (t, p) in enumerate(zip(tokens, probs)) if p < 0.5]\n",
    "    \n",
    "    if uncertain:\n",
    "        print(f\"\\n‚ö†Ô∏è Uncertain tokens (< 50% confidence):\")\n",
    "        for idx, token, prob in uncertain:\n",
    "            print(f\"   [{idx}] '{token}': {prob:.1%}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All tokens above 50% confidence\")\n",
    "    \n",
    "    return probs\n",
    "\n",
    "# Compare factual vs creative prompts\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FACTUAL PROMPT\")\n",
    "print(\"=\"*70)\n",
    "analyze_confidence(\"What is 2 + 2?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATIVE PROMPT\")\n",
    "print(\"=\"*70)\n",
    "analyze_confidence(\"Write a creative word that describes happiness:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Logprobs for Classification\n",
    "\n",
    "A powerful use case: get classification confidence without generating text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè∑Ô∏è Sentiment Classification with Confidence\n",
      "============================================================\n",
      "\n",
      "üìù \"This product is absolutely amazing! Best purchase ...\"\n",
      "   Prediction: Positive\n",
      "   Confidence: 100.0% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üü¢\n",
      "\n",
      "üìù \"Terrible experience. Would not recommend....\"\n",
      "   Prediction: Negative\n",
      "   Confidence: 100.0% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üü¢\n",
      "\n",
      "üìù \"The package arrived on time....\"\n",
      "   Prediction: Positive\n",
      "   Confidence: 100.0% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üü¢\n",
      "\n",
      "üìù \"It's okay, nothing special but not bad either....\"\n",
      "   Prediction: Neutral\n",
      "   Confidence: 100.0% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà üü¢\n"
     ]
    }
   ],
   "source": [
    "def classify_with_confidence(text, categories):\n",
    "    \"\"\"Classify text and get confidence from logprobs.\"\"\"\n",
    "    \n",
    "    # Format categories for prompt\n",
    "    cat_str = \"/\".join(categories)\n",
    "    \n",
    "    prompt = f\"\"\"Classify the following text into one category: {cat_str}\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "Category (respond with only the category name):\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=10,\n",
    "        temperature=0,\n",
    "        logprobs=True,\n",
    "        top_logprobs=5\n",
    "    )\n",
    "    \n",
    "    # Get the first meaningful token\n",
    "    predicted = response.choices[0].message.content.strip()\n",
    "    first_token = response.choices[0].logprobs.content[0]\n",
    "    confidence = logprob_to_prob(first_token.logprob)\n",
    "    \n",
    "    # Get alternative classifications\n",
    "    alternatives = []\n",
    "    for alt in first_token.top_logprobs:\n",
    "        alt_text = alt.token.strip().lower()\n",
    "        for cat in categories:\n",
    "            if alt_text in cat.lower():\n",
    "                alternatives.append((cat, logprob_to_prob(alt.logprob)))\n",
    "                break\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"prediction\": predicted,\n",
    "        \"confidence\": confidence,\n",
    "        \"alternatives\": alternatives\n",
    "    }\n",
    "\n",
    "# Test sentiment classification\n",
    "categories = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "test_texts = [\n",
    "    \"This product is absolutely amazing! Best purchase ever!\",\n",
    "    \"Terrible experience. Would not recommend.\",\n",
    "    \"The package arrived on time.\",\n",
    "    \"It's okay, nothing special but not bad either.\",\n",
    "]\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Sentiment Classification with Confidence\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_texts:\n",
    "    result = classify_with_confidence(text, categories)\n",
    "    \n",
    "    conf_bar = \"‚ñà\" * int(result['confidence'] * 20)\n",
    "    conf_level = \"üü¢\" if result['confidence'] > 0.9 else \"üü°\" if result['confidence'] > 0.7 else \"üî¥\"\n",
    "    \n",
    "    print(f\"\\nüìù \\\"{text[:50]}...\\\"\")\n",
    "    print(f\"   Prediction: {result['prediction']}\")\n",
    "    print(f\"   Confidence: {result['confidence']:.1%} {conf_bar} {conf_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perplexity: Measuring Overall Confidence\n",
    "\n",
    "**Perplexity** measures how \"surprised\" the model is by a sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PERPLEXITY COMPARISON\n",
      "============================================================\n",
      "\n",
      "üìù Prompt: What is 1 + 1?\n",
      "üì§ Response: 1 + 1 equals 2....\n",
      "\n",
      "üìä Perplexity: 1.00\n",
      "   Interpretation: Very confident (common/predictable output)\n",
      "\n",
      "üìù Prompt: Explain quantum computing briefly.\n",
      "üì§ Response: Quantum computing is a type of computation that leverages the principles of quantum mechanics to pro...\n",
      "\n",
      "üìä Perplexity: 1.09\n",
      "   Interpretation: Very confident (common/predictable output)\n",
      "\n",
      "üìù Prompt: Write a haiku about a purple elephant.\n",
      "üì§ Response: In twilight's embrace,  \n",
      "A purple elephant roams,  \n",
      "Dreams in gentle steps....\n",
      "\n",
      "üìä Perplexity: 1.38\n",
      "   Interpretation: Very confident (common/predictable output)\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(logprobs):\n",
    "    \"\"\"Calculate perplexity from log probabilities.\"\"\"\n",
    "    # Perplexity = exp(-average_logprob)\n",
    "    avg_logprob = np.mean(logprobs)\n",
    "    perplexity = math.exp(-avg_logprob)\n",
    "    return perplexity\n",
    "\n",
    "def analyze_perplexity(prompt):\n",
    "    \"\"\"Analyze response perplexity.\"\"\"\n",
    "    \n",
    "    response = get_response_with_logprobs(prompt, max_tokens=50)\n",
    "    content = response.choices[0].logprobs.content\n",
    "    \n",
    "    logprobs = [t.logprob for t in content]\n",
    "    perplexity = calculate_perplexity(logprobs)\n",
    "    \n",
    "    print(f\"\\nüìù Prompt: {prompt}\")\n",
    "    print(f\"üì§ Response: {response.choices[0].message.content[:100]}...\")\n",
    "    print(f\"\\nüìä Perplexity: {perplexity:.2f}\")\n",
    "    \n",
    "    if perplexity < 2:\n",
    "        print(\"   Interpretation: Very confident (common/predictable output)\")\n",
    "    elif perplexity < 5:\n",
    "        print(\"   Interpretation: Moderately confident\")\n",
    "    else:\n",
    "        print(\"   Interpretation: Uncertain (uncommon/creative output)\")\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Compare different types of prompts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERPLEXITY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompts = [\n",
    "    \"What is 1 + 1?\",  # Should be very confident\n",
    "    \"Explain quantum computing briefly.\",  # Technical but known\n",
    "    \"Write a haiku about a purple elephant.\",  # Creative/uncertain\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    analyze_perplexity(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Application: Hallucination Detection\n",
    "\n",
    "Low-confidence tokens might indicate potential hallucinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Hallucination Detection Analysis\n",
      "============================================================\n",
      "üìù Prompt: tell me how many oceans are living on the moon\n",
      "üì§ Response: The Moon does not have any oceans like Earth does. While there are features on the Moon's surface that resemble large bodies of water, such as the dark basaltic plains known as \"maria,\" these are not oceans but rather solidified lava flows. The Moon has no liquid water bodies, and any water present is primarily in the form of ice, found in permanently shadowed craters at the poles.\n",
      "\n",
      "üìä Threshold: 30% confidence\n",
      "\n",
      "‚ö†Ô∏è Found 1 potentially uncertain claims:\n",
      "\n",
      "   Token: ' large'\n",
      "   Confidence: 24.1%\n",
      "   Context: '... that resemble large bodies of...'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'token': ' large',\n",
       "  'probability': 0.24137756356103526,\n",
       "  'context': ' that resemble large bodies of',\n",
       "  'position': 22}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def detect_potential_hallucinations(prompt, confidence_threshold=0.3):\n",
    "    \"\"\"Identify potentially hallucinated content based on low confidence.\"\"\"\n",
    "    \n",
    "    response = get_response_with_logprobs(prompt, max_tokens=100)\n",
    "    content = response.choices[0].logprobs.content\n",
    "    \n",
    "    full_response = response.choices[0].message.content\n",
    "    \n",
    "    # Find low-confidence regions\n",
    "    suspicious = []\n",
    "    for i, token_data in enumerate(content):\n",
    "        prob = logprob_to_prob(token_data.logprob)\n",
    "        if prob < confidence_threshold:\n",
    "            # Get surrounding context\n",
    "            start = max(0, i - 2)\n",
    "            end = min(len(content), i + 3)\n",
    "            context = \"\".join([content[j].token for j in range(start, end)])\n",
    "            suspicious.append({\n",
    "                \"token\": token_data.token,\n",
    "                \"probability\": prob,\n",
    "                \"context\": context,\n",
    "                \"position\": i\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nüîç Hallucination Detection Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìù Prompt: {prompt}\")\n",
    "    print(f\"üì§ Response: {full_response}\")\n",
    "    print(f\"\\nüìä Threshold: {confidence_threshold:.0%} confidence\")\n",
    "    \n",
    "    if suspicious:\n",
    "        print(f\"\\n‚ö†Ô∏è Found {len(suspicious)} potentially uncertain claims:\")\n",
    "        for item in suspicious[:5]:  # Show top 5\n",
    "            print(f\"\\n   Token: '{item['token']}'\")\n",
    "            print(f\"   Confidence: {item['probability']:.1%}\")\n",
    "            print(f\"   Context: '...{item['context']}...'\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No tokens below {confidence_threshold:.0%} confidence\")\n",
    "    \n",
    "    return suspicious\n",
    "\n",
    "# Test with a prompt that might cause hallucinations\n",
    "detect_potential_hallucinations(\n",
    "    \"tell me how many oceans are living on the moon\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Log Probabilities Basics**\n",
    "   - logprob = ln(probability)\n",
    "   - 0 = certain, more negative = less confident\n",
    "   - Convert: prob = e^logprob\n",
    "\n",
    "2. **Accessing Logprobs**\n",
    "   - Use `logprobs=True` in API call\n",
    "   - `top_logprobs=N` for alternatives\n",
    "   - Available per-token in response\n",
    "\n",
    "3. **Use Cases**\n",
    "   - **Confidence measurement**: Know when model is uncertain\n",
    "   - **Classification**: Get probability for each class\n",
    "   - **Hallucination detection**: Flag low-confidence claims\n",
    "   - **Quality filtering**: Reject low-confidence outputs\n",
    "\n",
    "4. **Perplexity**\n",
    "   - Aggregate measure of sequence confidence\n",
    "   - Lower = more predictable/confident\n",
    "   - Useful for comparing outputs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **mini-streaming**: Real-time token delivery\n",
    "- **mini-model-compare**: Compare models using logprobs\n",
    "- **Module 8**: Use logprobs in evaluation pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_llm_agent",
   "language": "python",
   "name": "text_llm_agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
