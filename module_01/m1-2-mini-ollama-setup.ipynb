{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß© Mini-Lab: Local LLM Setup with Ollama\n",
        "\n",
        "**Module 1: Setup & Working Style for LLM Apps** | **Duration: ~20 min** | **Type: Mini-Lab**\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this mini-lab, you will be able to:\n",
        "\n",
        "1. **Understand** open vs closed models\n",
        "2. **Install and configure** Ollama for local LLM inference\n",
        "3. **Use** local models through the OpenAI-compatible API\n",
        "\n",
        "## Target Concepts\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| Open vs Closed Models | Understanding model access paradigms |\n",
        "| Local LLMs | Running models locally for privacy and cost savings |\n",
        "| Ollama | Easy-to-use local LLM deployment tool |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Open vs Closed Models\n",
        "\n",
        "### üîì Open Source Models (Open Weights)\n",
        "\n",
        "| Model | Provider | Parameters | Best For |\n",
        "|-------|----------|------------|----------|\n",
        "| **Llama 3.2** | Meta | 1B-70B | General purpose |\n",
        "| **Mistral** | Mistral AI | 7B | Fast inference |\n",
        "| **Phi-3** | Microsoft | 3.8B | Edge devices |\n",
        "| **Qwen 2.5** | Alibaba | 0.5B-72B | Coding, math |\n",
        "\n",
        "**Benefits:** ‚úÖ Run locally, ‚úÖ Free, ‚úÖ Privacy, ‚úÖ Customizable\n",
        "\n",
        "### üîí Closed/Proprietary Models (API Only)\n",
        "\n",
        "| Model | Provider | Best For |\n",
        "|-------|----------|----------|\n",
        "| **GPT-4o** | OpenAI | Multimodal, reasoning |\n",
        "| **Claude 3** | Anthropic | Analysis, long context |\n",
        "| **Gemini Pro** | Google | Multimodal |\n",
        "\n",
        "**Benefits:** ‚úÖ Best quality, ‚úÖ Easy to use, ‚úÖ No hardware needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Installing Ollama\n",
        "\n",
        "Ollama makes running local LLMs easy - like Docker for AI models.\n",
        "\n",
        "### Installation\n",
        "\n",
        "**macOS:**\n",
        "```bash\n",
        "brew install ollama\n",
        "```\n",
        "\n",
        "**Windows:**\n",
        "1. Download from [ollama.ai/download](https://ollama.ai/download)\n",
        "2. Run the installer\n",
        "3. Ollama runs as a background service\n",
        "\n",
        "**Linux:**\n",
        "```bash\n",
        "curl -fsSL https://ollama.ai/install.sh | sh\n",
        "```\n",
        "\n",
        "### Start Ollama Server\n",
        "\n",
        "On Windows, Ollama starts automatically. On Mac/Linux:\n",
        "```bash\n",
        "ollama serve\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import requests\n",
        "\n",
        "def check_ollama():\n",
        "    \"\"\"Check if Ollama is installed and running.\"\"\"\n",
        "    \n",
        "    print(\"üîç Checking Ollama Installation\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check CLI\n",
        "    try:\n",
        "        result = subprocess.run([\"ollama\", \"--version\"], capture_output=True, text=True, timeout=5)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"‚úÖ Ollama CLI: {result.stdout.strip()}\")\n",
        "        else:\n",
        "            print(\"‚ùå Ollama CLI not found\")\n",
        "            return False\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå Ollama not installed - download from ollama.ai\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "    \n",
        "    # Check server\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Ollama server running on port 11434\")\n",
        "            models = response.json().get(\"models\", [])\n",
        "            if models:\n",
        "                print(f\"\\nüì¶ Installed models:\")\n",
        "                for m in models:\n",
        "                    size_gb = m.get(\"size\", 0) / (1024**3)\n",
        "                    print(f\"   - {m['name']} ({size_gb:.1f} GB)\")\n",
        "            else:\n",
        "                print(\"\\nüì¶ No models installed yet\")\n",
        "            return True\n",
        "    except requests.ConnectionError:\n",
        "        print(\"‚ùå Ollama server not running\")\n",
        "        print(\"   Start with: ollama serve\")\n",
        "        return False\n",
        "    \n",
        "    return False\n",
        "\n",
        "check_ollama()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download a Model\n",
        "\n",
        "### Recommended Models\n",
        "\n",
        "| Model | Size | RAM | Best For |\n",
        "|-------|------|-----|----------|\n",
        "| `llama3.2:3b` | ~2GB | 4GB+ | Learning |\n",
        "| `llama3.2:8b` | ~5GB | 8GB+ | General use |\n",
        "| `qwen2.5-coder:7b` | ~4GB | 8GB+ | Coding |\n",
        "\n",
        "### Download Command\n",
        "\n",
        "Run in your terminal (not in Jupyter):\n",
        "\n",
        "```bash\n",
        "# Small model - good for testing\n",
        "ollama pull llama3.2:3b\n",
        "\n",
        "# Test it\n",
        "ollama run llama3.2:3b \"What is Python?\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_models():\n",
        "    \"\"\"List available Ollama models.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            return [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
        "    except:\n",
        "        pass\n",
        "    return []\n",
        "\n",
        "models = list_models()\n",
        "if models:\n",
        "    print(f\"‚úÖ {len(models)} model(s) available: {models}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No models installed\")\n",
        "    print(\"   Run: ollama pull llama3.2:3b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Using Ollama with Python\n",
        "\n",
        "Ollama provides an **OpenAI-compatible API** - same code, different backend!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Create client pointing to Ollama\n",
        "ollama_client = OpenAI(\n",
        "    base_url=\"http://localhost:11434/v1\",\n",
        "    api_key=\"ollama\"  # Required but not used\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Ollama client initialized\")\n",
        "print(\"   Base URL: http://localhost:11434/v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_ollama(model_name: str = None):\n",
        "    \"\"\"Test a local model.\"\"\"\n",
        "    \n",
        "    # Auto-select model\n",
        "    if model_name is None:\n",
        "        available = list_models()\n",
        "        if not available:\n",
        "            print(\"‚ùå No models available\")\n",
        "            print(\"   Run: ollama pull llama3.2:3b\")\n",
        "            return False\n",
        "        model_name = available[0]\n",
        "    \n",
        "    print(f\"\\nüß™ Testing: {model_name}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    try:\n",
        "        response = ollama_client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Say 'Hello from local LLM!' in exactly those words.\"}],\n",
        "            max_tokens=30,\n",
        "            temperature=0\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Success!\")\n",
        "        print(f\"üì§ Response: {response.choices[0].message.content}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return False\n",
        "\n",
        "test_ollama()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Switching Between Local and Cloud\n",
        "\n",
        "Use the same code for both providers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from typing import Literal\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def get_client(provider: Literal[\"openai\", \"ollama\"] = \"openai\") -> tuple[OpenAI, str]:\n",
        "    \"\"\"Get LLM client for the specified provider.\"\"\"\n",
        "    \n",
        "    if provider == \"openai\":\n",
        "        client = OpenAI()  # Uses OPENAI_API_KEY\n",
        "        model = \"gpt-4o-mini\"\n",
        "    else:  # ollama\n",
        "        client = OpenAI(\n",
        "            base_url=\"http://localhost:11434/v1\",\n",
        "            api_key=\"ollama\"\n",
        "        )\n",
        "        models = list_models()\n",
        "        model = models[0] if models else \"llama3.2:3b\"\n",
        "    \n",
        "    return client, model\n",
        "\n",
        "\n",
        "def ask(question: str, provider: str = \"openai\"):\n",
        "    \"\"\"Ask a question using specified provider.\"\"\"\n",
        "    \n",
        "    client, model = get_client(provider)\n",
        "    print(f\"\\nü§ñ Using: {provider} / {model}\")\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": question}],\n",
        "        max_tokens=100,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    print(f\"üí¨ {response.choices[0].message.content}\")\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with OpenAI\n",
        "ask(\"What is 2 + 2?\", provider=\"openai\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with Ollama (if available)\n",
        "if list_models():\n",
        "    ask(\"What is 2 + 2?\", provider=\"ollama\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No Ollama models - skipping\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. When to Use Each\n",
        "\n",
        "### Use **Local (Ollama)** When:\n",
        "- üîí Processing sensitive data\n",
        "- üí∞ High volume, simple tasks\n",
        "- üåê Offline environments\n",
        "- üß™ Learning & experimentation\n",
        "\n",
        "### Use **Cloud (OpenAI)** When:\n",
        "- üéØ Maximum quality needed\n",
        "- üöÄ Production applications\n",
        "- üì± Limited local hardware\n",
        "- üìä Complex reasoning tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Summary\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Open vs Closed Models**\n",
        "   - Open: Llama, Mistral, Phi - run locally, customize\n",
        "   - Closed: GPT-4, Claude - API only, best quality\n",
        "\n",
        "2. **Ollama Setup**\n",
        "   - Install from ollama.ai\n",
        "   - Pull models: `ollama pull llama3.2:3b`\n",
        "   - OpenAI-compatible API on port 11434\n",
        "\n",
        "3. **Python Integration**\n",
        "   - Same `openai` library, different `base_url`\n",
        "   - Easy to switch between local and cloud\n",
        "\n",
        "### Setup Checklist\n",
        "\n",
        "- [ ] Ollama installed and running\n",
        "- [ ] At least one model downloaded\n",
        "- [ ] Tested both OpenAI and Ollama\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **lab-hello-llm**: Build a CLI that works with both providers"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
