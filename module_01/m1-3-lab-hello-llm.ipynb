{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß± Lab: Hello LLM - Your First AI App\n",
        "\n",
        "**Module 1: Setup & Working Style for LLM Apps** | **Duration: ~30 min** | **Type: Lab (Wall)**\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will have built a working LLM application that:\n",
        "\n",
        "1. **Loads** API keys securely from environment variables\n",
        "2. **Switches** between cloud (OpenAI) and local (Ollama) models\n",
        "3. **Controls** determinism with temperature and seed\n",
        "4. **Logs** interactions for debugging\n",
        "\n",
        "## Prerequisites (Concepts Covered)\n",
        "\n",
        "| Concept | From |\n",
        "|---------|------|\n",
        "| API Keys & Environment Variables | mini-env-setup |\n",
        "| Secrets Hygiene | mini-env-setup |\n",
        "| Open vs Closed Models | mini-ollama-setup |\n",
        "| Local LLMs & Ollama | mini-ollama-setup |\n",
        "| Determinism Controls | This lab |\n",
        "| Basic Logging | This lab |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Project Setup\n",
        "\n",
        "First, let's set up our project structure using uv:\n",
        "\n",
        "```bash\n",
        "# Create and navigate to project\n",
        "uv init hello-llm && cd hello-llm\n",
        "\n",
        "# Add dependencies\n",
        "uv add openai python-dotenv\n",
        "```\n",
        "\n",
        "Your `.env` file should have:\n",
        "```bash\n",
        "OPENAI_API_KEY=sk-your-key-here\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import Literal\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"‚úì Imports loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Basic Logging Setup\n",
        "\n",
        "Logging helps debug LLM applications - track inputs, outputs, and errors.\n",
        "\n",
        "**Key Rule:** Never log API keys or sensitive data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\"\n",
        ")\n",
        "logger = logging.getLogger(\"hello-llm\")\n",
        "\n",
        "# Test logging\n",
        "logger.info(\"Logging configured\")\n",
        "logger.debug(\"This won't show (level is INFO)\")\n",
        "logger.warning(\"This is a warning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Secure Configuration\n",
        "\n",
        "Create a configuration class that safely loads and validates settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"Application configuration - loads from environment.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        self.default_model = os.getenv(\"DEFAULT_MODEL\", \"gpt-4o-mini\")\n",
        "        self.ollama_url = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434/v1\")\n",
        "        \n",
        "        logger.info(\"Config loaded from environment\")\n",
        "        # NEVER log actual keys!\n",
        "        logger.info(f\"OpenAI key: {'configured' if self.openai_key else 'missing'}\")\n",
        "    \n",
        "    def validate(self) -> bool:\n",
        "        \"\"\"Validate required configuration.\"\"\"\n",
        "        if not self.openai_key:\n",
        "            logger.error(\"OPENAI_API_KEY not set\")\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "config = Config()\n",
        "config.validate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Determinism Controls\n",
        "\n",
        "LLMs are probabilistic - same input can give different outputs. Control this with:\n",
        "\n",
        "| Parameter | Effect |\n",
        "|-----------|--------|\n",
        "| `temperature=0` | Most deterministic (greedy) |\n",
        "| `seed=123` | Reproducible randomness |\n",
        "| `top_p=1.0` | Full probability distribution |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate determinism with temperature=0\n",
        "client = OpenAI()\n",
        "\n",
        "def deterministic_call(prompt: str, seed: int = 42):\n",
        "    \"\"\"Make a deterministic API call.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,  # Most deterministic\n",
        "        seed=seed,      # Reproducible\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Run same prompt twice - should get identical results\n",
        "prompt = \"What is 2+2? Answer with just the number.\"\n",
        "\n",
        "result1 = deterministic_call(prompt)\n",
        "result2 = deterministic_call(prompt)\n",
        "\n",
        "print(f\"First call:  {result1}\")\n",
        "print(f\"Second call: {result2}\")\n",
        "print(f\"Identical: {result1 == result2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Multi-Provider LLM Client\n",
        "\n",
        "Build a client that works with both OpenAI and Ollama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def list_ollama_models() -> list[str]:\n",
        "    \"\"\"Get available Ollama models.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=3)\n",
        "        if response.status_code == 200:\n",
        "            return [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
        "    except:\n",
        "        pass\n",
        "    return []\n",
        "\n",
        "\n",
        "class LLMClient:\n",
        "    \"\"\"Unified LLM client for multiple providers.\"\"\"\n",
        "    \n",
        "    def __init__(self, provider: Literal[\"openai\", \"ollama\"] = \"openai\"):\n",
        "        self.provider = provider\n",
        "        \n",
        "        if provider == \"openai\":\n",
        "            self.client = OpenAI()\n",
        "            self.model = \"gpt-4o-mini\"\n",
        "        else:\n",
        "            self.client = OpenAI(\n",
        "                base_url=\"http://localhost:11434/v1\",\n",
        "                api_key=\"ollama\"\n",
        "            )\n",
        "            models = list_ollama_models()\n",
        "            self.model = models[0] if models else \"llama3.2:3b\"\n",
        "        \n",
        "        logger.info(f\"LLMClient initialized: {provider}/{self.model}\")\n",
        "    \n",
        "    def chat(self, message: str, temperature: float = 0.7, seed: int = None) -> str:\n",
        "        \"\"\"Send a chat message and get a response.\"\"\"\n",
        "        \n",
        "        logger.info(f\"Chat request: {message[:50]}...\")\n",
        "        \n",
        "        kwargs = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": message}],\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": 200\n",
        "        }\n",
        "        \n",
        "        if seed is not None:\n",
        "            kwargs[\"seed\"] = seed\n",
        "        \n",
        "        response = self.client.chat.completions.create(**kwargs)\n",
        "        result = response.choices[0].message.content\n",
        "        \n",
        "        logger.info(f\"Response received: {len(result)} chars\")\n",
        "        return result\n",
        "\n",
        "# Test with OpenAI\n",
        "llm = LLMClient(\"openai\")\n",
        "print(f\"\\n{llm.chat('Hello! Say hi in one sentence.')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with Ollama (if available)\n",
        "if list_ollama_models():\n",
        "    llm_local = LLMClient(\"ollama\")\n",
        "    print(f\"\\n{llm_local.chat('Hello! Say hi in one sentence.')}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Ollama not available - skipping local test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Complete Application\n",
        "\n",
        "Let's bring everything together into a simple Q&A assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def md(text):\n",
        "    \"\"\"Display text as rendered markdown.\"\"\"\n",
        "    display(Markdown(text))\n",
        "\n",
        "\n",
        "class HelloLLM:\n",
        "    \"\"\"A simple Q&A assistant demonstrating all setup concepts.\"\"\"\n",
        "    \n",
        "    def __init__(self, provider: str = \"openai\", deterministic: bool = False):\n",
        "        self.llm = LLMClient(provider)\n",
        "        self.deterministic = deterministic\n",
        "        self.history = []\n",
        "        \n",
        "        logger.info(f\"HelloLLM started: provider={provider}, deterministic={deterministic}\")\n",
        "    \n",
        "    def ask(self, question: str) -> str:\n",
        "        \"\"\"Ask a question and get an answer.\"\"\"\n",
        "        \n",
        "        # Set determinism parameters\n",
        "        temperature = 0 if self.deterministic else 0.7\n",
        "        seed = 42 if self.deterministic else None\n",
        "        \n",
        "        # Get response\n",
        "        answer = self.llm.chat(question, temperature=temperature, seed=seed)\n",
        "        \n",
        "        # Log interaction (without sensitive data)\n",
        "        self.history.append({\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"question\": question,\n",
        "            \"answer_length\": len(answer),\n",
        "            \"provider\": self.llm.provider,\n",
        "            \"model\": self.llm.model\n",
        "        })\n",
        "        \n",
        "        return answer\n",
        "    \n",
        "    def show_history(self):\n",
        "        \"\"\"Display interaction history.\"\"\"\n",
        "        print(f\"\\nüìú History ({len(self.history)} interactions):\")\n",
        "        for i, h in enumerate(self.history, 1):\n",
        "            print(f\"  {i}. [{h['timestamp'][:19]}] {h['question'][:40]}... ({h['answer_length']} chars)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create assistant with OpenAI\n",
        "assistant = HelloLLM(provider=\"openai\", deterministic=True)\n",
        "\n",
        "# Ask some questions\n",
        "questions = [\n",
        "    \"What is machine learning in one sentence?\",\n",
        "    \"Name 3 popular programming languages.\",\n",
        "    \"What does API stand for?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"\\n‚ùì {q}\")\n",
        "    answer = assistant.ask(q)\n",
        "    md(f\"**Answer:** {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show interaction history\n",
        "assistant.show_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comparing Providers\n",
        "\n",
        "Let's compare responses from different providers on the same question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_providers(question: str):\n",
        "    \"\"\"Compare responses from different providers.\"\"\"\n",
        "    \n",
        "    print(f\"\\n‚ùì Question: {question}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # OpenAI\n",
        "    openai_llm = LLMClient(\"openai\")\n",
        "    openai_response = openai_llm.chat(question, temperature=0)\n",
        "    print(f\"\\n‚òÅÔ∏è OpenAI ({openai_llm.model}):\")\n",
        "    print(f\"   {openai_response}\")\n",
        "    \n",
        "    # Ollama (if available)\n",
        "    if list_ollama_models():\n",
        "        ollama_llm = LLMClient(\"ollama\")\n",
        "        ollama_response = ollama_llm.chat(question, temperature=0)\n",
        "        print(f\"\\nüíª Ollama ({ollama_llm.model}):\")\n",
        "        print(f\"   {ollama_response}\")\n",
        "    else:\n",
        "        print(\"\\nüíª Ollama: Not available\")\n",
        "\n",
        "compare_providers(\"Explain recursion in one sentence.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Summary\n",
        "\n",
        "### What We Built\n",
        "\n",
        "A complete LLM application with:\n",
        "\n",
        "1. **Secure Configuration** - API keys loaded from environment\n",
        "2. **Multi-Provider Support** - Works with OpenAI and Ollama\n",
        "3. **Determinism Controls** - Reproducible outputs with temperature=0 and seed\n",
        "4. **Logging** - Track interactions without exposing secrets\n",
        "\n",
        "### Key Patterns\n",
        "\n",
        "```python\n",
        "# Secure config loading\n",
        "load_dotenv()\n",
        "client = OpenAI()  # Auto-reads OPENAI_API_KEY\n",
        "\n",
        "# Deterministic calls\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[...],\n",
        "    temperature=0,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Switch to local model\n",
        "local_client = OpenAI(\n",
        "    base_url=\"http://localhost:11434/v1\",\n",
        "    api_key=\"ollama\"\n",
        ")\n",
        "```\n",
        "\n",
        "### Checklist\n",
        "\n",
        "- [x] Environment variables configured\n",
        "- [x] Secrets hygiene followed\n",
        "- [x] OpenAI and Ollama both working\n",
        "- [x] Determinism controls understood\n",
        "- [x] Basic logging implemented\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "You're now ready to explore **Module 2: LLM Core Concepts** where you'll learn about:\n",
        "- Tokenization and context windows\n",
        "- Temperature and sampling parameters\n",
        "- Streaming responses"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
